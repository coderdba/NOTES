================================
DIFY ON LAPTOP - 2024 April-May
================================

Self hosted installation

================================
INSTALL
================================
https://docs.dify.ai/getting-started/install-self-hosted/docker-compose

git clone git@github.com:langgenius/dify.git
cd dify/docker
docker compose up -d

docker ps -a
CONTAINER ID   IMAGE                                                        COMMAND                  CREATED         STATUS                      PORTS                                                                                  NAMES
862b31e69fd7   nginx:latest                                                 "/docker-entrypoint.…"   7 minutes ago   Up 7 minutes                0.0.0.0:80->80/tcp, :::80->80/tcp                                                      docker-nginx-1
0142df2deb4a   langgenius/dify-api:0.6.4                                    "/bin/bash /entrypoi…"   7 minutes ago   Up 6 minutes                5001/tcp                                                                               docker-api-1
3d9a7e0f9e71   langgenius/dify-api:0.6.4                                    "/bin/bash /entrypoi…"   7 minutes ago   Up 7 minutes                5001/tcp                                                                               docker-worker-1
962b2dbd5520   semitechnologies/weaviate:1.19.0                             "/bin/weaviate --hos…"   7 minutes ago   Up 7 minutes                                                                                                       docker-weaviate-1
cce97a34104d   redis:6-alpine                                               "docker-entrypoint.s…"   7 minutes ago   Up 7 minutes (healthy)      6379/tcp                                                                               docker-redis-1
56eed493d059   postgres:15-alpine                                           "docker-entrypoint.s…"   7 minutes ago   Up 7 minutes (healthy)      5432/tcp                                                                               docker-db-1
4765468fab1c   langgenius/dify-web:0.6.4                                    "/bin/sh ./entrypoin…"   7 minutes ago   Up 7 minutes                3000/tcp                                                                               docker-web-1
153de1c03265   langgenius/dify-sandbox:latest                               "/main"                  7 minutes ago   Up 7 minutes  
    
Access the UI: http://localhost/install 

====================================
LOCAL MODEL USING OLLAMA
====================================
Dify doc: https://docs.dify.ai/tutorials/model-configuration/ollama
Official doc: https://github.com/ollama/ollama
    API Doc: https://github.com/ollama/ollama/blob/main/docs/api.md

Tutorials: Ollama with locally downloaded model file:
https://medium.com/@gabrielrodewald/running-models-with-ollama-step-by-step-60b6f6125807
https://discuss.huggingface.co/t/how-to-download-a-model-and-run-it-with-ollama-locally/77317

Download ollama installer file from https://ollama.com/download/windows
Install it
It will install to C:\Users\myusername\AppData\Local\Programs\Ollama

Set environment variable OLLAMA_MODELS = C:\opt\llm\models
--> this did not seem to help - ollama list did not show any models

USE A SPECIFIC MODEL
Create a file Modelfile.txt with content like the following indicating which model to use:
FROM ./mistral-7b-instruct-v0.1.Q4_K_M.gguf.bin

Create a model blob 
- Will get created in the models folder under a 'blob' subfolder
  NOTE: This will use up additional space in the blob subfolder

ollama create example -f Modelfile.txt

ollama list
NAME            ID              SIZE    MODIFIED
example:latest  ea009b1ff01d    4.4 GB  3 minutes ago

Run the model
ollama run example --> 'example' is the name of the local blob model created from the downloaded model
NOTE: This will start an interactive shell

Run Ollama as server
ollama serve

-------------
API Calls
-------------
NOTE: Need to supply the model name in the request body

- List models
GET http://localhost:11434/api/tags

- Generate an answer
POST http://localhost:11434/api/generate
with body:
{
  "model": "example",
  "prompt": "Why is the sky blue?"
}
--> The response will be one json for each word in the response sentence which is a bit crazy!!

{
    "model": "example",
    "created_at": "2024-04-25T06:04:28.9559249Z",
    "response": " This",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.1559437Z",
    "response": " question",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.3310589Z",
    "response": " has",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.4957726Z",
    "response": " conf",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.6652588Z",
    "response": "ound",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.8488923Z",
    "response": "ed",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.0221722Z",
    "response": " people",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.1931946Z",
    "response": " for",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.3541808Z",
    "response": " centuries",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.5094005Z",
    "response": ",",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.6723862Z",
    "response": " but",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.8484434Z",
    "response": " the",
    "done": f ... ...
...
...
...

====================================
LOCAL MODEL USING LOCALAI - *** FAILED ***
====================================
Dify doc - https://docs.dify.ai/tutorials/model-configuration/localai

Install - https://localai.io/basics/getting_started/
Running specific model - https://localai.io/docs/getting-started/manual/

--------------------------
INSTALL LOCALAI
--------------------------
From Dify doc - https://docs.dify.ai/tutorials/model-configuration/localai

- Clone the LocalAI code repository and navigate to the specified directory.
$ git clone https://github.com/go-skynet/LocalAI
$ cd LocalAI/examples/langchain-chroma
Replace content of "models" file from "../models" to "C:\opt\LLM\models" to denote location of downloaded models.

- Download example LLM and Embedding models
Here, we choose two smaller models that are compatible across all platforms. 
ggml-gpt4all-j serves as the default LLM model, 
and all-MiniLM-L6-v2 serves as the default Embedding model, for quick local deployment.

-- Downloaded these (not the ones mentioned in the doc)
NOTE: Downloaded to C:\opt\LLM\models
LLM Model:
https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF --> mistral-7b-instruct-v0.1.Q4_K_M.gguf.bin
 --> already downloaded for a different POC

Embedding models:
https://huggingface.co/skeskinen/ggml/tree/main/all-MiniLM-L6-v2 --> ggml-model-f16.bin
https://huggingface.co/skeskinen/ggml/tree/main/all-MiniLM-L6-v2 --> ggml-model-q4_0.bin

-- DID NOT DOWNOLAD THESE - From Doc 
LLM Model:
$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j

Embeddings model:
$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert

- Configure the .env file.
This is in the langchain-chroma directory
$ cp .env.example .env

- (FAILED) Start LocalAI
This took a long time and failed.
docker compose up -d --build

- (FAILED) Therefore, use this: https://localai.io/docs/getting-started/manual/
Modified the docker-compose.yml in examples/langchain-chroma as follows:
a. Images - changed latest to latest-cpu
b. Volumes - changed host volume to the folder that contains the downloaded models

Pull image:
docker pull quay.io/go-skynet/local-ai:latest-cpu
--> This pull itself FAILED

Run command:
docker compose up -d 
(In doc: docker compose up -d --pull always)

-------------------
docker-compose.yml - Modified
-------------------
version: '3.6'

services:
  api:
    image: quay.io/go-skynet/local-ai:latest-cpu
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    env_file:
      - ../../.env
    #volumes:
    #  - ./models:/models:cached
    volumes:
      - C:\opt\LLM\models:/models:cached
    command: ["/usr/bin/local-ai"]

