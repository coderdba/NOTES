================================
DIFY ON LAPTOP - 2024 April-May
================================

Self hosted installation

================================
INSTALL
================================
https://docs.dify.ai/getting-started/install-self-hosted/docker-compose

git clone git@github.com:langgenius/dify.git
cd dify/docker
docker compose up -d

docker ps -a
CONTAINER ID   IMAGE                                                        COMMAND                  CREATED         STATUS                      PORTS                                                                                  NAMES
862b31e69fd7   nginx:latest                                                 "/docker-entrypoint.…"   7 minutes ago   Up 7 minutes                0.0.0.0:80->80/tcp, :::80->80/tcp                                                      docker-nginx-1
0142df2deb4a   langgenius/dify-api:0.6.4                                    "/bin/bash /entrypoi…"   7 minutes ago   Up 6 minutes                5001/tcp                                                                               docker-api-1
3d9a7e0f9e71   langgenius/dify-api:0.6.4                                    "/bin/bash /entrypoi…"   7 minutes ago   Up 7 minutes                5001/tcp                                                                               docker-worker-1
962b2dbd5520   semitechnologies/weaviate:1.19.0                             "/bin/weaviate --hos…"   7 minutes ago   Up 7 minutes                                                                                                       docker-weaviate-1
cce97a34104d   redis:6-alpine                                               "docker-entrypoint.s…"   7 minutes ago   Up 7 minutes (healthy)      6379/tcp                                                                               docker-redis-1
56eed493d059   postgres:15-alpine                                           "docker-entrypoint.s…"   7 minutes ago   Up 7 minutes (healthy)      5432/tcp                                                                               docker-db-1
4765468fab1c   langgenius/dify-web:0.6.4                                    "/bin/sh ./entrypoin…"   7 minutes ago   Up 7 minutes                3000/tcp                                                                               docker-web-1
153de1c03265   langgenius/dify-sandbox:latest                               "/main"                  7 minutes ago   Up 7 minutes  
    
Access the UI: http://localhost/install 

====================================
LOCAL MODEL USING LOCALAI
====================================
Dify doc - https://docs.dify.ai/tutorials/model-configuration/localai

Install - https://localai.io/basics/getting_started/
Running specific model - https://localai.io/docs/getting-started/manual/

--------------------------
INSTALL LOCALAI
--------------------------
From Dify doc - https://docs.dify.ai/tutorials/model-configuration/localai

- Clone the LocalAI code repository and navigate to the specified directory.
$ git clone https://github.com/go-skynet/LocalAI
$ cd LocalAI/examples/langchain-chroma
Replace content of "models" file from "../models" to "C:\opt\LLM\models" to denote location of downloaded models.

- Download example LLM and Embedding models
Here, we choose two smaller models that are compatible across all platforms. 
ggml-gpt4all-j serves as the default LLM model, 
and all-MiniLM-L6-v2 serves as the default Embedding model, for quick local deployment.

-- Downloaded these (not the ones mentioned in the doc)
NOTE: Downloaded to C:\opt\LLM\models
LLM Model:
https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF --> mistral-7b-instruct-v0.1.Q4_K_M.gguf.bin
 --> already downloaded for a different POC

Embedding models:
https://huggingface.co/skeskinen/ggml/tree/main/all-MiniLM-L6-v2 --> ggml-model-f16.bin
https://huggingface.co/skeskinen/ggml/tree/main/all-MiniLM-L6-v2 --> ggml-model-q4_0.bin

-- DID NOT DOWNOLAD THESE - From Doc 
LLM Model:
$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j

Embeddings model:
$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert

- Configure the .env file.
This is in the langchain-chroma directory
$ cp .env.example .env

- Start LocalAI
This took a long time and failed.
docker compose up -d --build

- Therefore, use this: https://localai.io/docs/getting-started/manual/
Modified the docker-compose.yml in examples/langchain-chroma as follows:
a. Images - changed latest to latest-cpu
b. Volumes - changed host volume to the folder that contains the downloaded models

Pull image:
docker pull quay.io/go-skynet/local-ai:latest-cpu

Run command:
docker compose up -d 
(In doc: docker compose up -d --pull always)

-------------------
docker-compose.yml - Modified
-------------------
version: '3.6'

services:
  api:
    image: quay.io/go-skynet/local-ai:latest-cpu
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    env_file:
      - ../../.env
    #volumes:
    #  - ./models:/models:cached
    volumes:
      - C:\opt\LLM\models:/models:cached
    command: ["/usr/bin/local-ai"]


