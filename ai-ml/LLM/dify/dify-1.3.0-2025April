=============================================
DIFY 1.3.0 INSTALL 
=============================================
https://dify.ai/
https://github.com/langgenius
https://github.com/langgenius/dify/releases
https://github.com/langgenius/dify/releases/tag/1.3.0

Install stuff: https://docs.dify.ai/getting-started/install-self-hosted/docker-compose

git clone https://github.com/langgenius/dify.git --branch 1.3.0
cd docker
docker compose up -d

http://localhost/install --> to create admin user/pass

==================================
MODEL PROVIDERS
==================================
NameInitialIcon --> Settings --> model provider
Install azure openai plugin (http://localhost/plugins)
Install ollama plugin for local ollama models
Refresh that page
For each added model plugin, do 'add model' and add the model details, api keys etc

-----------
OLLAMA
-----------
Model Type: LLM
Base URL: http://host.docker.internal:11434
Model Name: codellama:7b
Rest of the fields will be picked up by Dify

----------------------
AZURE OPENAI
----------------------
Model Type: LLM
Deployment name: gpt-4o-mini
Endpoint URL: https://x-dev.openai.azure.com/
API KEY: lksjdlskjfls
API Version: 2024-08-01-preview
Base Model: gpt-4o-mini

================================
APP - CHATBOT
================================
localhost --> apps (localhost/apps)
Choose chatbot
Name: chat-norag1
Description: chatbot1 without rag

================================
TOOL - CUSTOM TOOL
================================

How it works:
- It creates a tool by transparently writing wrapper around APIs
- Give Dify the API spec in OpenAPI format
- Give it the credentials (API Key)
- Then it will create the tool wrapper implicitly and publish as a tool

API:
- The chat-azureopenai - chat-hist-05-agent-supervisor-lg-pg multi-function agentic AI API
- NOTE: If API is running on the same machine, and Dify is docker based, then use host.docker.internal to access the host address (localhost does not work)

- OpenAPI spec: (YML)

openapi: "3.0.0"
info:
  version: "1.0.0"
  title: "Query API"
  description: "API for querying data"
servers:
  - url: "http://host.docker.internal:5101"
paths:
  /query:
    get:
      summary: "Query data"
      description: "Returns data based on the query parameter"
      parameters:
        - name: "query"
          in: "query"
          required: true
          schema:
            type: "string"
          description: "The query string"
      responses:
        200:
          description: "Successful response"
          content:
            application/json:
              schema:
                type: "object"
                properties:
                  data:
                    type: "array"
                    items:
                      type: "object"
        400:
          description: "Invalid query parameter"
        500:
          description: "Internal server error"


--------------------
STEPS
--------------------
Main page --> Tools --> Custom
--> Create custom tool
- Name = chat-hist-05-agent-supervisor-lg-pg
- OpenAPI spec = as above
- Authorization = none
- Other fields = blanks

It will read the endpoints from spec and add 'tools':
Available Tools
Name	    Description                              	Method	Path	  Actions
query_get	Returns data based on the query parameter	get	    /query	Test (to test)

Do a test.
Paramter  Value
query     where is usa located

You should see the output from the API.

================================
AGENT
================================
NOTE: This 'agent' is not the same as langchain or llamaindex agents.  This is more like a chat-assistant that uses tools.

Main page --> studio --> agent
Add new agent:
NAME = agent-chat-hist-05-agent-supervisor-lg-pg
Tools = chat-hist-05-agent-supervisor-lg-pg --> query_get

This will give a chatbot with tools being called for questions.


================================
WORKFLOW
================================
https://docs.dify.ai/guides/workflow/node/agent
https://docs.dify.ai/guides/workflow/node/agent#select-an-agent-strategy
From the dropdown menu, select the desired Agent reasoning strategy. 
Dify provides two built-in strategies, Function Calling and ReAct, which can be installed from the Marketplace â†’ Agent Strategies category.

First install "Dify Agent Strategies" plugin from marketplace for calling agent with ReAct or other strategies.
- https://marketplace.dify.ai/plugins/langgenius/agent
- https://docs.dify.ai/plugins/quick-start/install-plugins
--> for this, go to main page, click "Plugins" in the top-right corner of the Dify platform to access the plugin management page. 
--> You can install plugins via Marketplace, GitHub, or Manual Upload.
--> This specific one is in marketplace

Main page --> studio --> workflow --> new workflow
It gives one node "start"
Then add a node using + button at the bottom.
It expects agent strategy
--> choose 'agent' ReAct in that box

AGENTIC STRATEGY = ReAct
MODEL = gpt-4o-mini
TOOL LIST = Choose the query_get tool from the tool created earlier
INSTRUCTION = Run the user query
QUERY = "where is france" ---->  How to parameterize this?? 
--> use sys.query to accept stuff entered in chatbot (https://docs.dify.ai/guides/workflow/variables)


================================
CHATFLOW
================================
Main page --> studio --> chatflow --> new workflow

By default this comes with a LLM node.
START - LLM - ANSWER
We can add additional nodes.

Add an Agent node between START and LLM --> START - AGENT - LLM - ANSWER
AGENTIC STRATEGY - Function calling (can be ReAct also, assuming we have more than one tool)
LLM = gpt-4o-mini
TOOL LIST - Choose the query_get tool from the tool created earlier
INSTRUCTION - Respond to user query
QUERY -  Type "{" and choose sys.query
MEMORY - Turned it on (can be turned off if history is not required)

Configure LLM node:
MODEL = gpt-4o-mini
CONTEXT = left blank
SYSTEM = left blank
MEMORIES = left as is
USER = chose two variables: Agent/text (that is, output from agent, which will appear as Agent/{x}text), sys.query (which will appear as START/{x}sys.query)
MEMORY = enabled it

Configure ANSWER node:
ANSWER = LLM/{x}Text

=========================
ISSUES
=========================
Can't create app from template 
https://github.com/langgenius/dify/issues/18855
https://github.com/langgenius/dify/issues/18653 --> I put a comment in this





