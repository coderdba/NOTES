==========================================
  DIFY INSTALL AND USE
==========================================

https://docs.dify.ai/getting-started/install-self-hosted/docker-compose
https://docs.dify.ai/getting-started/readme/features-and-specifications

======================
  INSTALL
======================
Download software from Git for a specific release: https://github.com/langgenius/dify/releases
Unzip
Go to docker folder: dify-0.15.2\docker
Copy env file: copy .env.example .env
Start app: docker compose up -d

======================
  ACCESS WEBUI
======================
http://localhost --> default port 80 for nginx

Asks you to create an admin acct.
Give email, username password

Then it asks you to logon.
Enter email and password to logon.

======================
INTEGRATE LLM MODEL
======================
Open-source models:
- https://docs.dify.ai/development/models-integration
- https://docs.dify.ai/development/models-integration/ollama

Non Open-source models:
- https://docs.dify.ai/guides/model-configuration

------------------
OLLAMA
------------------
https://docs.dify.ai/development/models-integration/ollama

- Find which model is running
ollama list
NAME            ID              SIZE    MODIFIED
codellama:7b    8fdf8f752f6e    3.8 GB  3 months ago

- Setup in Dify
Go to settings under your name at the right hand top dropdown.
In Settings > Model Providers > Ollama, fill in: 
model type - llm
model name - codellama:7b (or other readable name)
base url - http://localhost:11434
completion mode - chat
Model context size - 4096 (default - can change)
Upper bound of max tokens - 4096 (default - can change)
vision support - no
function call support - no
