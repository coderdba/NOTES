==========================================
  DIFY INSTALL AND USE
==========================================

https://docs.dify.ai/getting-started/install-self-hosted/docker-compose
https://docs.dify.ai/getting-started/readme/features-and-specifications

======================
  INSTALL
======================
Download software from Git for a specific release: https://github.com/langgenius/dify/releases
Unzip
Go to docker folder: dify-0.15.2\docker
Copy env file: copy .env.example .env
Start app: docker compose up -d

======================
  ACCESS WEBUI
======================
http://localhost --> default port 80 for nginx

Asks you to create an admin acct.
Give email, username password

Then it asks you to logon.
Enter email and password to logon.

======================
INTEGRATE LLM MODEL
======================
Open-source models:
- https://docs.dify.ai/development/models-integration
- https://docs.dify.ai/development/models-integration/ollama

Non Open-source models:
- https://docs.dify.ai/guides/model-configuration

------------------
OLLAMA
------------------
https://docs.dify.ai/development/models-integration/ollama

- Set OLLAMA_HOST to 0.0.0.0
To avoid the following error, set OLLAMA_HOST environment variable to 0.0.0.0 and restart OLLAMA
(NOTE: ollama serve puts it in FOREGROUND.  Start in background by pressing icon in start menu of Windows)
ERROR to avoid:
An error occurred during credentials validation: API request failed with status code 500: {"error":"error starting the external llama server: fork/exec C:\\Users\\GSM078\\AppData\\Local\\Temp\\ollama237530857\\runners\\cpu_avx2\\ollama_llama_server.exe: The system cannot find the file specified. "}

- Find which model is running
ollama list
NAME            ID              SIZE    MODIFIED
codellama:7b    8fdf8f752f6e    3.8 GB  3 months ago

- Setup in Dify
Go to settings under your name at the right hand top dropdown.
In Settings > Model Providers > Ollama, fill in: 
model type - llm
model name - codellama:7b (or other readable name)
base url - http://host.docker.internal:11434 (as dify runs as a docker container to access host localhost wont work)
completion mode - chat
Model context size - 4096 (default - can change)
Upper bound of max tokens - 4096 (default - can change)
vision support - no
function call support - no
Then press save
--> That will enable local ollama model and show in the enabled list




