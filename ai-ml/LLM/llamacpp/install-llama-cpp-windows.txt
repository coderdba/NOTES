================================================
INSTALL LLAMA CPP PYTHON PACKAGE ON WINDOWS 10
================================================

https://python.langchain.com/v0.1/docs/integrations/llms/llamacpp/
--> https://github.com/abetlen/llama-cpp-python
https://github.com/mpwang/llama-cpp-windows-guide

https://python.langchain.com/v0.2/docs/integrations/llms/llamacpp/

============================
PYTHON VIRTUAL ENVIRONMENT
============================
https://docs.python.org/3/library/venv.html
c:\>python -m venv c:\path\to\myenv

python -m venv venv
cd venv/Scripts
activate
python.exe -m pip install --upgrade pip

=========================
Installation (2024 May)
=========================
Steps from: https://python.langchain.com/v0.1/docs/integrations/llms/llamacpp/
And from: https://llama-cpp-python.readthedocs.io/en/stable/
------------------------------------------------
Requirements
------------------------------------------------
Python 3.8+ --> 3.11 installed
C compiler --> installed (no idea how, was installed sometime ago)
Linux: gcc or clang
Windows: Visual Studio or MinGW --> installed Visual Studio 2022
MacOS: Xcode

----------------------------------------------------
INSTALL WITHOUT ANY EXTRA STUFF LIKE GPU (assuming this will install cpu-only)
----------------------------------------------------
(venv) C:\opt\python-venv\AI1\venv>pip install llama-cpp-python
Collecting llama-cpp-python
  Using cached llama_cpp_python-0.2.76.tar.gz (49.4 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting typing-extensions>=4.5.0 (from llama-cpp-python)
  Using cached typing_extensions-4.12.0-py3-none-any.whl.metadata (3.0 kB)
Collecting numpy>=1.20.0 (from llama-cpp-python)
  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)
Collecting diskcache>=5.6.1 (from llama-cpp-python)
  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)
Collecting jinja2>=2.11.3 (from llama-cpp-python)
  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)
  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)
Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)
Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)
Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)
Using cached typing_extensions-4.12.0-py3-none-any.whl (37 kB)
Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml) ... done
  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp311-cp311-win_amd64.whl size=3281178 sha256=05763ba9c27c51347023bc5926012d75059c0f972e779d0005c380124ba05715
  Stored in directory: c:\users\admin\appdata\local\pip\cache\wheels\78\ad\8f\d3fc4b4932eea66384e6a3ca2d500d82728a05d92be3c6246e
Successfully built llama-cpp-python
Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python
Successfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.76 numpy-1.26.4 typing-extensions-4.12.0

(venv) C:\opt\python-venv\AI1\venv>pip list
Package           Version
----------------- -------
diskcache         5.6.3
Jinja2            3.1.4
llama_cpp_python  0.2.76
MarkupSafe        2.1.5
numpy             1.26.4
pip               24.0
setuptools        65.5.0
typing_extensions 4.12.0

----------------------------------------------------
INSTALL WITH HIPBLAS SUPPORT FOR AMD GPU - FAILED
----------------------------------------------------
set CMAKE_ARGS=-DLLAMA_HIPBLAS=on (not sure if the value must be enclosed within double quotes)
pip install llama-cpp-python

(venv) C:\opt\python-venv\AI1\venv> pip install llama-cpp-python
Collecting llama-cpp-python
  Downloading llama_cpp_python-0.2.75.tar.gz (48.7 MB)
     ---------------------------------------- 48.7/48.7 MB 1.6 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Collecting typing-extensions>=4.5.0 (from llama-cpp-python)
  Downloading typing_extensions-4.12.0-py3-none-any.whl.metadata (3.0 kB)
Collecting numpy>=1.20.0 (from llama-cpp-python)
  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)
     ---------------------------------------- 61.0/61.0 kB 95.7 kB/s eta 0:00:00
Collecting diskcache>=5.6.1 (from llama-cpp-python)
  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)
Collecting jinja2>=2.11.3 (from llama-cpp-python)
  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)
  Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)
Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)
   ---------------------------------------- 45.5/45.5 kB 751.3 kB/s eta 0:00:00
Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)
   ---------------------------------------- 133.3/133.3 kB 1.6 MB/s eta 0:00:00
Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)
   ---------------------------------------- 15.8/15.8 MB 2.0 MB/s eta 0:00:00
Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)
Downloading MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [53 lines of output]
      *** scikit-build-core 0.9.4 using CMake 3.29.3 (wheel)
      *** Configuring CMake...
      2024-05-24 11:29:54,827 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None
      loading initial cache file C:\Users\ADMIN\AppData\Local\Temp\tmpjzwn470a\build\CMakeInit.txt
      -- Building for: Visual Studio 17 2022
      -- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.19045.
      -- The C compiler identification is MSVC 19.36.32535.0
      -- The CXX compiler identification is MSVC 19.36.32535.0
      -- Detecting C compiler ABI info
      -- Detecting C compiler ABI info - done
      -- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.36.32532/bin/Hostx64/x64/cl.exe - skipped
      -- Detecting C compile features
      -- Detecting C compile features - done
      -- Detecting CXX compiler ABI info
      -- Detecting CXX compiler ABI info - done
      -- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.36.32532/bin/Hostx64/x64/cl.exe - skipped
      -- Detecting CXX compile features
      -- Detecting CXX compile features - done
      -- Found Git: C:/Program Files/Git/cmd/git.exe (found version "2.41.0.windows.2")
      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD
      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
      -- Looking for pthread_create in pthreads
      -- Looking for pthread_create in pthreads - not found
      -- Looking for pthread_create in pthread
      -- Looking for pthread_create in pthread - not found
      -- Found Threads: TRUE
      CMake Warning at vendor/llama.cpp/CMakeLists.txt:561 (message):
        Only LLVM is supported for HIP, hint: CC=/opt/rocm/llvm/bin/clang


      CMake Warning at vendor/llama.cpp/CMakeLists.txt:565 (message):
        Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++


      CMake Error at vendor/llama.cpp/CMakeLists.txt:568 (find_package):
        By not providing "Findhip.cmake" in CMAKE_MODULE_PATH this project has
        asked CMake to find a package configuration file provided by "hip", but
        CMake did not find one.

        Could not find a package configuration file provided by "hip" with any of
        the following names:

          hipConfig.cmake
          hip-config.cmake

        Add the installation prefix of "hip" to CMAKE_PREFIX_PATH or set "hip_DIR"
        to a directory containing one of the above files.  If "hip" provides a
        separate development package or SDK, be sure it has been installed.


      -- Configuring incomplete, errors occurred!

      *** CMake configuration failed
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for llama-cpp-python
Failed to build llama-cpp-python



