==================================================
SESSION ID FROM MCP SERVER - Mcp-Session-Id
==================================================
https://modelcontextprotocol.io/docs/concepts/transports#python-client

Session Management
Streamable HTTP supports stateful sessions to maintain context across multiple requests:

Session Initialization: Servers may assign a session ID during initialization by including it in an Mcp-Session-Id header
Session Persistence: Clients must include the session ID in all subsequent requests using the Mcp-Session-Id header
Session Termination: Sessions can be explicitly terminated by sending an HTTP DELETE request with the session ID

Example session flow:
// Server assigns session ID during initialization
app.post("/mcp", (req, res) => {
  if (req.body.method === "initialize") {
    const sessionId = generateSecureId();
    res.setHeader("Mcp-Session-Id", sessionId);
    // Store session state...
  }
  // Handle request...
});

// Client includes session ID in subsequent requests
fetch("/mcp", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Mcp-Session-Id": sessionId,
  },
  body: JSON.stringify(request),
});

Resumability and Redelivery
To support resuming broken connections, Streamable HTTP provides:

Event IDs: Servers can attach unique IDs to SSE events for tracking
Resume from Last Event: Clients can resume by sending the Last-Event-ID header
Message Replay: Servers can replay missed messages from the disconnection point
This ensures reliable message delivery even with unstable network connections.

â€‹
Security Considerations
When implementing Streamable HTTP transport, follow these security best practices:

Validate Origin Headers: Always validate the Origin header on all incoming connections to prevent DNS rebinding attacks
Bind to Localhost: When running locally, bind only to localhost (127.0.0.1) rather than all network interfaces (0.0.0.0)
Implement Authentication: Use proper authentication for all connections
Use HTTPS: Always use TLS/HTTPS for production deployments
Validate Session IDs: Ensure session IDs are cryptographically secure and properly validated
Without these protections, attackers could use DNS rebinding to interact with local MCP servers from remote websites.


=====================================
PYTHON CLIENT CODE - COPILOT 
=====================================
import requests
import json

# Replace this with your actual MCP server URL
MCP_SERVER_URL = "https://your-mcp-server.com/mcp"

# Step 1: Send initialize request
init_request = {
    "jsonrpc": "2.0",
    "id": "init-001",
    "method": "initialize",
    "params": {
        "client": "python-client",
        "version": "1.0"
    }
}

response = requests.post(
    MCP_SERVER_URL,
    headers={
        "Content-Type": "application/json",
        "Accept": "application/json, text/event-stream"
    },
    data=json.dumps(init_request)
)

if response.status_code == 200:
    session_id = response.headers.get("Mcp-Session-Id")
    print("Mcp-Session-Id:", session_id)

    # Step 2: Send a follow-up request using the session ID
    follow_up_request = {
        "jsonrpc": "2.0",
        "id": "req-002",
        "method": "getStatus",
        "params": {}
    }

    follow_up_response = requests.post(
        MCP_SERVER_URL,
        headers={
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Mcp-Session-Id": session_id
        },
        data=json.dumps(follow_up_request)
    )

    print("Follow-up response:", follow_up_response.json())
else:
    print("Failed to initialize session. Status code:", response.status_code)
    print("Response:", response.text)

========================================================================
WITH LANGCHAIN - COPILOT CODE 
========================================================================

Here's a complete example of how to build a Python MCP client using LangChain's MCP adapter to connect to a streamable HTTP MCP server, retrieve tools, and invoke them using a LangChain-compatible agent.

âœ… 1. Install Required Packages
pip install langchain-mcp-adapters langgraph "langchain[openai]"

âœ… 2. Start a Streamable HTTP MCP Server
You can use the example server provided in the langchain-mcp-adapters GitHub repo: https://github.com/langchain-ai/langchain-mcp-adapters

cd examples/servers/streamable-http-stateless/
uv run mcp-simple-streamablehttp-stateless --port 3000

This will start a server at http://localhost:3000/mcp/

âœ… 3. Python Client Code Using LangChain MCP Adapter
import asyncio
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

# Initialize the LLM (you can use any LangChain-compatible model)
llm = ChatOpenAI(model="gpt-4", temperature=0)

async def main():
    # Connect to the MCP server via streamable HTTP
    async with streamablehttp_client("http://localhost:3000/mcp/") as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            print("âœ… MCP Session Initialized")

            # Load tools from the MCP server
            tools = await load_mcp_tools(session)
            print(f"ðŸ› ï¸ Loaded Tools: {[tool.name for tool in tools]}")

            # Create a LangChain agent with the tools
            agent = create_react_agent(llm, tools)

            # Invoke the agent with a user query
            response = await agent.ainvoke({
                "messages": [("user", "What is (3 + 5) x 12?")]
            })

            print("ðŸ¤– Agent Response:")
            print(response["messages"][-1].content)

if __name__ == "__main__":
    asyncio.run(main())

ðŸ§  What This Does
Connects to a streamable HTTP MCP server.
Initializes a session and retrieves the Mcp-Session-Id internally.
Loads available tools from the server.
Uses a LangChain-compatible LLM to invoke those tools via a ReAct agent.

========================================================================
WITH LANGCHAIN MULTIPLE MCP SERVERS IN ONE CLIENT - COPILOT CODE 
========================================================================

----------------------------------------------------------------------
Client program - not yet getting session ids
----------------------------------------------------------------------
Hereâ€™s a complete example of a Python client using MultiServerMCPClient from langchain_mcp_adapters to connect to multiple MCP servers, load tools from each, and use them with a LangChain agent.

âœ… 1. Install Required Packages
pip install langchain-mcp-adapters langgraph "langchain[openai]"

âœ… 2. Python Client Using MultiServerMCPClient
import asyncio
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools

# Define the MCP server URLs
MCP_SERVERS = [
    "http://localhost:3000/mcp/",
    "http://localhost:3001/mcp/"
]

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)

async def main():
    # Create a MultiServerMCPClient
    async with MultiServerMCPClient(MCP_SERVERS) as client:
        await client.initialize()
        print("âœ… Multi-server MCP session initialized")

        # Load tools from all servers
        tools = await load_mcp_tools(client)
        print(f"ðŸ› ï¸ Loaded tools from multiple servers: {[tool.name for tool in tools]}")

        # Create a LangChain ReAct agent
        agent = create_react_agent(llm, tools)

        # Invoke the agent with a user query
        response = await agent.ainvoke({
            "messages": [("user", "Use any available tools to calculate the square root of 144.")]
        })

        print("ðŸ¤– Agent Response:")
        print(response["messages"][-1].content)

if __name__ == "__main__":
    asyncio.run(main())

ðŸ§  What This Does
Connects to multiple MCP servers using MultiServerMCPClient.
Loads tools from all connected servers.
Uses a LangChain-compatible LLM to invoke those tools via a ReAct agent.

----------------------------------------------------------------------
Client program getting session ids from multiple mcp servers
----------------------------------------------------------------------
âœ… Accessing Session IDs from MultiServerMCPClient
Each server connection inside MultiServerMCPClient is a ClientSession object. You can access them like this:

for url, session in client.sessions.items():
    print(f"Server: {url}")
    print(f"Session ID: {session.session_id}")

ðŸ” Updated Example Snippet
Add this right after await client.initialize() in the previous code:

# Print session IDs for each MCP server
for url, session in client.sessions.items():
    print(f"ðŸ”— Connected to: {url}")
    print(f"ðŸ†” MCP Session ID: {session.session_id}")

âœ… Full Python Client Code
import asyncio
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools

# Define the full MCP endpoint URLs
MCP_ENDPOINTS = [
    "http://localhost:3000/mcp/math",
    "http://localhost:3000/mcp/utils"
]

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)

async def main():
    # Connect to both MCP endpoints
    async with MultiServerMCPClient(MCP_ENDPOINTS) as client:
        await client.initialize()
        print("âœ… Multi-server MCP session initialized")

        # Print session IDs for each endpoint
        for url, session in client.sessions.items():
            print(f"ðŸ”— Connected to: {url}")
            print(f"ðŸ†” MCP Session ID: {session.session_id}")

        # Load tools from both endpoints
        tools = await load_mcp_tools(client)
        print(f"ðŸ› ï¸ Loaded tools: {[tool.name for tool in tools]}")

        # Create a LangChain ReAct agent
        agent = create_react_agent(llm, tools)

        # Invoke the agent with a user query
        response = await agent.ainvoke({
            "messages": [("user", "What is the reverse of 'LangChain' and also 12 times 3?")]
        })

        print("ðŸ¤– Agent Response:")
        print(response["messages"][-1].content)

if __name__ == "__main__":
    asyncio.run(main())

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Doing subsequent tool calls using session id
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
How can we do subsequent tool calls passing session id to the servers

To perform subsequent tool calls using the same session ID with MCP servers, you donâ€™t need to manually pass the session ID in most cases when using MultiServerMCPClient. It handles session management internally. However, if you want to explicitly control or reuse sessions for advanced use cases, hereâ€™s how you can do it:

âœ… How It Works Internally
Each ClientSession inside MultiServerMCPClient maintains its own session_id. When you call tools via LangChain agents, the adapter automatically includes the correct Mcp-Session-Id in the headers for each request.

âœ… Manual Tool Invocation with Session ID
If you want to manually invoke a tool using a specific session, you can do something like this:

# Access a specific session
session = client.sessions["http://localhost:3000/mcp/math"]

# Manually call a tool using that session
result = await session.call_tool("add_numbers", {"a": 5, "b": 7})
print("Manual tool result:", result)

This ensures the tool call is made using the correct session and server.

âœ… Subsequent Calls via Agent (Automatic)
If you're using a LangChain agent (like ReAct), you donâ€™t need to do anything special. The agent will:

Use the tools loaded from each session.
Automatically include the correct Mcp-Session-Id when calling tools.
So you can just do:

response = await agent.ainvoke({
    "messages": [("user", "Now multiply the result by 2.")]
})

And the agent will handle session continuity.

ðŸ§  Summary
Method	Session ID Handling
MultiServerMCPClient	Automatic per endpoint
Manual tool call	Use session.call_tool(...)
LangChain agent	Automatic via tool wrappers

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
FULL CODE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Hereâ€™s a complete example that demonstrates both:

Manual tool invocation using a specific session and session ID.
Agent-based invocation using LangChainâ€™s ReAct agent.
âœ… Combined Example: Manual + Agent Tool Calls
import asyncio
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools

# MCP endpoints
MCP_ENDPOINTS = [
    "http://localhost:3000/mcp/math",
    "http://localhost:3000/mcp/utils"
]

# Initialize LLM
llm = ChatOpenAI(model="gpt-4", temperature=0)

async def main():
    async with MultiServerMCPClient(MCP_ENDPOINTS) as client:
        await client.initialize()
        print("âœ… Sessions initialized")

        # Print session IDs
        for url, session in client.sessions.items():
            print(f"ðŸ”— {url} â†’ ðŸ†” {session.session_id}")

        # Manual tool call using math session
        math_session = client.sessions["http://localhost:3000/mcp/math"]
        result = await math_session.call_tool("add_numbers", {"a": 10, "b": 5})
        print("ðŸ§® Manual tool call (add_numbers):", result)

        # Load tools for agent
        tools = await load_mcp_tools(client)
        agent = create_react_agent(llm, tools)

        # Agent-based follow-up using previous result
        response = await agent.ainvoke({
            "messages": [("user", f"Now multiply {result} by 3.")]
        })

        print("ðŸ¤– Agent Response:")
        print(response["messages"][-1].content)

if __name__ == "__main__":
    asyncio.run(main())

ðŸ§  What This Demonstrates
Manual call to add_numbers using a specific session.
Agent call that uses the result of the manual call.
Session IDs are printed for transparency and debugging.









---------------------------------------------------------------------------------
mcp servers mounted with fastapi for the two mcp servers in the client example
---------------------------------------------------------------------------------
âœ… 1. Install Required Packages
pip install fastapi uvicorn langchain-mcp-adapters

âœ… 2. MCP Server 1: Math Tools
from fastapi import FastAPI
from langchain_mcp_adapters.server.fastapi import create_mcp_app
from langchain.tools import tool

@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

@tool
def multiply_numbers(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

tools = [add_numbers, multiply_numbers]

app = FastAPI()
app.mount("/mcp", create_mcp_app(tools))

Run it with:
uvicorn server1:app --port 3000

âœ… 3. MCP Server 2: Utility Tools
from fastapi import FastAPI
from langchain_mcp_adapters.server.fastapi import create_mcp_app
from langchain.tools import tool

@tool
def reverse_string(text: str) -> str:
    """Reverse a string."""
    return text[::-1]

@tool
def count_words(text: str) -> int:
    """Count the number of words in a string."""
    return len(text.split())

tools = [reverse_string, count_words]

app = FastAPI()
app.mount("/mcp", create_mcp_app(tools))

Run it with:
uvicorn server2:app --port 3001

ðŸ§  Summary
Server 1 (port 3000): Offers math tools (add_numbers, multiply_numbers)
Server 2 (port 3001): Offers utility tools (reverse_string, count_words)
Both are mounted at /mcp and are compatible with MultiServerMCPClient.

--------------------------------------------------------------------------------------
Combine both servers into two separate mcp endpoints in one single fastapi server
--------------------------------------------------------------------------------------
NOTE: THIS MAY NOT BE ENTIRELY CORRECT CODE

You can absolutely run two separate MCP endpoints on a single FastAPI server by mounting two different MCP apps at different paths (e.g., /mcp/math and /mcp/utils).

âœ… Combined FastAPI Server with Two MCP Endpoints

from fastapi import FastAPI
from langchain_mcp_adapters.server.fastapi import create_mcp_app
from langchain.tools import tool

# Math tools
@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

@tool
def multiply_numbers(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

# Utility tools
@tool
def reverse_string(text: str) -> str:
    """Reverse a string."""
    return text[::-1]

@tool
def count_words(text: str) -> int:
    """Count the number of words in a string."""
    return len(text.split())

# Create MCP apps
math_tools = [add_numbers, multiply_numbers]
utils_tools = [reverse_string, count_words]

math_mcp_app = create_mcp_app(math_tools)
utils_mcp_app = create_mcp_app(utils_tools)

# Main FastAPI app
app = FastAPI()

# Mount each MCP app at a different route
app.mount("/mcp/math", math_mcp_app)
app.mount("/mcp/utils", utils_mcp_app)


ðŸš€ Run the Server
Save this as combined_server.py and run:
uvicorn combined_server:app --port 3000


Youâ€™ll now have:

http://localhost:3000/mcp/math â†’ Math tools
http://localhost:3000/mcp/utils â†’ Utility tools









