==============================
OLLAMA INSTALL USE - WINDOWS
==============================

Refer to https://github.com/coderdba/NOTES/blob/master/ai-ml/LLM/dify/dify-2024-Trial1.txt
- Ollama install/use as part of Dify project

=======================
DOCUMENTATION
=======================
https://github.com/ollama/ollama/blob/main/docs/faq.md
https://github.com/ollama/ollama -- README has a lot of stuff

=======================
ENVIRONMENT VARIABLES
=======================
---------------------
TO RUN OLLAMA SERVER
---------------------
OLLAMA_HOST=127.0.0.1 (setting as 0.0.0.0 causes programs not recognize ollama)
OLLAMA_MODELS=C:\opt\LLM\models
OLLAMA_ORIGINS=http://0.0.0.0

---------------------
INSIDE PROGRAMS
---------------------
OLLAMA_HOST env var is not necessary.  Looks like it is taken from system/user env var instead of .env env var.

=======================
INSTALL OLLAMA SERVER
=======================
----------------
DOWNLOAD OLLAMA
----------------
https://ollama.com/download

-----------
INSTALL
-----------
ollamasetup.exe  /DIR="d:/opt/ollama"
This will pop up a window and installs.

------------
RUN
------------
ollama serve (puts server in foreground)
OR
run the ollama icon in the start menu (will run as daemon)

-------------------
BROWSER ACCESS
-------------------
localhost:11434

------------------------------
LIST MODELS AVAILABLE LOCALLY
------------------------------
http://localhost:11434/api/tags
(INITIALLY NONE)
{"models":[]}

==========================
PULL A MODEL
==========================
Steps from https://github.com/ollama/ollama

ollama pull llama3.2:1b

Model	Parameters	Size	Download
Llama 3.2	3B	2.0GB	ollama run llama3.2
Llama 3.2	1B	1.3GB	ollama run llama3.2:1b
Llama 3.1	8B	4.7GB	ollama run llama3.1
Llama 3.1	70B	40GB	ollama run llama3.1:70b
Llama 3.1	405B	231GB	ollama run llama3.1:405b

==========================
API Calls
==========================
NOTE: Need to supply the model name in the request body

- List models
GET http://localhost:11434/api/tags

- Generate an answer
POST http://localhost:11434/api/generate
with body:
{
  "model": "example",
  "prompt": "Why is the sky blue?"
}
--> The response will be one json for each word in the response sentence which is a bit crazy!!

{
    "model": "example",
    "created_at": "2024-04-25T06:04:28.9559249Z",
    "response": " This",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.1559437Z",
    "response": " question",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.3310589Z",
    "response": " has",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.4957726Z",
    "response": " conf",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.6652588Z",
    "response": "ound",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.8488923Z",
    "response": "ed",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.0221722Z",
    "response": " people",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.1931946Z",
    "response": " for",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.3541808Z",
    "response": " centuries",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.5094005Z",
    "response": ",",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.6723862Z",
    "response": " but",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.8484434Z",
    "response": " the",
    "done": f ... ...





