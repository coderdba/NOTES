==============================
OLLAMA INSTALL USE - WINDOWS
==============================

Refer to https://github.com/coderdba/NOTES/blob/master/ai-ml/LLM/dify/dify-2024-Trial1.txt
- Ollama install/use as part of Dify project

=======================
DOCUMENTATION
=======================
https://github.com/ollama/ollama/blob/main/docs/faq.md
https://github.com/ollama/ollama -- README has a lot of stuff

=======================
ENVIRONMENT VARIABLES
=======================
---------------------
TO RUN OLLAMA SERVER
---------------------
OLLAMA_HOST=127.0.0.1 (setting as 0.0.0.0 causes programs not recognize ollama)
OLLAMA_MODELS=D:\opt\LLM-Models\Ollama
OLLAMA_ORIGINS=http://0.0.0.0

---------------------
INSIDE PROGRAMS
---------------------
OLLAMA_HOST env var is not necessary.  Looks like it is taken from system/user env var instead of .env env var.

=======================
INSTALL OLLAMA SERVER
=======================
----------------
DOWNLOAD OLLAMA
----------------
https://ollama.com/download

-----------
INSTALL
-----------
ollamasetup.exe  /DIR="d:/opt/ollama"
This will pop up a window and installs.

------------
RUN
------------
ollama serve (puts server in foreground)
OR
run the ollama icon in the start menu (will run as daemon)

-------------------
BROWSER ACCESS
-------------------
localhost:11434

------------------------------
LIST MODELS AVAILABLE LOCALLY
------------------------------
http://localhost:11434/api/tags
(INITIALLY NONE)
{"models":[]}

==========================
PULL A MODEL
==========================
Steps from https://github.com/ollama/ollama

- LLM model
ollama pull llama3.2:1b
ollama pull codellama

- Embedding model
ollama pull nomic-embed-text

Model	Parameters	Size	Download
Llama 3.2	3B	2.0GB	ollama run llama3.2
Llama 3.2	1B	1.3GB	ollama run llama3.2:1b
Llama 3.2 Vision	11B	7.9GB	ollama run llama3.2-vision
Llama 3.2 Vision	90B	55GB	ollama run llama3.2-vision:90b
Llama 3.1	8B	4.7GB	ollama run llama3.1
Llama 3.1	70B	40GB	ollama run llama3.1:70b
Llama 3.1	405B	231GB	ollama run llama3.1:405b
Phi 3 Mini	3.8B	2.3GB	ollama run phi3
Phi 3 Medium	14B	7.9GB	ollama run phi3:medium
Gemma 2	2B	1.6GB	ollama run gemma2:2b
Gemma 2	9B	5.5GB	ollama run gemma2
Gemma 2	27B	16GB	ollama run gemma2:27b
Mistral	7B	4.1GB	ollama run mistral
Moondream 2	1.4B	829MB	ollama run moondream
Neural Chat	7B	4.1GB	ollama run neural-chat
Starling	7B	4.1GB	ollama run starling-lm
Code Llama	7B	3.8GB	ollama run codellama
Llama 2 Uncensored	7B	3.8GB	ollama run llama2-uncensored
LLaVA	7B	4.5GB	ollama run llava
Solar	10.7B	6.1GB	ollama run solar

==========================
API Calls
==========================
NOTE: Need to supply the model name in the request body

- List models
GET http://localhost:11434/api/tags

- Generate an answer
POST http://localhost:11434/api/generate
with body:
{
  "model": "example",
  "prompt": "Why is the sky blue?"
}
--> The response will be one json for each word in the response sentence which is a bit crazy!!

{
    "model": "example",
    "created_at": "2024-04-25T06:04:28.9559249Z",
    "response": " This",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.1559437Z",
    "response": " question",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.3310589Z",
    "response": " has",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.4957726Z",
    "response": " conf",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.6652588Z",
    "response": "ound",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:29.8488923Z",
    "response": "ed",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.0221722Z",
    "response": " people",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.1931946Z",
    "response": " for",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.3541808Z",
    "response": " centuries",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.5094005Z",
    "response": ",",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.6723862Z",
    "response": " but",
    "done": false
}
{
    "model": "example",
    "created_at": "2024-04-25T06:04:30.8484434Z",
    "response": " the",
    "done": f ... ...





