Fine Tuning vs Vector Store: https://www.gettingstarted.ai/what-is-the-difference-between-fine-tuning-and-vector-embeddings/
- https://platform.openai.com/docs/guides/fine-tuning/common-use-cases?ref=gettingstarted.ai

Grounding, graph databases:
- https://medium.com/@bukowski.daniel/context-is-everything-how-to-approach-fine-tuning-a-grounded-llm-application-36418c8dc6ad

==================================
LARGE QUESTIONS (copilot answer)
==================================
When dealing with large questions, searching from a vector store can indeed present some challenges. Letâ€™s explore these challenges and potential solutions:

Dimensionality:
Large questions may result in high-dimensional vector representations.
High dimensionality can lead to increased computational costs during similarity searches.
Solution: Use dimensionality reduction techniques (e.g., PCA, t-SNE) to reduce the vector dimensions while preserving relevant information.

Storage and Retrieval:
Large questions require more storage space for their embeddings.
Retrieving similar vectors becomes slower due to the increased search space.
Solution: Optimize storage by using approximate nearest neighbor (ANN) algorithms (e.g., HNSW, FAISS) for faster retrieval.

Query Complexity:
Complex questions may have nuanced semantics that are challenging to capture in embeddings.
Solution: Pre-process questions (e.g., remove stop words, normalize) to simplify their representation.

Vocabulary Variation:
Large questions introduce more diverse vocabulary.
Rare or out-of-vocabulary words may not have well-trained embeddings.
Solution: Use subword embeddings (e.g., FastText, BytePair) to handle unseen words.

Contextual Understanding:
Large questions often require context beyond individual words.
Fixed embeddings (e.g., Word2Vec) may not capture context well.
Solution: Utilize contextual embeddings (e.g., BERT, RoBERTa) that consider surrounding context.

Computational Resources:
Large questions demand more memory and processing power during embedding computation.
Solution: Optimize batch processing and parallelization to efficiently handle large-scale queries.
In summary, while handling large questions can be challenging, thoughtful design, dimensionality reduction, and efficient algorithms can mitigate these issues. If you have any specific use case or need further assistance, feel free to ask! 
