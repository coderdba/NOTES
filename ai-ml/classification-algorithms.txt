https://www.projectpro.io/article/7-types-of-classification-algorithms-in-machine-learning/435#toc-1

Types of Classification Tasks in Machine Learning
Binary Classification
Multi-Class Classification
Multi-Label Classification
Imbalanced Classification

Binary Classification 
- This type of classification involves separating the dataset into two categories. 
  It means that the output variable can only take two values. 
- Ex: spam, not spam
- Algorithms
  K-Nearest Neighbours
  Naive Bayes
  Decision Trees
  Support Vector Machines
  Logistic Regression

Multi-Class Classification
- In multi-class classification, the output variable can have more than two possible values. 
- Example 
  Identifying the flower type in the case of Iris Dataset where 
  we have four input variables: petal length, sepal length, petal width, sepal width, 
  and three possible labels of flowers: Iris Setosa, Iris Versicolor, and Iris Virginica. 

Multi-Label Classification
- This is an extraordinary type of classification task with multiple output variables for each instance from the dataset. 
  That is, one instance can have multiple labels.
- Example In Image classification, a single image may contain more than one object, 
  which can be labeled by the algorithm, like bus, car, person, etc.

Imbalanced Classification
- Imbalanced classification refers to classification problems where the instances of the dataset 
  have a biased or skewed distribution. In other words, one class of input variables has a 
  higher frequency than the others. 
- Imbalanced Classification Example: Detecting fraudulent transactions through a credit card in a transaction dataset.
  Usually, such transactions are remarkably less in number, and this would thus make it difficult for the machine to 
  learn such transactions.

==========================
Here is a list of different types of classifiers in machine learning that you will learn about:
==========================
Naive Bayes Classifier
Logistic Regression
Decision Tree
Random Forests
Support Vector Machines
K-Nearest Neighbour
K-Means Clustering

=========================
Naive Bayes Classifier
=========================
Naive Bayes classifier, is one of the simplest and most effective classification machine learning algorithms. Its basis is Bayes' theorem which describes how the probability of an event is evaluated based on prior knowledge of conditions that might be related to the event. Mathematically, this theorem states-

{P}(Y|X)= {P(X|Y) * P(Y)} / {P(X)}

Where P(Y|X) is the probability of an event Y, given that even X has already occurred. 

P(X) is the probability of event X,

P(Y) is the probability of event Y,

P(X|Y) is the likelihood of event X given a fixed value of Y.

If X represents a feature variable and Y represents a target variable, then the Bayes Classifier will assign that label to the feature that produces the highest probability. For simplicity, consider a two-class problem where the feature variable can have only two possible values, Y=1 or Y=0. Then, the Bayes Classifier will predict class 1 if Pr(Y=1|X=x0) > 0.5, and class two otherwise.

In cases of more than one feature, we can use the following formula for evaluating the probability,
,

P(Y|X1=x1,X2=x2)={P(X1=x1,X2=x2|Y) * P(Y)} / {P(X1=x1,X2=x2)}

where we have assumed that the two features X1 and X2, are independent of each other. In fact, because of this assumption, the word 'Naive' is attached to Bayes' classifier.
