Reference: https://www.2am.tech/blog/integrate-ollama-with-visual-studio-code-for-ai-coding-assistance

===============================
VSCODE PLUGIN - "Continue"
===============================
Install extension/plugin "Continue" on VsCode

Once installed "Continue" with a tick symbol appears on right hand bottom corner.
Click it and choose "Open Chat"
That will take you to configuration.
Choose "Local Assistant" in the left hand top of that panel.

It may ask to add OpenAI model and its api key.
- In that window, at the bottom there is 'stay local' - choose that.
- Then it will show commands to install ollama, models etc

If you have already installed ollama and models, you only have to edit config file.

===============================
CONFIGURE
===============================

CONFIG.YAML - INITIAL 
name: Local Assistant
version: 1.0.0
schema: v1
models:
  - name: Llama 3.1 8B
    provider: ollama
    model: llama3.1:8b
    roles:
      - chat
      - edit
      - apply
  - name: Qwen2.5-Coder 1.5B
    provider: ollama
    model: qwen2.5-coder:1.5b-base
    roles:
      - autocomplete
  - name: Nomic Embed
    provider: ollama
    model: nomic-embed-text:latest
    roles:
      - embed
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase


