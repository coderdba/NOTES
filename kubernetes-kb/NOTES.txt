=================
KUBERNETES NOTES
=================

https://kops.sigs.k8s.io/boot-sequence/

==============
BOOT SEQUENCE
==============
ETCD - from /etc/systemd/system/etcd.service
API Server, Controller Manager, Scheduler - by kubelet service in /usr/lib/systemd/system/kubelet.service and kubelet.service.d

https://kops.sigs.k8s.io/boot-sequence/
On masters:
kube-apiserver
kube-controller-manager (which runs miscellaneous controllers)
kube-scheduler (which assigns pods to nodes)

Also:
etcd --> If it is created as a pod (not in my case where I start etcd by its service)
dns-controller (like coredns) --> this was created afterwards as a pod - so apiserver/etcd know about it already

On nodes:
kube-proxy (which configures iptables so that the k8s-network will work)

==============
ARCHITECTURE
==============
https://kubernetes.io/docs/concepts/overview/components/

Master Nodes - 'control plane'
- API Server
- Controller Manager
- Scheduler
- ETCD
- Optional - cloud controller manager
- Kubelet (optional) - to start the api server, controller manager and scheduler as pods (if configured so - otherwise, configure those to start as services)

Worker/Node
- Kubelet
- Kube-Proxy

Api-Server
- Kubernetes API
- Can have many instances for HA and load-balancing

ETCD
- KV Pair to keep cluster data

Scheduler
- Assigns nodes to pods based on affinity, resource availability and policies

Controller Manager
- Made of several controllers under the main process
- Node controller: Responsible for noticing and responding when nodes go down.
- Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.
- Endpoints controller: Populates the Endpoints object (that is, joins Services & Pods).
- Service Account & Token controllers: Create default accounts and API access tokens for new namespaces

cloud-controller-manager
- cloud-specific control logic
- Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding
- Route controller: For setting up routes in the underlying cloud infrastructure
- Service controller: For creating, updating and deleting cloud provider load balancers

kubelet
- An agent that runs on each node in the cluster. 
- It makes sure that containers are running in a Pod.
- Takes a set of PodSpecs that are provided through various mechanisms 
- Ensures that the containers described in those PodSpecs are running and healthy. 
- Does not manage containers which were not created by Kubernetes

kube-proxy
- Network proxy that runs on each node in your cluste
- Implementing part of the Kubernetes Service concept
- Maintains network rules on nodes
- Network rules allow network communication to your Pods from network sessions - inside or outside of cluster (inside cluster via local inter-pod dns, externally via services)
- Uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself

Container runtime
- Software that is responsible for running containers.
- Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface)

Addons - DNS
- https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
- https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/
- Necessary
- Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.
- Containers started by Kubernetes automatically include this DNS server in their DNS searches
- Example: CoreDNS - with service created as "kube-dns"
- What objects get DNS records? https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
-- Services
-- Pods

Overlay Network
- https://kubernetes.io/docs/concepts/cluster-administration/addons/
- https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727#:~:text=Such%20a%20combination%20of%20virtual,and%20forth%20on%20any%20node.
- Like Flannel, Calico

POD IP Addresses (overlay network)
- https://ronaknathani.com/blog/2020/08/how-a-kubernetes-pod-gets-an-ip-address/ (also has Flannel network IP range confguration)
- Calico configuration: https://docs.projectcalico.org/reference/cni-plugin/configuration (in /etc/cni/net.d/10-calico.conflist --> "ipv4_pools")
- Calico configuration: https://www.ibm.com/docs/en/cloud-private/3.1.2?topic=ins-calico
- https://docs.projectcalico.org/networking/legacy-firewalls
- https://kubernetes.io/docs/concepts/cluster-administration/networking/
- https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/
- CALICO IP Range configuration in calico-etcd.yaml --> This should fall within `--cluster-cidr` 
  From --> https://docs.projectcalico.org/getting-started/kubernetes/installation/config-options
            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
            # chosen from this range. Changing this value after installation will have
            # no effect. This should fall within `--cluster-cidr`.
            # - name: CALICO_IPV4POOL_CIDR
            #   value: "192.168.0.0/16"
            # Disable file logging so `kubectl logs` works.
- SOMETHING ABOUT network-plugin=cni 
-- https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/
-- On VMWare doc: https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/

Kubernetes Network Model:
- Very Good: https://www.ibm.com/docs/en/cloud-private/3.1.2?topic=networking-kubernetes-network-model
As shown in the example, the network_cidr(10.1.0.0/16, which is the default value in config.yaml) and service_cluster_ip_range(10.0.0.1/24, 
which is the default value in config.yaml) conflict with the infrastructure network CIDR 10.0.0.0/8. 
This can break the communication between the pods and the legacy non-cloud-native-workload. 
To avoid such situations, the attributes network_cidr and service_cluster_ip_range in cluster.yaml 
and config.yaml must be set so that they don't conflict with the infrastructure CIDR.
Note: All containers in a single pod share the port space. If container A uses port 80, 
you cannot have container B inside the same pod that uses port 80. Only containers that are on different pods can use the same port number.
- Official doc - says Calico is needed - https://kubernetes.io/docs/concepts/cluster-administration/networking/#kubernetes-model
