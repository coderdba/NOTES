================================================================================
INSTALL AS PODS - SEPARATELY INSTALL EACH POD - NOT WITH INIT
================================================================================

Kubernetes version: 1.21
Docker version:

================
TBD
================
Install etcdctl on model VM

================
OWN REFERENCES
================

This attempt:
- Document: This file
- Scripts repo (shell and ansible): coderdba-coding-org/kubernetes/2021-multinode-as-pods-with-certs-v1.21

Earlier attempt: 
- Single node with certs: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-single-node-pods-with-certs.txt
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-install-as-pods-multinode-no-cert.txt
--- old filename: - https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-install-as-pods-separately.txt
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2020-multinode-as-pods-with-certs-v15.5.txt

Cert generation scripts:
- Own SCRIPTS: https://github.com/coderdba-coding-org/kubernetes/tree/master/2020-multinode-as-pods-with-certs-v15.5/certs
- Own SCRIPTS(older): https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-15.5/roles/certs-gen

- Cert generation With a Makefile: https://github.com/etcd-io/etcd/blob/main/hack/tls-setup/
- Cert generation Manually: https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/
- Own Doc: https://github.com/coderdba-coding-org/kubernetes/tree/master/2020-multinode-as-pods-with-certs-v15.5/certs
- Own Doc: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-single-node-pods-with-certs.txt

K8S Cluster code: (non-ansible)
- https://github.com/coderdba-coding-org/k8s-single-node-vm-as-pods-1
- https://github.com/coderdba-coding-org/k8s-single-node-vm-as-pods-2
- https://github.com/coderdba-coding-org/k8s-multinode-vm-as-pods-1

K8S Cluster code: (ansible)
- Single node with certs: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-1
- Single node with certs: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-15.5
- Node prep: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-machine-prep/roles/yum

Configuration files, shell scripts: https://github.com/coderdba-coding-org/k8s-multinode-vm-as-pods-1
Some basic stuff: https://github.com/coderdba-coding-org/ansible/tree/master/various01/k8s-deploy01
Ansible single node: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-1
Ansible single node: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-15.5

VM Setup: 
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/kub-machines/k8s-model-vm.txt
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-install-as-pods-separately.txt
- https://github.com/coderdba/NOTES/blob/master/docker-kb/install-config-oel-2019.txt

================
OTHER REFERENCES
================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-- REAL MAIN ONES
https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4 (simple docker-image based cluster)
--> Includes setting up using images in 'Option 2 - self hosted'
https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/  (3-node with certs -BUT WITH BINARIES)
--> Use this for concepts and CERTS 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Kubeadm init method:
- https://medium.com/twodigits/install-kubernetes-1-21-1-on-centos-8-stream-include-fix-cap-perfmon-acf23a6879c6

From binaries:
- http://pwittrock.github.io/docs/getting-started-guides/centos/centos_manual_config/

Installing Kubectl, Kubeadm, Kubelet
- https://computingforgeeks.com/manually-pull-container-images-used-by-kubernetes-kubeadm/
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
- https://www.linuxtechi.com/install-kubernetes-1-7-centos7-rhel7/
- https://www.howtoforge.com/tutorial/centos-kubernetes-docker-cluster/

ETCD from binaries (just for reference):
https://computingforgeeks.com/setup-etcd-cluster-on-centos-debian-ubuntu/

ETCD Configuration settings:
https://etcd.io/docs/v3.1/op-guide/configuration/

ETCDCTL:
https://computingforgeeks.com/setup-etcd-cluster-on-centos-debian-ubuntu/

ETCD clustering:
https://etcd.io/docs/v3.5/op-guide/clustering/

ETCD as container:
- https://etcd.io/docs/v3.5/op-guide/container/

ETCD SSL/Certs:
- https://etcd.io/docs/v3.5/op-guide/security/
-- https://github.com/etcd-io/etcd/blob/main/hack/tls-setup
-- https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md

Cert generation with cfssl:
- https://rob-blackbourn.medium.com/how-to-use-cfssl-to-create-self-signed-certificates-d55f76ba5781

Ansible modules: 
- Shell: https://www.middlewareinventory.com/blog/ansible-shell-examples/#Example_1_Execute_a_Single_Command_with_Ansible_Shell
- Shell: https://docs.ansible.com/ansible/latest/collections/ansible/builtin/shell_module.html
- Shell: https://www.middlewareinventory.com/blog/ansible-shell-examples/#Example_7_Execute_multiple_commands_in_a_Single_Shell_Play
- Shell - multiple commands: https://stackoverflow.com/questions/24851575/ansible-how-to-pass-multiple-commands

- Yum: https://docs.ansible.com/ansible/2.3/yum_module.html#examples
- Docker python module: https://stackoverflow.com/questions/53941356/failed-to-import-docker-or-docker-py-no-module-named-docker

Sed:
- http://www.yourownlinux.com/2015/04/sed-command-in-linux-append-and-insert-lines-to-file.html
- https://stackoverflow.com/questions/17998763/sed-commenting-a-line-matching-a-specific-string-and-that-is-not-already-comme/17999003
- https://www.cyberciti.biz/faq/how-to-use-sed-to-find-and-replace-text-in-files-in-linux-unix-shell/

Ansible playbooks:
- Docker: https://faun.pub/configuring-docker-using-ansible-8dc1d9df3f69
- Docker: https://gist.github.com/yonglai/d4617d6914d5f4eb22e4e5a15c0e9a03

===============================================================
PREP MODEL VM - PART 1 - ANSIBLE DOCKER SETUP
===============================================================
---------------------
TO USE DOCKER PLUGIN
---------------------
- SET VARIABLE TO INDICATE PYTHON VERSION IN THE VM (not laptop)
https://stackoverflow.com/questions/53941356/failed-to-import-docker-or-docker-py-no-module-named-docker
Add this ansible_python_interpreter on to your 'hosts' file:

Change [servers:vars] to [your-group-of-server-names:vars]

For Python >= 2.7
[servers:vars]
ansible_python_interpreter=/usr/bin/python3 # For Python3 [default Ubuntu-18.04]

Python <= 2.7
[servers:vars]
ansible_python_interpreter=/usr/bin/python # For Python2.7

- INSTALL REQUIRED MODULES IN VM (not laptop)

-- To avoid:  The error was: No module named docker
# pip install docker==4.4.4 (if using python 2.7 or less)
# pip install docker (if using python 3+ --> docker module should be version 5+)

-- To avoid: The error was: No module named selectors
# pip install selector

----- Also, do this:
https://dockerquestions.com/2021/07/07/ansible-playbook-error-against-remote-host/
      From the above website:
      If you try to use docker module, you’ll notice that its using the “websocket” module, which in turn is using the “selectors” module.

      [root@mybox ~]# python
      Python 2.7.5 (default, Nov 16 2020, 22:23:17)
      [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
      Type “help”, “copyright”, “credits” or “license” for more information.
      >>> import docker
      Traceback (most recent call last):
      File “”, line 1, in
      File “/usr/lib/python2.7/site-packages/docker/__init__.py”, line 2, in
      from .api import APIClient
      File “/usr/lib/python2.7/site-packages/docker/api/__init__.py”, line 2, in
      from .client import APIClient
      File “/usr/lib/python2.7/site-packages/docker/api/client.py”, line 8, in
      import websocket
      File “/usr/lib/python2.7/site-packages/websocket/__init__.py”, line 22, in
      from ._app import WebSocketApp
      File “/usr/lib/python2.7/site-packages/websocket/_app.py”, line 25, in
      import selectors
      ImportError: No module named selectors
      >>>

      Im my case, version 1.1.0 of “websocket-client” was installed. After downgrading it, i was able to use the docker module again,

[root@k9sv112model log]# pip list | grep webs
websocket-client (1.1.1)

[root@k9sv112model log]#  pip install websocket-client==0.57.0
Collecting websocket-client==0.57.0
  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)
    100% |████████████████████████████████| 204kB 454kB/s 
Requirement already satisfied (use --upgrade to upgrade): six in /usr/lib/python2.7/site-packages (from websocket-client==0.57.0)
Installing collected packages: websocket-client
  Found existing installation: websocket-client 1.1.1
    Uninstalling websocket-client-1.1.1:
      Successfully uninstalled websocket-client-1.1.1
Successfully installed websocket-client-0.57.0
You are using pip version 8.1.2, however version 21.2.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.

========================================================
PREP MODEL VM PART 2 - KUBERNETES SOFTWARE
========================================================

----------------------------
VM BASIC SETTINGS
----------------------------
VM hostname: k9sv112model
Memory: 2048 MB (2GB)
Vbox network: vboxnet0
Adapter type: host-only
Adapter name in VM: enp0s8
IP: 192.168.99.100 (pingable from laptop) (set using nmtui command in VM)

----------------------------
COPY SSH PUBLIC KEY TO VM
----------------------------
Copy SSH public key to vm for passwordless logon for manual and Ansible work from laptop.

laptop$ scp id_rsa.pub.gdby root@192.168.99.100:/root/.ssh/authorized-keys
--> asks for root password and then copies it

Try logon now without password:
laptop$ ssh-add ~/.ssh/id_rsa.gdby (the private key)
laptop$ ssh root@192.168.99.100
[root@k9sv112model ~]# 

---------------------------------
DISABLE SELINUX
---------------------------------
# setenforce 0

Edit the file /etc/sysconfig/selinux (or /etc/selinux/config) and set enforcing as disabled

---------------------------------
DISABLE SWAP
---------------------------------
# swapoff -a

Edit /etc/fstab and comment out line of swap
#/dev/mapper/ol-swap     swap                    swap    defaults        0 0

---------------------------------
ENABLE br_netfilter
---------------------------------
# modprobe br_netfilter
# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

Also, put it in /etc/sysctl.conf as follows:
net.bridge.bridge-nf-call-iptables = 1

And, make it persistent:
# sysctl -p

---------------------------------------
INSTALL DOCKER-CE (community edition)
---------------------------------------
- FIRST INSTALL CONTAINER-SELINUX > v2.9 --> say, 2.119
http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.119.1-1.c57a6f9.el7.noarch.rpm
(perviously had used in 2019: http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.68-1.el7.noarch.rpm)

--> Download this, and then do: (dont do rpm -ivh container-selinux-2.74-1.el7.noarch.rpm)
# yum install -y container-selinux-2.119.1-1.c57a6f9.el7.noarch.rpm

- INSTALL DOCKER
Check, and install if needed - dependencies with the following command:
# yum install -y yum-utils device-mapper-persistent-data lvm2

Next, add the Docker-ce repository with the command:
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

Install Docker-ce with the command:
# yum install -y docker-ce

- ENABLE DOCKER SERVICE
# systemctl enable docker

- START DOCKER
# service docker start

- CHECK CGROUP
# docker info | grep -i cgroup
Cgroup Driver: cgroupfs

----------------------------------------
INSTALL CFSSL - TO GENERATE CERTIFICATES
----------------------------------------
https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/

Installation of cfssl
1- Download the binaries.

$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
2- Add the execution permission to the binaries.

$ chmod +x cfssl*
3- Move the binaries to /usr/local/bin.

$ sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl
$ sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
4- Verify the installation.

$ cfssl version

-----------------------------------
FIREWALL - FOR MASTER NODES
-----------------------------------
firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=2379-2380/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload

-----------------------------------
FIREWALL - FOR WORKER NODES
-----------------------------------
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload

--------
VERIFY
--------
# firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp0s3 enp0s8
  sources: 
  services: ssh dhcpv6-client
  ports: 6443/tcp 2379-2380/tcp 10250/tcp 10251/tcp 10252/tcp 10255/tcp
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 

==========================================
PYTHON SETUP - TO USE ANSIBLE DOCKER PULL
==========================================
https://linuxize.com/post/how-to-install-pip-on-centos-7/
yum install epel-release
yum install python-pip

https://docs.ansible.com/ansible/2.8/modules/docker_image_module.html
pip install docker (for python >= 2.7)
pip install docker-py (for python 2.6)

==========================================
PULL DOCKER IMAGES NEEDED FOR KUBERNETES
==========================================

----------------------------------------------------------------------
LIST OF IMAGES FROM CLUSTER WITH KUBEADM-INIT
----------------------------------------------------------------------
[root@ks1 etc]# docker images
REPOSITORY                               TAG        IMAGE ID       CREATED         SIZE
nginx                                    latest     08b152afcfae   2 weeks ago     133MB
nginx                                    <none>     4f380adfc10f   6 weeks ago     133MB
istio/proxyv2                            1.10.2     704d8c3c91a4   6 weeks ago     282MB
istio/pilot                              1.10.2     8c8ea32730d4   6 weeks ago     217MB
k8s.gcr.io/kube-apiserver                v1.21.2    106ff58d4308   7 weeks ago     126MB
k8s.gcr.io/kube-controller-manager       v1.21.2    ae24db9aa2cc   7 weeks ago     120MB
k8s.gcr.io/kube-proxy                    v1.21.2    a6ebd1c1ad98   7 weeks ago     131MB
k8s.gcr.io/kube-scheduler                v1.21.2    f917b8c8f55b   7 weeks ago     50.6MB
calico/node                              v3.19.1    c4d75af7e098   2 months ago    168MB
calico/pod2daemon-flexvol                v3.19.1    5660150975fb   2 months ago    21.7MB
calico/cni                               v3.19.1    5749e8b276f9   2 months ago    146MB
calico/kube-controllers                  v3.19.1    5d3d5ddc8605   2 months ago    60.6MB
k8s.gcr.io/pause                         3.4.1      0f8457a4c2ec   6 months ago    683kB
k8s.gcr.io/coredns/coredns               v1.8.0     296a6d5035e2   9 months ago    42.5MB
busybox                                  latest     f0b02e9d092d   9 months ago    1.23MB
nginx                                    <none>     f35646e83998   9 months ago    133MB
k8s.gcr.io/etcd                          3.4.13-0   0369cf4303ff   11 months ago   253MB
istio/examples-bookinfo-reviews-v3       1.16.2     83e6a8464b84   13 months ago   694MB
istio/examples-bookinfo-reviews-v2       1.16.2     39cff5d782e1   13 months ago   694MB
istio/examples-bookinfo-reviews-v1       1.16.2     181be23dc1af   13 months ago   694MB
istio/examples-bookinfo-ratings-v1       1.16.2     99ce598b98cf   13 months ago   161MB
istio/examples-bookinfo-details-v1       1.16.2     edf6b9bea3db   13 months ago   149MB
istio/examples-bookinfo-productpage-v1   1.16.2     7f1e097aad6d   13 months ago   207MB

-----------------------------------
PULL IMAGES
-----------------------------------
- DOCKER IMAGE REPOS
https://console.cloud.google.com/gcr/images/google-containers/GLOBAL
https://quay.io/repository/jcmoraisjr/haproxy-ingress?tag=latest&tab=tags

- ETCD
# docker pull quay.io/coreos/etcd:latest
or
# docker pull quay.io/coreos/etcd:3.4.13-0

- MASTER (with additional components for workers also)
--- FOR SPECIFIC VERSIONS
https://kubernetes.io/docs/setup/release/notes/ 
gcr.io/google_containers/kube-proxy-amd64(and such)

-- REQUIRED
docker pull k8s.gcr.io/etcd:3.4.13-0
docker pull k8s.gcr.io/kube-apiserver:v1.21.2
docker pull k8s.gcr.io/kube-scheduler:v1.21.2
docker pull k8s.gcr.io/kube-controller-manager:v1.21.2
docker pull k8s.gcr.io/kube-proxy:v1.21.2
docker pull calico/node:v3.19.1
docker pull calico/pod2daemon-flexvol:v3.19.1
docker pull calico/cni:v3.19.1
docker pull calico/kube-controllers:v3.19.1
docker pull k8s.gcr.io/coredns/coredns:v1.8.0
docker pull k8s.gcr.io/pause-amd64:3.1

-- ADDITIONAL (TBD)
docker pull quay.io/jcmoraisjr/haproxy-ingress:v0.12.7
- older docker pull quay.io/jcmoraisjr/haproxy-ingress:v0.12.6 --> to make common vm image for master and worker
- https://quay.io/repository/jcmoraisjr/haproxy-ingress?tag=latest&tab=tags
docker pull gcr.io/google-containers/kube-addon-manager-amd64:v9.1.1
docker pull bitnami/metrics-server:0.5.0
- old docker pull gcr.io/google-containers/metrics-server-amd64:v0.3.6
docker pull gcr.io/google-containers/rescheduler:v0.4.0

-- DID NOT WORK
docker pull k8s.gcr.io/pause-amd64:3.4.1

-----------------------------------
INSTALL KUBEADM, KUBECTL, KUBELET
-----------------------------------
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
https://www.linuxtechi.com/install-kubernetes-1-7-centos7-rhel7/
https://www.howtoforge.com/tutorial/centos-kubernetes-docker-cluster/

- STEPS FROM https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# Set SELinux in permissive mode (effectively disabling it)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

--> For latest versions: sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
--> For specific versions: sudo yum install -y kubelet-1.21.2-0.x86_64 kubeadm-1.21.2-0.x86_64 kubectl-1.22.0-0.x86_64 --disableexcludes=kubernetes
            The following get installed (cni as well gets installed)
            kubeadm-1.21.2-0.x86_64
            kubelet-1.21.2-0.x86_64
            kubernetes-cni-0.8.7-0.x86_64
            kubectl-1.22.0-0.x86_64
            
- KUBELET SERVICE STATUS
# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: inactive (dead)
     Docs: https://kubernetes.io/docs/
     
- ENABLE KUBELET SERVICE
sudo systemctl enable --now kubelet

---------------------
INSTALL ETCDCTL
---------------------
NOTE: Do only if it is necessary - we can always use docker exec to run the etcdctl command from within the etcd container itself
- With yum: yum install etcd - not sure which repo
- Tar/zip: https://computingforgeeks.com/setup-etcd-cluster-on-centos-debian-ubuntu/

============================================================================
ETCD - TRY JUST ETCD - WITH NO CERTIFICATES - to learn, and verify vm setup
============================================================================

https://etcd.io/docs/v3.5/op-guide/container
  
---------------
PREP VM
---------------
Clone the model machine
VM hostname: ksn1 (sn for single-node)
Memory: 2048 MB (2GB)
Vbox network: vboxnet0
Adapter type: host-only
Adapter name in VM: enp0s8
IP: 192.168.99.101 (pingable from laptop) (set using nmtui command in VM)

Running a single node etcd
Use the host IP address when configuring etcd:

---------------------------
RUN ETCD - FOREGROUND
---------------------------
NOTE: This makes etcd docker command run in foreground - on command line - NOT AS A SERVICE
      Keep an additional terminal/tab to do verifications
      
Modified steps from https://etcd.io/docs/v3.5/op-guide/container
--> Kept the steps and commands, changed IP and such

- Configure a Docker volume to store etcd data:
NOTE: The folder on host/node for this volume will be: /var/lib/docker/volumes/etcd-data
      And, in the etcd container it will be: /etcd-data 
            --> could not view it in the container as 'docker exec -ti /bin/sh' took me to the docker container, but it did not have 'ls' command

# docker volume create --name etcd-data
# docker volume ls
DRIVER    VOLUME NAME
local     etcd-data


- Run etcd:
export NODE1=192.168.99.101
export DATA_DIR="etcd-data"
export ETCD_IMAGE="k8s.gcr.io/etcd:3.4.13-0"

docker run \
  -p 2379:2379 \
  -p 2380:2380 \
  --volume=${DATA_DIR}:/etcd-data \
  --name etcd ${ETCD_IMAGE} \
  /usr/local/bin/etcd \
  --data-dir=/etcd-data --name node1 \
  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \
  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \
  --initial-cluster node1=http://${NODE1}:2380

- MESSAGES (on screen)
raft2021/08/13 05:44:24 INFO: d5673e1f00b32e05 is starting a new election at term 1
raft2021/08/13 05:44:24 INFO: d5673e1f00b32e05 became candidate at term 2
raft2021/08/13 05:44:24 INFO: d5673e1f00b32e05 received MsgVoteResp from d5673e1f00b32e05 at term 2
raft2021/08/13 05:44:24 INFO: d5673e1f00b32e05 became leader at term 2
raft2021/08/13 05:44:24 INFO: raft.node: d5673e1f00b32e05 elected leader d5673e1f00b32e05 at term 2
2021-08-13 05:44:24.339696 I | etcdserver: setting up the initial cluster version to 3.4
2021-08-13 05:44:24.339748 I | etcdserver: published {Name:node1 ClientURLs:[http://192.168.99.101:2379]} to cluster a28fae6205c8aca6
2021-08-13 05:44:24.340086 I | embed: ready to serve client requests
2021-08-13 05:44:24.340859 N | embed: serving insecure client requests on [::]:2379, this is strongly discouraged!
2021-08-13 05:44:24.341438 N | etcdserver/membership: set the initial cluster version to 3.4
2021-08-13 05:44:24.341481 I | etcdserver/api: enabled capabilities for version 3.4

- VERIFY
# docker ps -a
CONTAINER ID   IMAGE                      COMMAND                  CREATED          STATUS          PORTS                                                                               NAMES
fa46a547646c   k8s.gcr.io/etcd:3.4.13-0   "/usr/local/bin/etcd…"   51 seconds ago   Up 49 seconds   4001/tcp, 0.0.0.0:2379-2380->2379-2380/tcp, :::2379-2380->2379-2380/tcp, 7001/tcp   etcd

- VERIFY - List the cluster member:

-- WITH ETCDCTL ON THE NODE
# etcdctl --endpoints=http://${NODE1}:2379 member list
# etcdctl --endpoints=http://${192.168.99.101}:2379 member list
--> These did not work yet as etcdctl was not yet installed on the VM/node

-- WITH ETCDCTL IN THE ETCD CONTAINER 
Reference: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-install-as-pods-multinode-no-cert.txt
This does "docker exec" into pod name "etcd" and runs etcdctl from within the pod

# docker exec etcd /bin/sh -c "export ETCDCTL_API=3 && /usr/local/bin/etcdctl member list"
d5673e1f00b32e05, started, node1, http://192.168.99.101:2380, http://192.168.99.101:2379, false

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- REFERENCE - STEPS AS IN DOC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
https://etcd.io/docs/v3.5/op-guide/container

            Running a single node etcd
            Use the host IP address when configuring etcd:

            export NODE1=192.168.1.21
            Configure a Docker volume to store etcd data:

            docker volume create --name etcd-data
            export DATA_DIR="etcd-data"

            Run the latest version of etcd:

            REGISTRY=quay.io/coreos/etcd
            # available from v3.2.5
            REGISTRY=gcr.io/etcd-development/etcd

            docker run \
              -p 2379:2379 \
              -p 2380:2380 \
              --volume=${DATA_DIR}:/etcd-data \
              --name etcd ${REGISTRY}:latest \
              /usr/local/bin/etcd \
              --data-dir=/etcd-data --name node1 \
              --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \
              --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \
              --initial-cluster node1=http://${NODE1}:2380

            List the cluster member:
            etcdctl --endpoints=http://${NODE1}:2379 member list

----------------------
ETCD SERVICE
----------------------
Reference: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-install-as-pods-multinode-no-cert.txt

To start etcd container when host machine starts - create a systemd service.

----
File: /etc/environment
----
# ETCD RELATED ENVIRONMENT 

REGISTRY=quay.io/coreos/etcd
# available from v3.2.5
# REGISTRY=gcr.io/etcd-development/etcd

# For each machine
#ETCD_VERSION=latest
ETCD_VERSION=3.4.13-0

# Note: TOKEN can be any string
TOKEN=my-etcd-token
CLUSTER_STATE=new
CLUSTER="ksn1=http://192.168.99.101:2380"
DATA_DIR="etcd-data"
THIS_NAME=ksn1
THIS_IP=192.168.99.101

----
File: /etc/systemd/system/etcd.service:
----
[Unit]
Description=etcd
Documentation=https://github.com/coreos
Wants=docker.service

[Service]
Type=simple
User=root
Group=root
IOSchedulingClass=2
IOSchedulingPriority=0
EnvironmentFile=/etc/environment

# START ETCD
ExecStart=/usr/bin/docker run --rm\
  --net=host \
  -p 2379:2379 \
  -p 2380:2380 \
  --volume=${DATA_DIR}:/etcd-data \
  --name etcd ${REGISTRY}:${ETCD_VERSION} \
  /usr/local/bin/etcd \
  --data-dir=/etcd-data --name ${THIS_NAME} \
  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \
  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \
  --initial-cluster ${CLUSTER} \
  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target

- START ETCD SERVICE
-- Reload the daemon configuration.
# systemctl daemon-reload

-- Enable etcd to start at boot time.
# systemctl enable etcd

-- Start etcd.
# systemctl start etcd

-- Verify
# journalctl -xe 
Aug 13 14:53:19 ksn1 docker[21061]: raft2021/08/13 09:23:19 INFO: raft.node: d5673e1f00b32e05 elected leader d5673e1f00b
Aug 13 14:53:19 ksn1 docker[21061]: 2021-08-13 09:23:19.912468 I | etcdserver: published {Name:ksn1 ClientURLs:[http://1
Aug 13 14:53:19 ksn1 docker[21061]: 2021-08-13 09:23:19.912680 I | embed: ready to serve client requests
Aug 13 14:53:19 ksn1 docker[21061]: 2021-08-13 09:23:19.913499 N | embed: serving insecure client requests on [::]:2379

-- Verify
# docker exec etcd /bin/sh -c "export ETCDCTL_API=3 && /usr/local/bin/etcdctl member list"
d5673e1f00b32e05, started, ksn1, http://192.168.99.101:2380, http://192.168.99.101:2379, false


=====================================================
GENERATE CERTIFICATES
=====================================================
Cert generation With a Makefile: https://github.com/etcd-io/etcd/blob/main/hack/tls-setup/
Cert generation Manually: https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/

Own Doc: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-single-node-pods-with-certs.txt
--> Much of the stuff in this section is based on this repo
Own SCRIPTS: https://github.com/coderdba-coding-org/kubernetes/tree/master/2020-multinode-as-pods-with-certs-v15.5/certs
--> The stuff in this section is based on this repo

Own SCRIPTS(older): https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-15.5/roles/certs-gen

---------------------------------------------------
LIST OF CERTIFICATES THAT WE WILL CREATE
---------------------------------------------------
CA Certificate and Key - certificate authority public cert, private key
Admin client cert - To connect to Kubernetes cluster as admin from another machine (Note: This is not required to create the cluster itself)
Kubelet client cert - For kubelet on each worker node to join the Kubernetes cluster
Kube-Proxy client cert - For kube-proxy on each worker node to join the Kubernetes cluster
API-Server cert - For kube-apiserver
Etcd cert - use API-Server cert itself - OR - GENERATE A DIFFERENT ONES - 1. CLIENT AND 2. PEER CERTS 
--> For etcd certs: https://etcd.io/docs/v3.5/op-guide/security/,  https://github.com/etcd-io/etcd/blob/main/hack/tls-setup/

-----------------
CA CERTIFICATE
-----------------
This will produce certificate-authority certificate - to use while generating other certificates

- Create the certificate authority configuration file
ca-config.json file:

{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "87600h"
      }
    }
  }
}

- Create the certificate authority signing request configuration file.
ca-csr.json file:

{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "ABC",
      "L": "ABC",
      "O": "Kubernetes",
      "OU": "Kubernetes",
      "ST": "ABC"
    }
  ]
}


- Generate the certificate authority certificate and private key (CA Cert and CA Key)
Run command: (ca-gen.sh)

# cfssl gencert -initca ca-csr.json | cfssljson -bare ca

2021/08/13 15:55:58 [INFO] generating a new CA key and certificate from CSR
2021/08/13 15:55:58 [INFO] generate received request
2021/08/13 15:55:58 [INFO] received CSR
2021/08/13 15:55:58 [INFO] generating key: rsa-2048
2021/08/13 15:55:58 [INFO] encoded CSR
2021/08/13 15:55:58 [INFO] signed certificate with serial number 488749427361051917701321279809948885154880231308

- It will produce these:
ca.csr - file containing the csr (certificate signing request)
ca.pem - ca certificate (public)
ca-key.pem - ca key (private)

--------------------------
ADMIN-CLIENT CERTIFICATE
--------------------------
This certificate will be used to connect to the Kubernetes cluster as an administrator.
NOTE:  No IP/hostname needed for this

- Create the admin-client csr config file
File: admin-csr.json
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "ABC",
      "L": "ABC",
      "O": "system:masters",
      "OU": "Kubernetes",
      "ST": "ABC"
    }
  ]
}

- Run the command to generate the admin-client certs
Script file: admin-gen.sh

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-profile=kubernetes admin-csr.json | \
cfssljson -bare admin

# ./admin-gen.sh
2021/08/13 16:35:16 [INFO] generate received request
2021/08/13 16:35:16 [INFO] received CSR
2021/08/13 16:35:16 [INFO] generating key: rsa-2048
2021/08/13 16:35:17 [INFO] encoded CSR
2021/08/13 16:35:17 [INFO] signed certificate with serial number 727036575979191653126949921137901971843610903400
2021/08/13 16:35:17 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

- It will produce these files
admin.pem - cert (public)
admin-key.pem - key (private)
admin.csr - file containing the csr (certificate signing request)

------------------------------------------------------
KUBELET-CLIENT CERTIFICATE (for worker nodes)
------------------------------------------------------
Worker-node kubelets will need a certificate to join the Kubernetes cluster
NOTE: The IP is for the specific WORKER node

- Configuration files
File kubelet-node1-csr.json file:
{
  "CN": "system:node:192.168.99.101",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "ABC",
      "L": "ABC",
      "O": "system:nodes",
      "OU": "Kubernetes",
      "ST": "ABC"
    }
  ]
}

File kubelet-node2-csr.json file:
{
  "CN": "system:node:192.168.99.102",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "ABC",
      "L": "ABC",
      "O": "system:nodes",
      "OU": "Kubernetes",
      "ST": "ABC"
    }
  ]
}

File kubelet-node3-csr.json file:
{
  "CN": "system:node:192.168.99.103",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "ABC",
      "L": "ABC",
      "O": "system:nodes",
      "OU": "Kubernetes",
      "ST": "ABC"
    }
  ]
}

- Cert generator command file
Note: In hostname 'k1', 'k2', 'k3' represent hostnames alongwith IP addresses
--> modify IP and hostname as needed
--> If creating a single-node cluster, then retain only one command call

File: kubelet-gen.sh

# 3-node model commands below
# Uncomment and change IP and hostname as needed
# If a single-node cluster, then copy just one and change IP and hostname as needed
#cfssl gencert \
#-ca=ca.pem \
#-ca-key=ca-key.pem \
#-config=ca-config.json \
#-hostname=192,168.99.201,k1 \
#-profile=kubernetes kubelet-k1-csr.json | \
#cfssljson -bare kubelet-k1
#
#cfssl gencert \
#-ca=ca.pem \
#-ca-key=ca-key.pem \
#-config=ca-config.json \
#-hostname=192,168.99.202,k2 \
#-profile=kubernetes kubelet-k2-csr.json | \
#cfssljson -bare kubelet-k2
#
#cfssl gencert \
#-ca=ca.pem \
#-ca-key=ca-key.pem \
#-config=ca-config.json \
#-hostname=192,168.99.203,k3 \
#-profile=kubernetes kubelet-k3-csr.json | \
#cfssljson -bare kubelet-k3

# Single node
cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-hostname=192,168.99.101,ksn1 \
-profile=kubernetes kubelet-ksn1-csr.json | \
cfssljson -bare kubelet-ksn1

- For single-node you get these files
kubelet-ksn1.pem - cert (public)
kubelet-ksn1-key.pem - key (private)
kubelet-ksn1.csr - cert signing request

=====================================================
ETCD WITH CERTS
=====================================================
