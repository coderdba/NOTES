================================================================================
INSTALL AS PODS - SEPARATELY INSTALL EACH POD - NOT WITH INIT
================================================================================

Kubernetes version: 1.21
Docker version:

================
OWN REFERENCES
================

Earlier attempt: 
- Single node with certs: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-single-node-pods-with-certs.txt
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2019-install-as-pods-multinode-no-cert.txt
--- old filename: - https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-install-as-pods-separately.txt
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-2020-multinode-as-pods-with-certs-v15.5.txt

Cert generation scripts:
- https://github.com/coderdba-coding-org/kubernetes/tree/master/2020-multinode-as-pods-with-certs-v15.5/certs

K8S Cluster code: (non-ansible)
- https://github.com/coderdba-coding-org/k8s-single-node-vm-as-pods-1
- https://github.com/coderdba-coding-org/k8s-single-node-vm-as-pods-2
- https://github.com/coderdba-coding-org/k8s-multinode-vm-as-pods-1

K8S Cluster code: (ansible)
- Single node with certs: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-1
- Single node with certs: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-15.5
- Node prep: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-machine-prep/roles/yum

Configuration files, shell scripts: https://github.com/coderdba-coding-org/k8s-multinode-vm-as-pods-1
Some basic stuff: https://github.com/coderdba-coding-org/ansible/tree/master/various01/k8s-deploy01
Ansible single node: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-1
Ansible single node: https://github.com/coderdba-coding-org/code1/tree/master/kubernetes/kube-vbox-ans-singlenode-15.5

VM Setup: 
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/kub-machines/k8s-model-vm.txt
- https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-install-as-pods-separately.txt
- https://github.com/coderdba/NOTES/blob/master/docker-kb/install-config-oel-2019.txt

================
OTHER REFERENCES
================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-- REAL MAIN ONES
https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4 (simple docker-image based cluster)
--> Includes setting up using images in 'Option 2 - self hosted'
https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/  (3-node with certs -BUT WITH BINARIES)
--> Use this for concepts and CERTS 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Kubeadm init method:
- https://medium.com/twodigits/install-kubernetes-1-21-1-on-centos-8-stream-include-fix-cap-perfmon-acf23a6879c6

Installing Kubectl, Kubeadm, Kubelet
- https://computingforgeeks.com/manually-pull-container-images-used-by-kubernetes-kubeadm/
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
- https://www.linuxtechi.com/install-kubernetes-1-7-centos7-rhel7/
- https://www.howtoforge.com/tutorial/centos-kubernetes-docker-cluster/

ETCD as container:
- https://etcd.io/docs/v3.5/op-guide/container/

ETCD SSL/Certs:
- https://etcd.io/docs/v3.5/op-guide/security/
-- https://github.com/etcd-io/etcd/blob/main/hack/tls-setup
-- https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md

Cert generation with cfssl:
- https://rob-blackbourn.medium.com/how-to-use-cfssl-to-create-self-signed-certificates-d55f76ba5781

Ansible modules: 
- Shell: https://www.middlewareinventory.com/blog/ansible-shell-examples/#Example_1_Execute_a_Single_Command_with_Ansible_Shell
- Shell: https://docs.ansible.com/ansible/latest/collections/ansible/builtin/shell_module.html
- Shell: https://www.middlewareinventory.com/blog/ansible-shell-examples/#Example_7_Execute_multiple_commands_in_a_Single_Shell_Play
- Shell - multiple commands: https://stackoverflow.com/questions/24851575/ansible-how-to-pass-multiple-commands

- Yum: https://docs.ansible.com/ansible/2.3/yum_module.html#examples
- Docker python module: https://stackoverflow.com/questions/53941356/failed-to-import-docker-or-docker-py-no-module-named-docker

Sed:
- http://www.yourownlinux.com/2015/04/sed-command-in-linux-append-and-insert-lines-to-file.html
- https://stackoverflow.com/questions/17998763/sed-commenting-a-line-matching-a-specific-string-and-that-is-not-already-comme/17999003
- https://www.cyberciti.biz/faq/how-to-use-sed-to-find-and-replace-text-in-files-in-linux-unix-shell/

Ansible playbooks:
- Docker: https://faun.pub/configuring-docker-using-ansible-8dc1d9df3f69
- Docker: https://gist.github.com/yonglai/d4617d6914d5f4eb22e4e5a15c0e9a03

===============================================================
PREP MODEL VM - PART 1 - ANSIBLE DOCKER SETUP
===============================================================
---------------------
TO USE DOCKER PLUGIN
---------------------
- SET VARIABLE TO INDICATE PYTHON VERSION IN THE VM (not laptop)
https://stackoverflow.com/questions/53941356/failed-to-import-docker-or-docker-py-no-module-named-docker
Add this ansible_python_interpreter on to your 'hosts' file:

Change [servers:vars] to [your-group-of-server-names:vars]

For Python >= 2.7
[servers:vars]
ansible_python_interpreter=/usr/bin/python3 # For Python3 [default Ubuntu-18.04]

Python <= 2.7
[servers:vars]
ansible_python_interpreter=/usr/bin/python # For Python2.7

- INSTALL REQUIRED MODULES IN VM (not laptop)

-- To avoid:  The error was: No module named docker
# pip install docker==4.4.4 (if using python 2.7 or less)
# pip install docker (if using python 3+ --> docker module should be version 5+)

-- To avoid: The error was: No module named selectors
# pip install selector

----- Also, do this:
https://dockerquestions.com/2021/07/07/ansible-playbook-error-against-remote-host/
      From the above website:
      If you try to use docker module, you’ll notice that its using the “websocket” module, which in turn is using the “selectors” module.

      [root@mybox ~]# python
      Python 2.7.5 (default, Nov 16 2020, 22:23:17)
      [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
      Type “help”, “copyright”, “credits” or “license” for more information.
      >>> import docker
      Traceback (most recent call last):
      File “”, line 1, in
      File “/usr/lib/python2.7/site-packages/docker/__init__.py”, line 2, in
      from .api import APIClient
      File “/usr/lib/python2.7/site-packages/docker/api/__init__.py”, line 2, in
      from .client import APIClient
      File “/usr/lib/python2.7/site-packages/docker/api/client.py”, line 8, in
      import websocket
      File “/usr/lib/python2.7/site-packages/websocket/__init__.py”, line 22, in
      from ._app import WebSocketApp
      File “/usr/lib/python2.7/site-packages/websocket/_app.py”, line 25, in
      import selectors
      ImportError: No module named selectors
      >>>

      Im my case, version 1.1.0 of “websocket-client” was installed. After downgrading it, i was able to use the docker module again,

[root@k9sv112model log]# pip list | grep webs
websocket-client (1.1.1)

[root@k9sv112model log]#  pip install websocket-client==0.57.0
Collecting websocket-client==0.57.0
  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)
    100% |████████████████████████████████| 204kB 454kB/s 
Requirement already satisfied (use --upgrade to upgrade): six in /usr/lib/python2.7/site-packages (from websocket-client==0.57.0)
Installing collected packages: websocket-client
  Found existing installation: websocket-client 1.1.1
    Uninstalling websocket-client-1.1.1:
      Successfully uninstalled websocket-client-1.1.1
Successfully installed websocket-client-0.57.0
You are using pip version 8.1.2, however version 21.2.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.

========================================================
PREP MODEL VM PART 2 - KUBERNETES SOFTWARE
========================================================

----------------------------
VM BASIC SETTINGS
----------------------------
VM hostname: k9sv112model
Memory: 2048 MB (2GB)
Vbox network: vboxnet0
Adapter type: host-only
Adapter name in VM: enp0s8
IP: 192.168.99.100 (pingable from laptop) (set using nmtui command in VM)

----------------------------
COPY SSH PUBLIC KEY TO VM
----------------------------
Copy SSH public key to vm for passwordless logon for manual and Ansible work from laptop.

laptop$ scp id_rsa.pub.gdby root@192.168.99.100:/root/.ssh/authorized-keys
--> asks for root password and then copies it

Try logon now without password:
laptop$ ssh-add ~/.ssh/id_rsa.gdby (the private key)
laptop$ ssh root@192.168.99.100
[root@k9sv112model ~]# 

---------------------------------
DISABLE SELINUX
---------------------------------
# setenforce 0

Edit the file /etc/sysconfig/selinux (or /etc/selinux/config) and set enforcing as disabled

---------------------------------
DISABLE SWAP
---------------------------------
# swapoff -a

Edit /etc/fstab and comment out line of swap
#/dev/mapper/ol-swap     swap                    swap    defaults        0 0

---------------------------------
ENABLE br_netfilter
---------------------------------
# modprobe br_netfilter
# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

Also, put it in /etc/sysctl.conf as follows:
net.bridge.bridge-nf-call-iptables = 1

And, make it persistent:
# sysctl -p

---------------------------------------
INSTALL DOCKER-CE (community edition)
---------------------------------------
- FIRST INSTALL CONTAINER-SELINUX > v2.9 --> say, 2.119
http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.119.1-1.c57a6f9.el7.noarch.rpm
(perviously had used in 2019: http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.68-1.el7.noarch.rpm)

--> Download this, and then do: (dont do rpm -ivh container-selinux-2.74-1.el7.noarch.rpm)
# yum install -y container-selinux-2.119.1-1.c57a6f9.el7.noarch.rpm

- INSTALL DOCKER
Check, and install if needed - dependencies with the following command:
# yum install -y yum-utils device-mapper-persistent-data lvm2

Next, add the Docker-ce repository with the command:
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

Install Docker-ce with the command:
# yum install -y docker-ce

- ENABLE DOCKER SERVICE
# systemctl enable docker

- START DOCKER
# service docker start

- CHECK CGROUP
# docker info | grep -i cgroup
Cgroup Driver: cgroupfs

----------------------------------------
INSTALL CFSSL - TO GENERATE CERTIFICATES
----------------------------------------
https://blog.inkubate.io/deploy-kubernetes-1-9-from-scratch-on-vmware-vsphere/

Installation of cfssl
1- Download the binaries.

$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
2- Add the execution permission to the binaries.

$ chmod +x cfssl*
3- Move the binaries to /usr/local/bin.

$ sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl
$ sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
4- Verify the installation.

$ cfssl version

-----------------------------------
FIREWALL - FOR MASTER NODES
-----------------------------------
firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=2379-2380/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload

-----------------------------------
FIREWALL - FOR WORKER NODES
-----------------------------------
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload

--------
VERIFY
--------
# firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp0s3 enp0s8
  sources: 
  services: ssh dhcpv6-client
  ports: 6443/tcp 2379-2380/tcp 10250/tcp 10251/tcp 10252/tcp 10255/tcp
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 

==========================================
PYTHON SETUP - TO USE ANSIBLE DOCKER PULL
==========================================
https://linuxize.com/post/how-to-install-pip-on-centos-7/
yum install epel-release
yum install python-pip

https://docs.ansible.com/ansible/2.8/modules/docker_image_module.html
pip install docker (for python >= 2.7)
pip install docker-py (for python 2.6)

==========================================
PULL DOCKER IMAGES NEEDED FOR KUBERNETES
==========================================

----------------------------------------------------------------------
LIST OF IMAGES FROM CLUSTER WITH KUBEADM-INIT
----------------------------------------------------------------------
[root@ks1 etc]# docker images
REPOSITORY                               TAG        IMAGE ID       CREATED         SIZE
nginx                                    latest     08b152afcfae   2 weeks ago     133MB
nginx                                    <none>     4f380adfc10f   6 weeks ago     133MB
istio/proxyv2                            1.10.2     704d8c3c91a4   6 weeks ago     282MB
istio/pilot                              1.10.2     8c8ea32730d4   6 weeks ago     217MB
k8s.gcr.io/kube-apiserver                v1.21.2    106ff58d4308   7 weeks ago     126MB
k8s.gcr.io/kube-controller-manager       v1.21.2    ae24db9aa2cc   7 weeks ago     120MB
k8s.gcr.io/kube-proxy                    v1.21.2    a6ebd1c1ad98   7 weeks ago     131MB
k8s.gcr.io/kube-scheduler                v1.21.2    f917b8c8f55b   7 weeks ago     50.6MB
calico/node                              v3.19.1    c4d75af7e098   2 months ago    168MB
calico/pod2daemon-flexvol                v3.19.1    5660150975fb   2 months ago    21.7MB
calico/cni                               v3.19.1    5749e8b276f9   2 months ago    146MB
calico/kube-controllers                  v3.19.1    5d3d5ddc8605   2 months ago    60.6MB
k8s.gcr.io/pause                         3.4.1      0f8457a4c2ec   6 months ago    683kB
k8s.gcr.io/coredns/coredns               v1.8.0     296a6d5035e2   9 months ago    42.5MB
busybox                                  latest     f0b02e9d092d   9 months ago    1.23MB
nginx                                    <none>     f35646e83998   9 months ago    133MB
k8s.gcr.io/etcd                          3.4.13-0   0369cf4303ff   11 months ago   253MB
istio/examples-bookinfo-reviews-v3       1.16.2     83e6a8464b84   13 months ago   694MB
istio/examples-bookinfo-reviews-v2       1.16.2     39cff5d782e1   13 months ago   694MB
istio/examples-bookinfo-reviews-v1       1.16.2     181be23dc1af   13 months ago   694MB
istio/examples-bookinfo-ratings-v1       1.16.2     99ce598b98cf   13 months ago   161MB
istio/examples-bookinfo-details-v1       1.16.2     edf6b9bea3db   13 months ago   149MB
istio/examples-bookinfo-productpage-v1   1.16.2     7f1e097aad6d   13 months ago   207MB

-----------------------------------
PULL IMAGES
-----------------------------------
- DOCKER IMAGE REPOS
https://console.cloud.google.com/gcr/images/google-containers/GLOBAL
https://quay.io/repository/jcmoraisjr/haproxy-ingress?tag=latest&tab=tags

- ETCD
# docker pull quay.io/coreos/etcd:latest
or
# docker pull quay.io/coreos/etcd:3.4.13-0

- MASTER (with additional components for workers also)
--- FOR SPECIFIC VERSIONS
https://kubernetes.io/docs/setup/release/notes/ 
gcr.io/google_containers/kube-proxy-amd64(and such)

-- REQUIRED
docker pull k8s.gcr.io/etcd:3.4.13-0
docker pull k8s.gcr.io/kube-apiserver:v1.21.2
docker pull k8s.gcr.io/kube-scheduler:v1.21.2
docker pull k8s.gcr.io/kube-controller-manager:v1.21.2
docker pull k8s.gcr.io/kube-proxy:v1.21.2
docker pull calico/node:v3.19.1
docker pull calico/pod2daemon-flexvol:v3.19.1
docker pull calico/cni:v3.19.1
docker pull calico/kube-controllers:v3.19.1
docker pull k8s.gcr.io/coredns/coredns:v1.8.0
docker pull k8s.gcr.io/pause-amd64:3.1

-- ADDITIONAL (TBD)
docker pull quay.io/jcmoraisjr/haproxy-ingress:v0.12.7
- older docker pull quay.io/jcmoraisjr/haproxy-ingress:v0.12.6 --> to make common vm image for master and worker
- https://quay.io/repository/jcmoraisjr/haproxy-ingress?tag=latest&tab=tags
docker pull gcr.io/google-containers/kube-addon-manager-amd64:v9.1.1
docker pull bitnami/metrics-server:0.5.0
- old docker pull gcr.io/google-containers/metrics-server-amd64:v0.3.6
docker pull gcr.io/google-containers/rescheduler:v0.4.0

-- DID NOT WORK
docker pull k8s.gcr.io/pause-amd64:3.4.1

-----------------------------------
INSTALL KUBEADM, KUBECTL, KUBELET
-----------------------------------
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
https://www.linuxtechi.com/install-kubernetes-1-7-centos7-rhel7/
https://www.howtoforge.com/tutorial/centos-kubernetes-docker-cluster/

- STEPS FROM https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# Set SELinux in permissive mode (effectively disabling it)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

--> For latest versions: sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
--> For specific versions: sudo yum install -y kubelet-1.21.2-0.x86_64 kubeadm-1.21.2-0.x86_64 kubectl-1.22.0-0.x86_64 --disableexcludes=kubernetes
            The following get installed (cni as well gets installed)
            kubeadm-1.21.2-0.x86_64
            kubelet-1.21.2-0.x86_64
            kubernetes-cni-0.8.7-0.x86_64
            kubectl-1.22.0-0.x86_64
            
- KUBELET SERVICE STATUS
# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: inactive (dead)
     Docs: https://kubernetes.io/docs/
     
- ENABLE KUBELET SERVICE
sudo systemctl enable --now kubelet

============================================================================
ETCD - TRY JUST ETCD - WITH NO CERTIFICATES - to learn, and verify vm setup
============================================================================

https://etcd.io/docs/v3.5/op-guide/container
  
---------------
PREP VM
---------------
Clone the model machine
VM hostname: ksn1 (sn for single-node)
Memory: 2048 MB (2GB)
Vbox network: vboxnet0
Adapter type: host-only
Adapter name in VM: enp0s8
IP: 192.168.99.101 (pingable from laptop) (set using nmtui command in VM)

Running a single node etcd
Use the host IP address when configuring etcd:

---------------
RUN ETCD
---------------
Modified steps from https://etcd.io/docs/v3.5/op-guide/container
--> Kept the steps and commands, changed IP and such

- Configure a Docker volume to store etcd data:
# docker volume create --name etcd-data
# docker volume ls
DRIVER    VOLUME NAME
local     etcd-data

- Run etcd:
export NODE1=192.168.99.101
export DATA_DIR="etcd-data"

docker run \
  -p 2379:2379 \
  -p 2380:2380 \
  --volume=${DATA_DIR}:/etcd-data \
  --name etcd ${REGISTRY}:latest \
  /usr/local/bin/etcd \
  --data-dir=/etcd-data --name node1 \
  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \
  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \
  --initial-cluster node1=http://${NODE1}:2380
  
- List the cluster member:
etcdctl --endpoints=http://${NODE1}:2379 member list

- REFERENCE - STEPS AS IN DOC

https://etcd.io/docs/v3.5/op-guide/container

Running a single node etcd
Use the host IP address when configuring etcd:

export NODE1=192.168.1.21
Configure a Docker volume to store etcd data:

docker volume create --name etcd-data
export DATA_DIR="etcd-data"

Run the latest version of etcd:

REGISTRY=quay.io/coreos/etcd
# available from v3.2.5
REGISTRY=gcr.io/etcd-development/etcd

docker run \
  -p 2379:2379 \
  -p 2380:2380 \
  --volume=${DATA_DIR}:/etcd-data \
  --name etcd ${REGISTRY}:latest \
  /usr/local/bin/etcd \
  --data-dir=/etcd-data --name node1 \
  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \
  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \
  --initial-cluster node1=http://${NODE1}:2380
List the cluster member:

etcdctl --endpoints=http://${NODE1}:2379 member list


