============================================================================================
SINGLE NODE KUBERNETES CLUSTER - CREATE WITH KUBEADM INIT (2021 JUNE)
============================================================================================

FEATURE STATE: Kubernetes v1.21

=======================
REFERENCES
=======================
ALSO SEE OLD NOTES TO SETUP THE NODE: https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/kub-machines/k8s-model-vm.txt
And for setup, also, https://github.com/coderdba/NOTES/blob/master/kubernetes-kb/install-vbox/WIP-install-as-pods-separately.txt

KUBERNETES INSTALL
Tutorial: https://devopscube.com/setup-kubernetes-cluster-kubeadm/

https://kubernetes.io/docs/setup/
--> https://kubernetes.io/docs/setup/production-environment/
--> https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/
--> https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

TOOLS INSTALL AND CLUSTER MANAGEMENT
https://kubernetes.io/docs/setup/
--> upon clicking "learning environment": https://kubernetes.io/docs/tasks/tools/

=======================
PREPARE THE NODE
=======================
https://kubernetes.io/docs/setup/
--> https://kubernetes.io/docs/setup/production-environment/
--> https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/

--------------------
INSTALL KUBEADM
--------------------
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

- Letting iptables see bridged traffic
NOTE: This may already have been done differently as in https://github.com/coderdba/NOTES/blob/7b9a11db99295c599d42f47401aba351506b7390/kubernetes-kb/kub-machines/k8s-model-vm.txt
--> 
    # modprobe br_netfilter
    # echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

    Also, put it in /etc/sysctl.conf (as in Oracle Linux) or /usr/lib/sysctl.d/00-system.conf (as in centos) as follows:
    net.bridge.bridge-nf-call-iptables = 1

    And, make it persistent:
    # sysctl -p

-- PER CURRENT DOCUMENTATION
Make sure that the br_netfilter module is loaded. 
This can be done by running lsmod | grep br_netfilter. 
To load it explicitly call sudo modprobe br_netfilter.

As a requirement for your Linux Node's iptables to correctly see bridged traffic, 
you should ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config, e.g.

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system

- Check required ports

Control-plane node(s)
Protocol	Direction	Port Range	Purpose	Used By
TCP	Inbound	6443*	Kubernetes API server	All
TCP	Inbound	2379-2380	etcd server client API	kube-apiserver, etcd
TCP	Inbound	10250	kubelet API	Self, Control plane
TCP	Inbound	10251	kube-scheduler	Self
TCP	Inbound	10252	kube-controller-manager	Self

Worker node(s)
Protocol	Direction	Port Range	Purpose	Used By
TCP	Inbound	10250	kubelet API	Self, Control plane
TCP	Inbound	30000-32767	NodePort Services†	All

- Runtime (docker and such)
Kubeadm will look at the socket for the runtime to detect.

Runtime	Path to Unix domain socket
Docker	/var/run/dockershim.sock
containerd	/run/containerd/containerd.sock
CRI-O	/var/run/crio/crio.sock

- Install kubeadm, kubectl, kubelet

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# Set SELinux in permissive mode (effectively disabling it)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

rpm -qa |grep kube
kubelet-1.21.2-0.x86_64
kubernetes-cni-0.8.7-0.x86_64
kubectl-1.21.2-0.x86_64
kubeadm-1.21.2-0.x86_64

sudo systemctl enable --now kubelet

- Configure cgroups driver
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/
If the user is not setting the cgroupDriver field under KubeletConfiguration, kubeadm init will default it to systemd.

======================================
CREATE CLUSTER
======================================
NOTE: Use the tutorial. Official doc does not state clearly how many nodes are needed and such

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
Tutorial: https://devopscube.com/setup-kubernetes-cluster-kubeadm/

---------------------------------
INSTALL LATEST/COMPATIBLE DOCKER
---------------------------------
Docker 18.x was not compatible with latest Kubernetes.

- Reinstall docker with latest version
yum remove docker-ce
yum install docker-ce docker-ce-cli containerd.io

---------------------------------
STOP FIREWALL
---------------------------------
systemctl stop firewalld
systemctl disable firewalld

---------------------------------
ADD THE HOST AND IP TO /etc/hosts
---------------------------------
vi /etc/hosts

192.168.40.101 ks1

---------------------------------
KUBEADM INIT - TO INSTALL MASTER
---------------------------------

- Creat a shell script
vi kubeadm-init.sh
export IPADDR="192.168.40.101"
export NODENAME=$(hostname -s)
kubeadm init --apiserver-advertise-address=$IPADDR  --apiserver-cert-extra-sans=$IPADDR  --pod-network-cidr=172.16.0.0/16 --node-name $NODENAME --ignore-preflight-errors Swap

- Run the shell script
[root@ks1 2021]# ./kubeadm-init.sh
[init] Using Kubernetes version: v1.21.2
[preflight] Running pre-flight checks
	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [ks1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.40.101]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [ks1 localhost] and IPs [192.168.40.101 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [ks1 localhost] and IPs [192.168.40.101 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 14.003364 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node ks1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node ks1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: goo16b.fsknqm9z4n8i6iey
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.40.101:6443 --token goo16b.fsknqm9z4n8i6iey \
	--discovery-token-ca-cert-hash sha256:ad641b790d073def86cd7234f86f229714c71490d8cc1ebcfa3fb42b88849eb0 

- VERIFY
# kubectl cluster-info

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
The connection to the server localhost:8080 was refused - did you specify the right host or port?

# kubectl get pods --all-namespaces
The connection to the server localhost:8080 was refused - did you specify the right host or port?

# docker ps -a
CONTAINER ID   IMAGE                    COMMAND                  CREATED         STATUS         PORTS     NAMES
90d0ce27648d   a6ebd1c1ad98             "/usr/local/bin/kube…"   7 minutes ago   Up 7 minutes             k8s_kube-proxy_kube-proxy-sswtt_kube-system_84c4ece8-5385-4059-9c69-20f1a0ec87bc_0
961ec60b4ae4   k8s.gcr.io/pause:3.4.1   "/pause"                 7 minutes ago   Up 7 minutes             k8s_POD_kube-proxy-sswtt_kube-system_84c4ece8-5385-4059-9c69-20f1a0ec87bc_0
3aa3b7bd16c3   f917b8c8f55b             "kube-scheduler --au…"   8 minutes ago   Up 8 minutes             k8s_kube-scheduler_kube-scheduler-ks1_kube-system_ba8688f4108c3c7dd06cc6441064c9f2_0
8b4a2396312d   ae24db9aa2cc             "kube-controller-man…"   8 minutes ago   Up 8 minutes             k8s_kube-controller-manager_kube-controller-manager-ks1_kube-system_e0b2a571984a85a54079fc429061e7c7_0
cc0f6fd72c6d   106ff58d4308             "kube-apiserver --ad…"   8 minutes ago   Up 8 minutes             k8s_kube-apiserver_kube-apiserver-ks1_kube-system_8a30f65c7d45f1458e512c2348b60808_0
f1b650cea912   0369cf4303ff             "etcd --advertise-cl…"   8 minutes ago   Up 8 minutes             k8s_etcd_etcd-ks1_kube-system_d6f2197d51efd972472cb527e4cfaa83_0
43c14ac67fb6   k8s.gcr.io/pause:3.4.1   "/pause"                 8 minutes ago   Up 8 minutes             k8s_POD_kube-scheduler-ks1_kube-system_ba8688f4108c3c7dd06cc6441064c9f2_0
660848f88ffe   k8s.gcr.io/pause:3.4.1   "/pause"                 8 minutes ago   Up 8 minutes             k8s_POD_kube-controller-manager-ks1_kube-system_e0b2a571984a85a54079fc429061e7c7_0
2d11cd974a6b   k8s.gcr.io/pause:3.4.1   "/pause"                 8 minutes ago   Up 8 minutes             k8s_POD_kube-apiserver-ks1_kube-system_8a30f65c7d45f1458e512c2348b60808_0
198aedbc0e84   k8s.gcr.io/pause:3.4.1   "/pause"                 8 minutes ago   Up 8 minutes             k8s_POD_etcd-ks1_kube-system_d6f2197d51efd972472cb527e4cfaa83_0

- FIX 8080 Port error
Problem discussed: https://discuss.kubernetes.io/t/the-connection-to-the-server-localhost-8080-was-refused-did-you-specify-the-right-host-or-port/1464/8
Solution from: https://devopscube.com/setup-kubernetes-cluster-kubeadm/

Configuration to access apiserver would be not in the KUBECONFIG path.
Either set KUBECONFIG to /etc/kubernetes/admin.conf 
- OR - do the following:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

- VERIFY AGAIN
NOTE: All IPs are host's IP - for kubernete's own pods
NOTE: coredns pods are in 'pending' state as we need a network plugin like Calico or Flannel

# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-558bd4d5db-29vtn      0/1     Pending   0          13m   <none>           <none>   <none>           <none>
kube-system   coredns-558bd4d5db-v2rbf      0/1     Pending   0          13m   <none>           <none>   <none>           <none>
kube-system   etcd-ks1                      1/1     Running   0          14m   192.168.40.101   ks1      <none>           <none>
kube-system   kube-apiserver-ks1            1/1     Running   0          14m   192.168.40.101   ks1      <none>           <none>
kube-system   kube-controller-manager-ks1   1/1     Running   0          14m   192.168.40.101   ks1      <none>           <none>
kube-system   kube-proxy-sswtt              1/1     Running   0          13m   192.168.40.101   ks1      <none>           <none>
kube-system   kube-scheduler-ks1            1/1     Running   0          14m   192.168.40.101   ks1      <none>           <none>

- FURTHER
From tutorial: https://devopscube.com/setup-kubernetes-cluster-kubeadm/

-- Accessing cluster from workstation
Note: You can copy the admin.conf file from the master to your workstation in $HOME/.kube/config location 
if you want to execute kubectl commands from the workstation.

-- Scheduling pods in master node
By default, apps won’t get scheduled on the master node. 
If you want to use the master node for scheduling apps, taint the master node.

kubectl taint nodes --all node-role.kubernetes.io/master-

- ETCD
# docker ps -a |grep etcd
f1b650cea912   0369cf4303ff             "etcd --advertise-cl…"   16 minutes ago   Up 16 minutes             k8s_etcd_etcd-ks1_kube-system_d6f2197d51efd972472cb527e4cfaa83_0
198aedbc0e84   k8s.gcr.io/pause:3.4.1   "/pause"                 16 minutes ago   Up 16 minutes             k8s_POD_etcd-ks1_kube-system_d6f2197d51efd972472cb527e4cfaa83_0


- DOCKER IMAGES DOWNLOADED BY kubeadm init
# docker images
REPOSITORY                           TAG        IMAGE ID       CREATED         SIZE
k8s.gcr.io/kube-apiserver            v1.21.2    106ff58d4308   2 weeks ago     126MB
k8s.gcr.io/kube-proxy                v1.21.2    a6ebd1c1ad98   2 weeks ago     131MB
k8s.gcr.io/kube-controller-manager   v1.21.2    ae24db9aa2cc   2 weeks ago     120MB
k8s.gcr.io/kube-scheduler            v1.21.2    f917b8c8f55b   2 weeks ago     50.6MB
k8s.gcr.io/pause                     3.4.1      0f8457a4c2ec   5 months ago    683kB
k8s.gcr.io/coredns/coredns           v1.8.0     296a6d5035e2   8 months ago    42.5MB
k8s.gcr.io/etcd                      3.4.13-0   0369cf4303ff   10 months ago   253MB

--------------------------
INSTALL POD-NETWORK ADDON
--------------------------
Reference: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
Steps from tutorial: https://devopscube.com/setup-kubernetes-cluster-kubeadm/

Kubeadm does not configure any network plugin. 
You need to install a network plugin of your choice.
I am using Calico network plugin for this setup.

- SETUP
Make sure you execute the kubectl command from where you have configured the kubeconfig file. 
Either from the master of your workstation with the connectivity to the kubernetes API.

- INSTALL CALICO
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-kube-controllers created

- VERIFY
NOTE: Now you can see all pods running - none in Pending state
NOTE: The IPs of calico-kube-controllers and coredns are 172.... series - the CIDR we gave for pod network - when we ran kubeadm init

# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP               NODE   NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-78d6f96c7b-rg25f   1/1     Running   0          111s   172.16.145.66    ks1    <none>           <none>
kube-system   calico-node-54plk                          1/1     Running   0          112s   192.168.40.101   ks1    <none>           <none>
kube-system   coredns-558bd4d5db-29vtn                   1/1     Running   0          18m    172.16.145.67    ks1    <none>           <none>
kube-system   coredns-558bd4d5db-v2rbf                   1/1     Running   0          18m    172.16.145.65    ks1    <none>           <none>
kube-system   etcd-ks1                                   1/1     Running   0          19m    192.168.40.101   ks1    <none>           <none>
kube-system   kube-apiserver-ks1                         1/1     Running   0          19m    192.168.40.101   ks1    <none>           <none>
kube-system   kube-controller-manager-ks1                1/1     Running   0          19m    192.168.40.101   ks1    <none>           <none>
kube-system   kube-proxy-sswtt                           1/1     Running   0          18m    192.168.40.101   ks1    <none>           <none>
kube-system   kube-scheduler-ks1                         1/1     Running   0          19m    192.168.40.101   ks1    <none>           <none>

=================================
CREATE AN NGNIX POD
=================================

Pod 'deployment' file - nginx.yml:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80 

Nodeport 'service' file - nginx-nodeport.yml:
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector: 
    app: nginx
  type: NodePort  
  ports:
    - port: 80
      targetPort: 80
      nodePort: 32000

- CREATE POD
kubectl apply -f nginx.yml

- CREATE SERVICE
kubectl apply -f nginx-nodeport.yml

- VERIFY
# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP               NODE   NOMINATED NODE   READINESS GATES
default       nginx-deployment-585449566-6mnpc           1/1     Running   1          6m56s   172.16.145.77    ks1    <none>           <none>
default       nginx-deployment-585449566-cgg7f           1/1     Running   1          6m56s   172.16.145.76    ks1    <none>           <none>
kube-system   calico-kube-controllers-78d6f96c7b-rg25f   1/1     Running   2          60m     172.16.145.73    ks1    <none>           <none>
kube-system   calico-node-54plk                          1/1     Running   2          60m     192.168.40.101   ks1    <none>           <none>
kube-system   coredns-558bd4d5db-29vtn                   1/1     Running   2          77m     172.16.145.74    ks1    <none>           <none>
kube-system   coredns-558bd4d5db-v2rbf                   1/1     Running   2          77m     172.16.145.75    ks1    <none>           <none>
kube-system   etcd-ks1                                   1/1     Running   2          77m     192.168.40.101   ks1    <none>           <none>
kube-system   kube-apiserver-ks1                         1/1     Running   2          77m     192.168.40.101   ks1    <none>           <none>
kube-system   kube-controller-manager-ks1                1/1     Running   2          77m     192.168.40.101   ks1    <none>           <none>
kube-system   kube-proxy-sswtt                           1/1     Running   2          77m     192.168.40.101   ks1    <none>           <none>
kube-system   kube-scheduler-ks1                         1/1     Running   2          77m     192.168.40.101   ks1    <none>           <none>

- VERIFY
# curl http://ks1:32000
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

=========================
POTENTIAL ISSUES
=========================

Following are the possible issues you might encounter in kubeadm setup.

Pod Out of memory and CPU: The master node should have a minimum of 2vCPU and 2 GB memory.

Nodes cannot connect to Master: Check the firewall between nodes and make sure all the nodes can talk to each other on the required kubernetes ports.

Calico Pod Restarts: Sometimes, if you use the same IP range for the node and pod network, Calico pods may not work as expected. 
So make sure the node and pod IP ranges don’t overlap. Overlapping IP addresses could result in issues for other applications running on the cluster as well.
