==================================================
       ORACLE RAC 12.2 ON VIRTUALBOX VMs
==================================================
https://oracle-base.com/articles/12c/oracle-db-12cr2-rac-installation-on-oracle-linux-6-using-virtualbox
https://balazspapp.wordpress.com/2017/03/19/installing-oracle-12c-release-2-12-2-rac-on-virtualbox-silent-installation-part-2/  (VERY IMPORTANT - TO RESOLVE /ETC/RESOLV.CONF use this doc)

Another one, with BIND/DNS install included:
       https://dillidba.blogspot.com/2018/02/step-by-step-oracle-grid-infrastructure.html
       For DNS/BINS: https://dillidba.blogspot.com/2018/02/install-and-configure-dns-for-oracle-rac.html

DNSMASQ FOR SCAN:  http://dbaora.com/configure-scan-dns-for-rac-11g-rac-12c-using-dnsmasq-in-oel5-oel6-2/

https://docs.oracle.com/en/database/oracle/oracle-database/12.2/tdprc/installing-oracle-grid.html#GUID-72D1994F-E838-415A-8E7D-30EA780D74A8
--> 2 day + RAC
https://docs.oracle.com/pls/topic/choose?ctx=db122&ids=CWLIN+CWSOL+CWAIX+CWHPX+CWWIN
--> platform specific

https://emilianofusaglia.net/2017/03/06/installing-oracle-grid-infrastrucure-12c-r2/
12.2 Grid Infrastructure and Database Upgrade steps for Exadata Database Machine running 11.2.0.3 and later on Oracle Linux (Doc ID 2111010.1)

https://docs.oracle.com/en/database/oracle/oracle-database/12.2/tdprc/installing-oracle-grid.html#GUID-986CA7AA-DA09-4261-866A-D08CD7A3E689

Silent install:
https://pierreforstmanndotcom.wordpress.com/2017/03/04/grid-infrastructure-12-2-0-1-silent-installation-on-oracle-linux-7/

Deinstall:
https://community.toadworld.com/platforms/oracle/b/weblog/archive/2017/08/08/deinstall-oracle-grid-infrastructure-12-2-failed-installation

Deinstall old version:
https://jameshuangsj.wordpress.com/2018/02/12/deinstall-12-1-0-2-grid-infrastructure-home-after-being-upgraded-to-12-2-0-1/

ISSUE WITH MEMORY_TARGET during install:
http://www.ashdba.com/ora-00845-memory_target-not-supported-on-this-system-while-starting-database-oracle-12c/

=========================
CREATE VMs
=========================
-----------------------
CREATE MODEL VM
-----------------------
Create a VM as in Virtualbox-kb/builds folder document
- Install OS
- Install Oracle Grid 12cR2 Prerequisite RPM
- Install other required RPMs
- Create users and group
- Perl fix
- ... etc

- --------------------------------------
- Create node machines cloning model VM
- --------------------------------------
-node1
Clone the model vm onto new vm rac1n1

-node2
Clone the model vm onto new vm rac1n2

- -------------------------------
- Change hostnames and IPs of VMs  
- -------------------------------

- NODE1 - rac1n1
# hostnamectl set-hostname rac1n1

- Setup network IPs
Edit network settings in GUI: system-tools --> network 
Public - 192.168.0.101
Private - 10.10.10.101
Virtual (just set in /etc/hosts) - 192.168.0.111

-- Private
# ifdown enp0s8
# ifconfig enp0s8 10.10.10.101 netmask 255.255.255.0
# ifup enp0s8

-- Public
# ifdown enp0s9
# ifconfig enp0s9 192.168.0.101 netmask 255.255.255.0
# ifup enp0s9

-- Verify
# ip addr list

3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7b:13:15 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.101/24 brd 10.10.10.255 scope global noprefixroute enp0s8

4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ca:34:38 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.101/24 brd 192.168.0.255 scope global noprefixroute enp0s9

- NODE2 - rac1n2
# hostnamectl set-hostname rac1n2

- Setup network IPs
Edit network settings in GUI: system-tools --> network 
Public - 192.168.0.102
Private - 10.10.10.102
Virtual (just set in /etc/hosts) - 192.168.0.112

-- Private
# ifdown enp0s8
# ifconfig enp0s8 10.10.10.102 netmask 255.255.255.0
# ifup enp0s8

-- Public
# ifdown enp0s9
# ifconfig enp0s9 192.168.0.102 netmask 255.255.255.0
# ifup enp0s9

-- Verify
# ip addr list

3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7b:13:15 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.102/24 brd 10.10.10.255 scope global noprefixroute enp0s8

4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ca:34:38 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.102/24 brd 192.168.0.255 scope global noprefixroute enp0s9

- MAKE /ETC/HOSTS ENTRIES - NODE1 AND NODE2
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4

# Public
192.168.0.101   rac1n1
192.168.0.102   rac1n2

# Private
10.10.10.101   rac1n1-priv
10.10.10.102   rac1n2-priv

# Virtual
192.168.0.111   rac1n1-vip
192.168.0.112   rac1n2-vip

# SCAN
192.168.0.201   rac1-scan
192.168.0.202   rac1-scan
192.168.0.203   rac1-scan

::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

======================
ADD ASM DISKS
======================
Minimum 40GB is needed
Add 4 disks of 12GB each
-> for 'data' diskgroup
-> for now, let us keep OCR and Voting disks in data diskgroup itself

--------
NODE1
--------

- Shutdown VM

- Attach four disks to the SATA controller 
       - 12GB each
       - Assuming diskgroup name to be data
       - Naming - rac1_asmdata01 through rac1_asmdata05
       - Create as 'fixed size' - NOT 'dynamically allocated' - only 'fixed' sized disks are shareable between nodes

$ mkdir -p /VboxVMDisks/rac1
$ cd /VboxVMDisks/rac1

$ # Create the disks and associate them with VirtualBox as virtual media.
$ VBoxManage createhd --filename rac1_asmdata01.vmdk --size 12288 --format VMDK --variant Fixed
$ VBoxManage createhd --filename rac1_asmdata02.vmdk --size 12288 --format VMDK --variant Fixed
$ VBoxManage createhd --filename rac1_asmdata03.vmdk --size 12288 --format VMDK --variant Fixed
$ VBoxManage createhd --filename rac1_asmdata04.vmdk --size 12288 --format VMDK --variant Fixed

$ # Connect them to the VM.
$ VBoxManage storageattach rac1n1 --storagectl "SATA" --port 1 --device 0 --type hdd --medium rac1_asmdata01.vmdk --mtype shareable
$ VBoxManage storageattach rac1n1 --storagectl "SATA" --port 2 --device 0 --type hdd --medium rac1_asmdata02.vmdk --mtype shareable
$ VBoxManage storageattach rac1n1 --storagectl "SATA" --port 3 --device 0 --type hdd --medium rac1_asmdata03.vmdk --mtype shareable
$ VBoxManage storageattach rac1n1 --storagectl "SATA" --port 4 --device 0 --type hdd --medium rac1_asmdata04.vmdk --mtype shareable
$
$ # Make shareable.
$ VBoxManage modifyhd rac1_asmdata01.vmdk --type shareable
$ VBoxManage modifyhd rac1_asmdata02.vmdk --type shareable
$ VBoxManage modifyhd rac1_asmdata03.vmdk --type shareable
$ VBoxManage modifyhd rac1_asmdata04.vmdk --type shareable

- Restart the VM

- -------------------------
- NODE1 - Partition the new disks
- -------------------------
https://superuser.com/questions/332252/how-to-create-and-format-a-partition-using-a-bash-script#984637

echo -e "n\np\n1\n\n\nw" | fdisk /dev/sdb
echo -e "n\np\n1\n\n\nw" | fdisk /dev/sdc
echo -e "n\np\n1\n\n\nw" | fdisk /dev/sdd
echo -e "n\np\n1\n\n\nw" | fdisk /dev/sde

# ls -l /dev/sd* 
--> should show /dev/sdb1 through /dev/sde1

-> which is equivalent to 
(note: \n is for newline or 'enter' key)
n=new
p=partition
Two \n's for accepting default first and last sector
w=write to partition table

Alternative:
(
#echo o # Create a new empty DOS partition table
echo n # Add a new partition
echo p # Primary partition
echo 1 # Partition number
echo   # First sector (Accept default: 1)
echo   # Last sector (Accept default: varies)
echo w # Write changes
) | sudo fdisk /dev/sdb

====================================================================================================
====================================================================================================
TRADITIONAL ASMLIB STEPS - 
(IF INSTEAD, USED Oracle ASM Filter Driver - somehow that conflicted with installed ASMLIB drivers and did not work)
- -------------------------------------
- NODE1 - Create ASM disks using ASMLIB
- -------------------------------------
# oracleasm createdisk DATA01 /dev/sdb1
# oracleasm createdisk DATA02 /dev/sdc1
# oracleasm createdisk DATA03 /dev/sdd1
# oracleasm createdisk DATA04 /dev/sde1

-- Verify
# ls -l /dev/oracleasm/disks
total 0
brw-rw----. 1 grid asmadmin 8, 17 Aug 21 17:30 DATA01
brw-rw----. 1 grid asmadmin 8, 33 Aug 21 17:32 DATA02
brw-rw----. 1 grid asmadmin 8, 49 Aug 21 17:32 DATA03
brw-rw----. 1 grid asmadmin 8, 65 Aug 21 17:32 DATA04

-- Verify
# oracleasm listdisks
DATA01
DATA02
DATA03
DATA04

- -------------------------------------
- NODE2 - Attach shared disks
- -------------------------------------

- Shutdown node1 - rac1n1
- Shutdown node2 - rac1n2

- NODE2 - Attach shared disks to node2
cd /VboxVMDisks/rac1
VBoxManage storageattach rac1n2 --storagectl "SATA" --port 1 --device 0 --type hdd --medium rac1_asmdata01.vmdk --mtype shareable
VBoxManage storageattach rac1n2 --storagectl "SATA" --port 2 --device 0 --type hdd --medium rac1_asmdata02.vmdk --mtype shareable
VBoxManage storageattach rac1n2 --storagectl "SATA" --port 3 --device 0 --type hdd --medium rac1_asmdata03.vmdk --mtype shareable
VBoxManage storageattach rac1n2 --storagectl "SATA" --port 4 --device 0 --type hdd --medium rac1_asmdata04.vmdk --mtype shareable

- NODE2 - Scan ASM disks
[root@rac1n2 dev]# oracleasm scandisks
Reloading disk partitions: done
Cleaning any stale ASM disks...
Scanning system for ASM disks...

[root@rac1n2 dev]# oracleasm listdisks
DATA01
DATA02
DATA03
DATA04

[root@rac1n2 dev]# ls -l /dev/oracleasm/disks/*
brw-rw----. 1 grid asmadmin 8, 17 Aug 21 18:50 /dev/oracleasm/disks/DATA01
brw-rw----. 1 grid asmadmin 8, 33 Aug 21 18:50 /dev/oracleasm/disks/DATA02
brw-rw----. 1 grid asmadmin 8, 49 Aug 21 18:50 /dev/oracleasm/disks/DATA03
brw-rw----. 1 grid asmadmin 8, 65 Aug 21 18:50 /dev/oracleasm/disks/DATA04


====================================================================================================
====================================================================================================

==========================================================================================================
OPTIONAL - DID NOT WORK WELL - CONFIGURE ASM DISKS - USING NEW Oracle ASM Filter Driver
==========================================================================================================
NOTE: The steps until adding labels worked 
- but, could not enable AFD driver in GUI - it errored out conflicting with ASMLIB drivers

NOTE: NEED TO DO A 'CONFIGURE' FIRST??
https://dbamarco.wordpress.com/2016/06/13/using-asm-filter-driver-right-from-the-beginning/

Node1:

- Logon as root

- SET ENVIRONMENT
# export ORACLE_HOME=/u01/app/gridhome12cr2
# export ORACLE_BASE=/u01/app/gridbase

- CONFIGURE
# $ORACLE_HOME/bin/asmcmd afd_label DATA01 /dev/sdb1 --init
# $ORACLE_HOME/bin/asmcmd afd_label DATA02 /dev/sdc1 --init
# $ORACLE_HOME/bin/asmcmd afd_label DATA03 /dev/sdd1 --init
# $ORACLE_HOME/bin/asmcmd afd_label DATA04 /dev/sde1 --init

- VERIFY
# $ORACLE_HOME/bin/asmcmd afd_lslbl /dev/sdb1
--------------------------------------------------------------------------------
Label                     Duplicate  Path
================================================================================
DATA01                                /dev/sdb1
...
...

FAILED HERE (as below): (in the gridSetup.sh GUI)

- Create ASM Diskgroup
Redundancy = external
Allocation unit size = 4MB (default)
Select disks = choose the four disks that should show up now as we have configured them using ASMCMD
Check the option Configure Oracle ASM Filter Driver.
--> pressing next with checking the ASM Filter Driver option gave error:
--> AFD9201, AFD9202 --> AFD Cannot be installed/loaded because asmlib is installed

       On both nodes: Remove the ASM libraries:
       # rpm -qa|grep asm
       oracleasm-support-2.1.11-2.el7.x86_64
       oracleasmlib-2.0.12-1.el7.x86_64
       # rpm -e oracleasm-support
       # rpm -e oracleasmlib

====================================================================================================
====================================================================================================

=========================
GRID INSTALL
=========================

---------------------
UNZIP GRID SOFTWARE
---------------------

On NODE1:
- Logon as 'grid'
- Unzip zip file
$ cd /u01/app/gridhome12cr2
$ unzip /media/sf_mystage_host/oracle-12cR2/linuxx64_12201_grid_home.zip

--------------------------------------------
INSTALL CLUSTER VERIFICATION UTILITY RPM
--------------------------------------------
On each node: 
(on node2, copy rpm from node1)

# cd /u01/app/gridhome12cr2/cv/rpm
# rpm -Uvh ./cvuqdisk-1.0.10-1.rpm

-----------
SETUP GRID
-----------
On Node1:

Logon directly as 'grid' - dont logon as root and su to grid (that will result in xterm not bringing setup GUI)

Start xterm
$ xterm

$ cd /u01/app/gridhome12cr2
$ ./gridSetup.sh

Options to choose/set:

- Configuration option = Configure Oracle grid infrastructure for a new cluster
- Cluster configuration = Configure an Oracle standalone cluster
- Grid plug & play 
       cluster name = rac1
       scan name = rac1-scan
       port = 1521

- Cluster node information
       Public hostname = rac1n1
       Hub/Leaf = HUB
       Virtual Hostname = rac1n1-vip
       
  Now, in the same screen, add the next node with similar attributes:
       Public hostname = rac1n2
       Hub/Leaf = HUB
       Virtual Hostname = rac1n2-vip
       
  In the same screen, test ssh connectivity by pressing the button.

  Upon pressing 'next' it will do some checks, including SSH...
  
- Network interface usage
Choose as follows:
enp0s3 = do not use (changed this from 'asm & private)
enp0s8 = 10.10.10.0 = ASM & Private (changed this from 'do not use')
enp0s9 = 192.168.0.0 = Public
vrbr0 = do not use

- Storage Options
Choose: Configure ASM Using Block Devices
(other option: Configure ASM on NFS)

- Grid Infrastructure Management
Choose 'no' for 'separate disk group for grid infra management (ocr, voting)'
--> though 'yes' is a better option

- Create ASM Diskgroup
Redundancy = external
Allocation unit size = 4MB (default)
Change discovery path = /dev/oracleasm/disks/*
Select disks = choose the four disks that should show up now as we have configured them using ASMCMD

- ASM Password
Used 'oracle' as password for 

- Failure isolation
--> did not choose IPMI

- Management options
--> did not choose EM registration

- Operating System Groups
ASM Administration (OSASM) = ASMADMIN
ASM DBA (OSDBA) = ASMDBA
ASM Operator (OSOPER) = NULL (optional)

- Installation Location
Oracle Base = /u01/app/gridbase
Oracle Grid Infra home (aka software location = /u01/app/gridhome12cr2 (already chosen and greyed)

NOTE: This requires other rac nodes grid-home to be empty (no software unzip at other nodes)
--> otherwise it will give error

- Create Inventory
Inventory Directory = /u01/app/oraInventory

Inventory owner = oinstall (already chosen and greyed out)

- Root script execution
Provided 'root' password here
(alternatively, can provide sudo password of current user to become root)

- NOW, press NEXT to start the work

--> Failed for cv rpm missing - installed it
--> Failed for /etc/resolv.conf - that racn1n1,rac1n2 could not be resolved
-----> commented out /etc/resolv.conf entries and also did chattr +i /etc/resolv.conf
# cat /etc/resolv.conf
# Generated by NetworkManager
#search mycompany.com
#nameserver 10.197.140.216
#nameserver 10.197.140.215
nameserver 192.168.122.1  --> this is the virtualbox dns name for the VM - and find that by "netstat -anp | grep dnsmasq"

--> Failed for DIS/NIS name service - for scan
----> probably the resolv.conf fix should fix this
-- JUST CHOOSE CHECKBOX 'IGNORE' WHICH IS NOT CLEARLY VISIBLE - NEAR RIGHT HAND TOP CORNER A BIT BELOW... ---
---- AND PROCEED

- Summary screen
Save the response-file as /home/grid/grid.rsp (default location/filename)

- ROOT.SH STEP
Popup asks if it can run root.sh using the credential provided - say yes

- Errors with ORA-00845: MEMORY_TARGET not supported - in the ASM-up step of root.sh
http://www.ashdba.com/ora-00845-memory_target-not-supported-on-this-system-while-starting-database-oracle-12c/

To get around that, edit /etc/fstab adding the following (1GB min is required - give a bit more)
# To get around ORA-00845: MEMORY_TARGET not supported during grid install - ASM up operation
tmpfs /dev/shm tmpfs rw,exec,size=1090M 0 0

Then, remount /dev/shm
# mount -o remount /dev/shm

- Then retry the operation in GUI

- Finally, it may error out for NTP settings 
-- then enably chrony
# systemctl enable chronyd.service
# systemctl start chronyd.service

-- Still, it will error out saying NTP servers are not configured
Dont worry about it, say ok or whatever and close the installer
---- at this point all is well

==========================================
VERIFY
==========================================
Logon as grid user

[grid@rac1n1 ~]$ . oraenv
ORACLE_SID = [grid] ? +ASM1
The Oracle base has been set to /u01/app/gridbase

[grid@rac1n1 ~]$ crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details       
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.ASMNET1LSNR_ASM.lsnr
               ONLINE  ONLINE       rac1n1                   STABLE
               ONLINE  ONLINE       rac1n2                   STABLE
ora.DATA.dg
               ONLINE  ONLINE       rac1n1                   STABLE
               ONLINE  ONLINE       rac1n2                   STABLE
ora.LISTENER.lsnr
               ONLINE  ONLINE       rac1n1                   STABLE
               ONLINE  ONLINE       rac1n2                   STABLE
ora.chad
               ONLINE  ONLINE       rac1n1                   STABLE
               ONLINE  ONLINE       rac1n2                   STABLE
ora.net1.network
               ONLINE  ONLINE       rac1n1                   STABLE
               ONLINE  ONLINE       rac1n2                   STABLE
ora.ons
               ONLINE  ONLINE       rac1n1                   STABLE
               ONLINE  ONLINE       rac1n2                   STABLE
ora.proxy_advm
               OFFLINE OFFLINE      rac1n1                   STABLE
               OFFLINE OFFLINE      rac1n2                   STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.LISTENER_SCAN1.lsnr
      1        ONLINE  ONLINE       rac1n2                   STABLE
ora.LISTENER_SCAN2.lsnr
      1        ONLINE  ONLINE       rac1n1                   STABLE
ora.LISTENER_SCAN3.lsnr
      1        ONLINE  ONLINE       rac1n1                   STABLE
ora.MGMTLSNR
      1        ONLINE  ONLINE       rac1n1                   169.254.214.30 10.10
                                                             .10.101,STABLE
ora.asm
      1        ONLINE  ONLINE       rac1n1                   Started,STABLE
      2        ONLINE  ONLINE       rac1n2                   Started,STABLE
      3        OFFLINE OFFLINE                               STABLE
ora.cvu
      1        ONLINE  ONLINE       rac1n1                   STABLE
ora.mgmtdb
      1        ONLINE  ONLINE       rac1n1                   Open,STABLE
ora.qosmserver
      1        ONLINE  ONLINE       rac1n1                   STABLE
ora.rac1n1.vip
      1        ONLINE  ONLINE       rac1n1                   STABLE
ora.rac1n2.vip
      1        ONLINE  ONLINE       rac1n2                   STABLE
ora.scan1.vip
      1        ONLINE  ONLINE       rac1n2                   STABLE
ora.scan2.vip
      1        ONLINE  ONLINE       rac1n1                   STABLE
ora.scan3.vip
      1        ONLINE  ONLINE       rac1n1                   STABLE
--------------------------------------------------------------------------------


[grid@rac1n1 ~]$ asmcmd
ASMCMD> lsdg
State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
MOUNTED  EXTERN  N         512             512   4096  4194304     49136    15084                0           15084              0             Y  DATA/
ASMCMD> exit

==========================================
STOP, START CRS
==========================================

NODE1:
Logon as root
# . oraenv
+ASM1

# crsctl stop crs -f

# crsctl stat res -t
CRS-4535: Cannot communicate with Cluster Ready Services
CRS-4000: Command Status failed, or completed with errors.

NODE2:
Logon as grid
$ crsctl stat res -t
should show all resources on rac1n2

Then, stop crs on node2:

Logon as root
# . oraenv
+ASM2
# crsctl stop crs -f



====================================================================================================
==========================================
APPENDIX - RESPONSE FILE - SAVED FROM GUI
==========================================
[grid@rac1n1 ~]$ cat grid.rsp

###############################################################################
## Copyright(c) Oracle Corporation 1998,2017. All rights reserved.           ##
##                                                                           ##
## Specify values for the variables listed below to customize                ##
## your installation.                                                        ##
##                                                                           ##
## Each variable is associated with a comment. The comment                   ##
## can help to populate the variables with the appropriate                   ##
## values.                                                                   ##
##                                                                           ##
## IMPORTANT NOTE: This file contains plain text passwords and               ##
## should be secured to have read permission only by oracle user             ##
## or db administrator who owns this installation.                           ##
##                                                                           ##
###############################################################################

###############################################################################
##                                                                           ##
## Instructions to fill this response file                                   ##
## To register and configure 'Grid Infrastructure for Cluster'               ##
##  - Fill out sections A,B,C,D,E,F and G                                    ##
##  - Fill out section G if OCR and voting disk should be placed on ASM      ##
##                                                                           ##
## To register and configure 'Grid Infrastructure for Standalone server'     ##
##  - Fill out sections A,B and G                                            ##
##                                                                           ##
## To register software for 'Grid Infrastructure'                            ##
##  - Fill out sections A,B and D                                            ##
##  - Provide the cluster nodes in section D when choosing CRS_SWONLY as     ##
##    installation option in section A                                       ##
##                                                                           ##
## To upgrade clusterware and/or Automatic storage management of earlier     ##
## releases                                                                  ##
##  - Fill out sections A,B,C,D and H                                        ##
##                                                                           ##
## To add more nodes to the cluster                                          ##
##  - Fill out sections A and D                                              ##
##  - Provide the cluster nodes in section D when choosing CRS_ADDNODE as    ##
##    installation option in section A                                       ##
##                                                                           ##
###############################################################################

#------------------------------------------------------------------------------
# Do not change the following system generated value. 
#------------------------------------------------------------------------------
oracle.install.responseFileVersion=/oracle/install/rspfmt_crsinstall_response_schema_v12.2.0

###############################################################################
#                                                                             #
#                          SECTION A - BASIC                                  #
#                                                                             #
###############################################################################


#-------------------------------------------------------------------------------
# Specify the location which holds the inventory files.
# This is an optional parameter if installing on  
# Windows based Operating System.
#-------------------------------------------------------------------------------
INVENTORY_LOCATION=/u01/app/oraInventory

#-------------------------------------------------------------------------------
# Specify the installation option.
# Allowed values: CRS_CONFIG or HA_CONFIG or UPGRADE or CRS_SWONLY or HA_SWONLY
#   - CRS_CONFIG  : To register home and configure Grid Infrastructure for cluster
#   - HA_CONFIG   : To register home and configure Grid Infrastructure for stand alone server
#   - UPGRADE     : To register home and upgrade clusterware software of earlier release
#   - CRS_SWONLY  : To register Grid Infrastructure Software home (can be configured for cluster 
#                   or stand alone server later)
#   - HA_SWONLY   : To register Grid Infrastructure Software home (can be configured for stand 
#                   alone server later. This is only supported on Windows.)
#   - CRS_ADDNODE : To add more nodes to the cluster
#-------------------------------------------------------------------------------
oracle.install.option=CRS_CONFIG

#-------------------------------------------------------------------------------
# Specify the complete path of the Oracle Base.
#-------------------------------------------------------------------------------
ORACLE_BASE=/u01/app/gridbase

################################################################################
#                                                                              #
#                              SECTION B - GROUPS                              #
#                                                                              #
#   The following three groups need to be assigned for all GI installations.   #
#   OSDBA and OSOPER can be the same or different.  OSASM must be different    #
#   than the other two.                                                        #
#   The value to be specified for OSDBA, OSOPER and OSASM group is only for    #
#   Unix based Operating System.                                               #
#                                                                              #
################################################################################
#-------------------------------------------------------------------------------
# The OSDBA_GROUP is the OS group which is to be granted SYSDBA privileges.
#-------------------------------------------------------------------------------
oracle.install.asm.OSDBA=asmdba

#-------------------------------------------------------------------------------
# The OSOPER_GROUP is the OS group which is to be granted SYSOPER privileges.
# The value to be specified for OSOPER group is optional.
# Value should not be provided if configuring Client Cluster - i.e. storageOption=CLIENT_ASM_STORAGE.
#-------------------------------------------------------------------------------
oracle.install.asm.OSOPER=

#-------------------------------------------------------------------------------
# The OSASM_GROUP is the OS group which is to be granted SYSASM privileges. This
# must be different than the previous two.
#-------------------------------------------------------------------------------
oracle.install.asm.OSASM=asmadmin

################################################################################
#                                                                              #
#                           SECTION C - SCAN                                   #
#                                                                              #
################################################################################
#-------------------------------------------------------------------------------
# Specify a name for SCAN
#-------------------------------------------------------------------------------
oracle.install.crs.config.gpnp.scanName=rac1-scan

#-------------------------------------------------------------------------------
# Specify a unused port number for SCAN service
#-------------------------------------------------------------------------------

oracle.install.crs.config.gpnp.scanPort=1521

################################################################################
#                                                                              #
#                           SECTION D - CLUSTER & GNS                         #
#                                                                              #
################################################################################
#-------------------------------------------------------------------------------
# Specify the required cluster configuration
# Allowed values: STANDALONE, DOMAIN, MEMBERDB, MEMBERAPP
#-------------------------------------------------------------------------------
oracle.install.crs.config.ClusterConfiguration=STANDALONE

#-------------------------------------------------------------------------------
# Specify 'true' if you would like to configure the cluster as Extended, else
# specify 'false'
#
# Applicable only for STANDALONE and DOMAIN cluster configuration
#-------------------------------------------------------------------------------
oracle.install.crs.config.configureAsExtendedCluster=false


#-------------------------------------------------------------------------------
# Specify the Member Cluster Manifest file
#
# Applicable only for MEMBERDB and MEMBERAPP cluster configuration
#-------------------------------------------------------------------------------
oracle.install.crs.config.memberClusterManifestFile=

#-------------------------------------------------------------------------------
# Specify a name for the Cluster you are creating.
#
# The maximum length allowed for clustername is 15 characters. The name can be 
# any combination of lower and uppercase alphabets (A - Z), (0 - 9), hyphen(-)
# and underscore(_).
#
# Applicable only for STANDALONE and DOMAIN cluster configuration
#-------------------------------------------------------------------------------
oracle.install.crs.config.clusterName=rac1

#-------------------------------------------------------------------------------
# Applicable only for STANDALONE, DOMAIN, MEMBERDB cluster configuration.
# Specify 'true' if you would like to configure Grid Naming Service(GNS), else
# specify 'false'
#-------------------------------------------------------------------------------
oracle.install.crs.config.gpnp.configureGNS=false

#-------------------------------------------------------------------------------
# Applicable only for STANDALONE and DOMAIN cluster configuration if you choose to configure GNS.
# Specify 'true' if you would like to assign SCAN name VIP and Node VIPs by DHCP
# , else specify 'false'
#-------------------------------------------------------------------------------
oracle.install.crs.config.autoConfigureClusterNodeVIP=false

#-------------------------------------------------------------------------------
# Applicable only if you choose to configure GNS.
# Specify the type of GNS configuration for cluster
# Allowed values are: CREATE_NEW_GNS and USE_SHARED_GNS
# Only USE_SHARED_GNS value is allowed for MEMBERDB cluster configuration.
#-------------------------------------------------------------------------------
oracle.install.crs.config.gpnp.gnsOption=

#-------------------------------------------------------------------------------
# Applicable only if SHARED_GNS is being configured for cluster
# Specify the path to the GNS client data file
#-------------------------------------------------------------------------------
oracle.install.crs.config.gpnp.gnsClientDataFile=

#-------------------------------------------------------------------------------
# Applicable only for STANDALONE and DOMAIN cluster configuration if you choose to 
# configure GNS for this cluster oracle.install.crs.config.gpnp.gnsOption=CREATE_NEW_GNS
# Specify the GNS subdomain and an unused virtual hostname for GNS service
#-------------------------------------------------------------------------------
oracle.install.crs.config.gpnp.gnsSubDomain=
oracle.install.crs.config.gpnp.gnsVIPAddress=

#-------------------------------------------------------------------------------
# Specify the list of sites - only if configuring an Extended Cluster
#-------------------------------------------------------------------------------
oracle.install.crs.config.sites=

#-------------------------------------------------------------------------------
# Specify the list of nodes that have to be configured to be part of the cluster.
#
# The list should a comma-separated list of tuples.  Each tuple should be a
# colon-separated string that contains
# - 1 field if you have chosen CRS_SWONLY as installation option, or
# - 1 field if configuring an Application Cluster, or
# - 3 fields if configuring a Flex Cluster
# - 3 fields if adding more nodes to the configured cluster, or
# - 4 fields if configuring an Extended Cluster
# 
# The fields should be ordered as follows:
# 1. The first field should be the public node name.
# 2. The second field should be the virtual host name
#    (Should be specified as AUTO if you have chosen 'auto configure for VIP'
#     i.e. autoConfigureClusterNodeVIP=true)
# 3. The third field indicates the role of node (HUB,LEAF). This has to 
#    be provided only if Flex Cluster is being configured.
#    For Extended Cluster only HUB should be specified for all nodes
# 4. The fourth field indicates the site designation for the node. To be specified only if configuring an Extended Cluster.
# The 2nd and 3rd fields are not applicable if you have chosen CRS_SWONLY as installation option
# The 2nd and 3rd fields are not applicable if configuring an Application Cluster
#
# Examples
# For registering GI for a cluster software: oracle.install.crs.config.clusterNodes=node1,node2
# For adding more nodes to the configured cluster: oracle.install.crs.config.clusterNodes=node1:node1-vip:HUB,node2:node2-vip:LEAF
# For configuring Application Cluster: oracle.install.crs.config.clusterNodes=node1,node2
# For configuring Flex Cluster: oracle.install.crs.config.clusterNodes=node1:node1-vip:HUB,node2:node2-vip:LEAF
# For configuring Extended Cluster: oracle.install.crs.config.clusterNodes=node1:node1-vip:HUB:site1,node2:node2-vip:HUB:site2
# You can specify a range of nodes in the tuple using colon separated fields of format
# hostnameprefix:lowerbound-upperbound:hostnamesuffix:vipsuffix:role of node
#
#-------------------------------------------------------------------------------
oracle.install.crs.config.clusterNodes=rac1n1:rac1n1-vip:HUB,rac1n2:rac1n2-vip:HUB

#-------------------------------------------------------------------------------
# The value should be a comma separated strings where each string is as shown below
# InterfaceName:SubnetAddress:InterfaceType
# where InterfaceType can be either "1", "2", "3", "4", or "5"
# InterfaceType stand for the following values
#   - 1 : PUBLIC
#   - 2 : PRIVATE
#   - 3 : DO NOT USE
#   - 4 : ASM
#   - 5 : ASM & PRIVATE
#
# For example: eth0:140.87.24.0:1,eth1:10.2.1.0:2,eth2:140.87.52.0:3
#
#-------------------------------------------------------------------------------
oracle.install.crs.config.networkInterfaceList=enp0s3:10.0.2.0:3,enp0s8:10.10.10.0:5,enp0s9:192.168.0.0:1,virbr0:192.168.122.0:3

#------------------------------------------------------------------------------
# Create a separate ASM DiskGroup to store GIMR data.
# Specify 'true' if you would like to separate GIMR data with clusterware data, 
# else specify 'false'
# Value should be 'true' for DOMAIN cluster configurations
# Value can be true/false for STANDALONE cluster configurations.
#------------------------------------------------------------------------------
oracle.install.asm.configureGIMRDataDG=false

################################################################################
#                                                                              #
#                              SECTION E - STORAGE                             #
#                                                                              #
################################################################################

#-------------------------------------------------------------------------------
# Specify the type of storage to use for Oracle Cluster Registry(OCR) and Voting
# Disks files
#   - FLEX_ASM_STORAGE
#   - CLIENT_ASM_STORAGE
#
# Applicable only for MEMBERDB cluster configuration
#-------------------------------------------------------------------------------
oracle.install.crs.config.storageOption=               	
################################################################################
#                                                                              #
#                               SECTION F - IPMI                               #
#                                                                              #
################################################################################

#-------------------------------------------------------------------------------
# Specify 'true' if you would like to configure Intelligent Power Management interface
# (IPMI), else specify 'false'
#-------------------------------------------------------------------------------
oracle.install.crs.config.useIPMI=false

#-------------------------------------------------------------------------------
# Applicable only if you choose to configure IPMI
# i.e. oracle.install.crs.config.useIPMI=true
# Specify the username and password for using IPMI service
#-------------------------------------------------------------------------------
oracle.install.crs.config.ipmi.bmcUsername=
oracle.install.crs.config.ipmi.bmcPassword=
################################################################################
#                                                                              #
#                                SECTION G - ASM                               #
#                                                                              #
################################################################################

#-------------------------------------------------------------------------------
# ASM Storage Type
# Allowed values are : ASM and ASM_ON_NAS
# ASM_ON_NAS applicable only if
# oracle.install.crs.config.ClusterConfiguration=STANDALONE
#-------------------------------------------------------------------------------
oracle.install.asm.storageOption=ASM

#-------------------------------------------------------------------------------
# NAS location to create ASM disk group for storing OCR/VDSK 
# Specify the NAS location where you want the ASM disk group to be created
# to be used to store OCR/VDSK files
# Applicable only if oracle.install.asm.storageOption=ASM_ON_NAS
#-------------------------------------------------------------------------------
oracle.install.asmOnNAS.ocrLocation=
#------------------------------------------------------------------------------
# Create a separate ASM DiskGroup on NAS to store GIMR data
# Specify 'true' if you would like to separate GIMR data with clusterware data, else
# specify 'false'
# Applicable only if oracle.install.asm.storageOption=ASM_ON_NAS
#------------------------------------------------------------------------------
oracle.install.asmOnNAS.configureGIMRDataDG=false

#-------------------------------------------------------------------------------
# NAS location to create ASM disk group for storing GIMR data
# Specify the NAS location where you want the ASM disk group to be created
# to be used to store the GIMR database
# Applicable only if oracle.install.asm.storageOption=ASM_ON_NAS
# and oracle.install.asmOnNAS.configureGIMRDataDG=true
#-------------------------------------------------------------------------------
oracle.install.asmOnNAS.gimrLocation=

#-------------------------------------------------------------------------------
# Password for SYS user of Oracle ASM
#-------------------------------------------------------------------------------
oracle.install.asm.SYSASMPassword=

#-------------------------------------------------------------------------------
# The ASM DiskGroup
#
# Example: oracle.install.asm.diskGroup.name=data
#
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.name=DATA

#-------------------------------------------------------------------------------
# Redundancy level to be used by ASM.
# It can be one of the following  
#   - NORMAL
#   - HIGH
#   - EXTERNAL
#   - FLEX#   - EXTENDED (required if oracle.install.crs.config.ClusterConfiguration=EXTENDED)
# Example: oracle.install.asm.diskGroup.redundancy=NORMAL
#
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.redundancy=EXTERNAL

#-------------------------------------------------------------------------------
# Allocation unit size to be used by ASM.
# It can be one of the following values
#   - 1
#   - 2
#   - 4
#   - 8
#   - 16
# Example: oracle.install.asm.diskGroup.AUSize=4
# size unit is MB
#
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.AUSize=4

#-------------------------------------------------------------------------------
# Failure Groups for the disk group
# If configuring for Extended cluster specify as list of "failure group name:site"
# tuples.
# Else just specify as list of failure group names
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.FailureGroups=

#-------------------------------------------------------------------------------
# List of disks and their failure groups to create a ASM DiskGroup
# (Use this if each of the disks have an associated failure group)
# Failure Groups are not required if oracle.install.asm.diskGroup.redundancy=EXTERNAL
# Example:
#     For Unix based Operating System:
#     oracle.install.asm.diskGroup.disksWithFailureGroupNames=/oracle/asm/disk1,FGName,/oracle/asm/disk2,FGName
#     For Windows based Operating System:
#     oracle.install.asm.diskGroup.disksWithFailureGroupNames=\\.\ORCLDISKDATA0,FGName,\\.\ORCLDISKDATA1,FGName
#
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.disksWithFailureGroupNames=/dev/oracleasm/disks/DATA02,,/dev/oracleasm/disks/DATA03,,/dev/oracleasm/disks/DATA04,,/dev/oracleasm/disks/DATA01,

#-------------------------------------------------------------------------------
# List of disks to create a ASM DiskGroup
# (Use this variable only if failure groups configuration is not required)
# Example:
#     For Unix based Operating System:
#     oracle.install.asm.diskGroup.disks=/oracle/asm/disk1,/oracle/asm/disk2
#     For Windows based Operating System:
#     oracle.install.asm.diskGroup.disks=\\.\ORCLDISKDATA0,\\.\ORCLDISKDATA1
#
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.disks=/dev/oracleasm/disks/DATA02,/dev/oracleasm/disks/DATA03,/dev/oracleasm/disks/DATA04,/dev/oracleasm/disks/DATA01

#-------------------------------------------------------------------------------
# List of failure groups to be marked as QUORUM.
# Quorum failure groups contain only voting disk data, no user data is stored
# Example:
#	oracle.install.asm.diskGroup.quorumFailureGroupNames=FGName1,FGName2
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.quorumFailureGroupNames=
#-------------------------------------------------------------------------------
# The disk discovery string to be used to discover the disks used create a ASM DiskGroup
#
# Example:
#     For Unix based Operating System:
#     oracle.install.asm.diskGroup.diskDiscoveryString=/oracle/asm/*
#     For Windows based Operating System:
#     oracle.install.asm.diskGroup.diskDiscoveryString=\\.\ORCLDISK*
#
#-------------------------------------------------------------------------------
oracle.install.asm.diskGroup.diskDiscoveryString=/dev/oracleasm/disks/*

#-------------------------------------------------------------------------------
# Password for ASMSNMP account
# ASMSNMP account is used by Oracle Enterprise Manager to monitor Oracle ASM instances
#-------------------------------------------------------------------------------
oracle.install.asm.monitorPassword=

#-------------------------------------------------------------------------------
# GIMR Storage data ASM DiskGroup
# Applicable only when 
# oracle.install.asm.configureGIMRDataDG=true
# Example: oracle.install.asm.GIMRDG.name=MGMT
#
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.name=

#-------------------------------------------------------------------------------
# Redundancy level to be used by ASM.
# It can be one of the following  
#   - NORMAL
#   - HIGH
#   - EXTERNAL
#   - FLEX#   - EXTENDED (only if oracle.install.crs.config.ClusterConfiguration=EXTENDED)
# Example: oracle.install.asm.gimrDG.redundancy=NORMAL
#
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.redundancy=

#-------------------------------------------------------------------------------
# Allocation unit size to be used by ASM.
# It can be one of the following values
#   - 1
#   - 2
#   - 4
#   - 8
#   - 16
# Example: oracle.install.asm.gimrDG.AUSize=4
# size unit is MB
#
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.AUSize=1

#-------------------------------------------------------------------------------
# Failure Groups for the GIMR storage data ASM disk group
# If configuring for Extended cluster specify as list of "failure group name:site"
# tuples.
# Else just specify as list of failure group names
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.FailureGroups=

#-------------------------------------------------------------------------------
# List of disks and their failure groups to create GIMR data ASM DiskGroup
# (Use this if each of the disks have an associated failure group)
# Failure Groups are not required if oracle.install.asm.gimrDG.redundancy=EXTERNAL
# Example:
#     For Unix based Operating System:
#     oracle.install.asm.gimrDG.disksWithFailureGroupNames=/oracle/asm/disk1,FGName,/oracle/asm/disk2,FGName
#     For Windows based Operating System:
#     oracle.install.asm.gimrDG.disksWithFailureGroupNames=\\.\ORCLDISKDATA0,FGName,\\.\ORCLDISKDATA1,FGName
#
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.disksWithFailureGroupNames=

#-------------------------------------------------------------------------------
# List of disks to create GIMR data ASM DiskGroup
# (Use this variable only if failure groups configuration is not required)
# Example:
#     For Unix based Operating System:
#     oracle.install.asm.gimrDG.disks=/oracle/asm/disk1,/oracle/asm/disk2
#     For Windows based Operating System:
#     oracle.install.asm.gimrDG.disks=\\.\ORCLDISKDATA0,\\.\ORCLDISKDATA1
#
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.disks=

#-------------------------------------------------------------------------------
# List of failure groups to be marked as QUORUM.
# Quorum failure groups contain only voting disk data, no user data is stored
# Example:
#	oracle.install.asm.gimrDG.quorumFailureGroupNames=FGName1,FGName2
#-------------------------------------------------------------------------------
oracle.install.asm.gimrDG.quorumFailureGroupNames=

#-------------------------------------------------------------------------------
# Configure AFD - ASM Filter Driver
# Applicable only for FLEX_ASM_STORAGE option
# Specify 'true' if you want to configure AFD, else specify 'false'
#-------------------------------------------------------------------------------
oracle.install.asm.configureAFD=false
#-------------------------------------------------------------------------------
# Configure RHPS - Rapid Home Provisioning Service
# Applicable only for DOMAIN cluster configuration
# Specify 'true' if you want to configure RHP service, else specify 'false'
#-------------------------------------------------------------------------------
oracle.install.crs.configureRHPS=false

################################################################################
#                                                                              #
#                             SECTION H - UPGRADE                              #
#                                                                              #
################################################################################
#-------------------------------------------------------------------------------
# Specify whether to ignore down nodes during upgrade operation.
# Value should be 'true' to ignore down nodes otherwise specify 'false'
#-------------------------------------------------------------------------------
oracle.install.crs.config.ignoreDownNodes=false               	
################################################################################
#                                                                              #
#                               MANAGEMENT OPTIONS                             #
#                                                                              #
################################################################################

#-------------------------------------------------------------------------------
# Specify the management option to use for managing Oracle Grid Infrastructure
# Options are:
# 1. CLOUD_CONTROL - If you want to manage your Oracle Grid Infrastructure with Enterprise Manager Cloud Control.
# 2. NONE   -If you do not want to manage your Oracle Grid Infrastructure with Enterprise Manager Cloud Control.
#-------------------------------------------------------------------------------
oracle.install.config.managementOption=NONE

#-------------------------------------------------------------------------------
# Specify the OMS host to connect to Cloud Control.
# Applicable only when oracle.install.config.managementOption=CLOUD_CONTROL
#-------------------------------------------------------------------------------
oracle.install.config.omsHost=

#-------------------------------------------------------------------------------
# Specify the OMS port to connect to Cloud Control.
# Applicable only when oracle.install.config.managementOption=CLOUD_CONTROL
#-------------------------------------------------------------------------------
oracle.install.config.omsPort=0

#-------------------------------------------------------------------------------
# Specify the EM Admin user name to use to connect to Cloud Control.
# Applicable only when oracle.install.config.managementOption=CLOUD_CONTROL
#-------------------------------------------------------------------------------
oracle.install.config.emAdminUser=

#-------------------------------------------------------------------------------
# Specify the EM Admin password to use to connect to Cloud Control.
# Applicable only when oracle.install.config.managementOption=CLOUD_CONTROL
#-------------------------------------------------------------------------------
oracle.install.config.emAdminPassword=
################################################################################
#                                                                              #
#                      Root script execution configuration                     #
#                                                                              #
################################################################################

#-------------------------------------------------------------------------------------------------------
# Specify the root script execution mode.
#
#   - true  : To execute the root script automatically by using the appropriate configuration methods.
#   - false : To execute the root script manually.
#
# If this option is selected, password should be specified on the console.
#-------------------------------------------------------------------------------------------------------
oracle.install.crs.rootconfig.executeRootScript=true

#--------------------------------------------------------------------------------------
# Specify the configuration method to be used for automatic root script execution.
#
# Following are the possible choices:
#   - ROOT
#   - SUDO
#--------------------------------------------------------------------------------------
oracle.install.crs.rootconfig.configMethod=ROOT
#--------------------------------------------------------------------------------------
# Specify the absolute path of the sudo program.
#
# Applicable only when SUDO configuration method was chosen.
#--------------------------------------------------------------------------------------
oracle.install.crs.rootconfig.sudoPath=

#--------------------------------------------------------------------------------------
# Specify the name of the user who is in the sudoers list. 
#
# Applicable only when SUDO configuration method was chosen.
#--------------------------------------------------------------------------------------
oracle.install.crs.rootconfig.sudoUserName=
#--------------------------------------------------------------------------------------
# Specify the nodes batch map.
#
# This should be a comma separated list of node:batch pairs.
# During upgrade, you can sequence the automatic execution of root scripts
# by pooling the nodes into batches. 
# A maximum of three batches can be specified. 
# Installer will execute the root scripts on all the nodes in one batch before
# proceeding to next batch.
# Root script execution on the local node must be in Batch 1.
# Only one type of node role can be used for each batch.
# Root script execution should be done first in all HUB nodes and then, when
# existent, in all the LEAF nodes.
#
# Examples:
# 1. oracle.install.crs.config.batchinfo=HUBNode1:1,HUBNode2:2,HUBNode3:2,LEAFNode4:3
# 2. oracle.install.crs.config.batchinfo=HUBNode1:1,LEAFNode2:2,LEAFNode3:2,LEAFNode4:2
# 3. oracle.install.crs.config.batchinfo=HUBNode1:1,HUBNode2:1,LEAFNode3:2,LEAFNode4:3
#
# Applicable only for UPGRADE install option. 
#--------------------------------------------------------------------------------------
oracle.install.crs.config.batchinfo=
################################################################################
#                                                                              #
#                           APPLICATION CLUSTER OPTIONS                        #
#                                                                              #
################################################################################

#-------------------------------------------------------------------------------
# Specify the Virtual hostname to configure virtual access for your Application
# The value to be specified for Virtual hostname is optional.
#-------------------------------------------------------------------------------
oracle.install.crs.app.applicationAddress=
