Good ones
https://victorops.com/site-reliability-engineering/guide-to-sre-and-the-four-golden-signals-of-monitoring/
https://thenewstack.io/a-site-reliability-engineers-advice-on-what-breaks-our-systems/

Google books
SRE google's: 
- SRE Book: https://sre.google/workbook/table-of-contents/
- SRE Workbook: https://sre.google/workbook/table-of-contents/
Oreilly etc: https://sre.google/books/
- https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf

Also see:
- https://www.gremlin.com/site-reliability-engineering/the-role-and-responsibilities-of-sres-in-software-engineering/

================
Key things
================

SLO
SLI
Error budget
Risk - cost vs benefit of improving reliability, 'as much as needed only' - maybe exceed a bit, not by too much (to reduce unnecessary cost and opportunity cost)
Tech debt
Reduce operational costs

reliability, performance, utilization

Golden Signals (monitoring): Latency, Traffic, Errors, Saturation (capacity)



================
Observability
================
Down
Hot
Slow
Errors
Supplementary metrics

Host - cpu, mem, disk space, cpu-steals, ping hostname, ping ip addr, connectivity to specific ports(ssh, 80, 443 etc)

================
BREAKING POINTS
================
https://thenewstack.io/a-site-reliability-engineers-advice-on-what-breaks-our-systems/

The “taxonomy” breaks catastrophic events into six categories.
Hitting limits
Spreading Slowness
Thundering Herds
Automation Interactions
Cyberattacks
Dependency Problems

Circuit breakers / Break-offs 
- dont keep on retrying forever hitting authentication system or dns and pose too much load on them
- instead, try a few times, put up a sorry page, send an alert
- Another solution: implement a circuit-breaker pattern. “Instead of dealing with retries on a per-request basis, you put a little widget of software in front that sees all the requests going out from a particular client, which might be serving many many user requests, to a backend service. And if it sees across all these requests that the back end service is not healthy, it’ll fail those requests fast, before they even go to the unhealthy service.”

One system can break other
- slow response from a called system ended up in too many connections piling up in load balancer - in ideal scenario, those would have come, worked and gone...

======================================
GOOGLE BOOK NOTES
======================================
Typical application/infra architecture reference: https://sre.google/sre-book/production-environment/#xref_production-environment_shakespeare

--------------------
Tenets of SRE
--------------------
availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning

Durable Focus on Engineering
Google caps operational work for SREs at 50% of their time
SREs should receive a maximum of two events per 8–12-hour on-call shift
blame-free postmortem culture
exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them

Maximum Change Velocity Without Violating a Service’s SLO
error budget
100% SLO is unrealistic, lost in noise
What level of availability will the users be happy with, given how they use the product?
What alternatives are available to users who are dissatisfied with the product’s availability?
What happens to users’ usage of the product at different availability levels?

Monitoring and Alerting
Monitor for specific condition - but alerting/emailing every one of them is not efficient - alert only for actionable ones (alerting program should decide that)
Alerts - where immediate action is needed
Tickets - where delayed action is ok
Logging - for forensic purposes (like DB response time, kafka read/write time ...)

Emergency Response - mean time to failure (MTTF) and mean time to repair (MTTR)
Reduce human intervention or perform somehow until humans intervene - throttles, shutting of certain non-priority functions ...

Change Management
Implementing progressive rollouts
Quickly and accurately detecting problems
Rolling back changes safely when problems arise

Demand Forecasting and Capacity Planning
An accurate organic demand forecast, which extends beyond the lead time required for acquiring capacity
An accurate incorporation of inorganic demand sources into the demand forecast
Regular load testing of the system to correlate raw capacity(servers, disks, and so on) to service capacity

Provisioning
Do quickly and correctly
Do only when necessary - as capacity is expensive

Efficiency and Performance
utilization
capacity target at a specific response speed

------------------------
PRODUCTION ENVIRONMENT
------------------------
https://sre.google/sre-book/production-environment/

Software Infrastructure
make the most efficient use of hardware infrastructure
code heavily multithreaded, so one task can easily use many cores. 
To facilitate dashboards, monitoring, and debugging, every server has an HTTP server that provides diagnostics and statistics for a given task.

Development Environment
  Apart from a few groups that have their own open source repositories (e.g., Android and Chrome), Google Software Engineers work from a single shared repository [Pot16]. This has a few important practical implications for our workflows:

  If engineers encounter a problem in a component outside of their project, they can fix the problem, send the proposed changes ("changelist," or CL) to the owner for review, and submit the CL to the mainline.
  Changes to source code in an engineer’s own project require a review. All software is reviewed before being submitted.
  
  When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it notifies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.

Shakespeare: A Sample Service
To provide a model of how a service would hypothetically be deployed in the Google production environment, let’s look at an example service that interacts with multiple Google technologies. Suppose we want to offer a service that lets you determine where a given word is used throughout all of Shakespeare’s works.

      We can divide this system into two parts:

      A batch component that reads all of Shakespeare’s texts, creates an index, and writes the index into a Bigtable. 
      This job need only run once, or perhaps very infrequently (as you never know if a new text might be discovered!).
      An application frontend that handles end-user requests. This job is always up, as users in all time zones will want to search in Shakespeare’s books.
      The batch component is a MapReduce comprising three phases.

      The mapping phase reads Shakespeare’s texts and splits them into individual words. This is faster if performed in parallel by multiple workers.
      The shuffle phase sorts the tuples by word.
      In the reduce phase, a tuple of (word, list of locations) is created.
      Each tuple is written to a row in a Bigtable, using the word as the key.

Job and Data Organization
      Load testing determined that our backend server can handle about 100 queries per second (QPS). Trials performed with a limited set of users lead us to expect a peak load of about 3,470 QPS, so we need at least 35 tasks. However, the following considerations mean that we need at least 37 tasks in the job, or N+2:

      During updates, one task at a time will be unavailable, leaving 36 tasks.
      A machine failure might occur during a task update, leaving only 35 tasks, just enough to serve peak load.13
      A closer examination of user traffic shows our peak usage is distributed globally: 1,430 QPS from North America, 290 from South America, 1,400 from Europe and Africa, and 350 from Asia and Australia. Instead of locating all backends at one site, we distribute them across the USA, South America, Europe, and Asia. Allowing for N+2 redundancy per region means that we end up with 17 tasks in the USA, 16 in Europe, and 6 in Asia. However, we decide to use 4 tasks (instead of 5) in South America, to lower the overhead of N+2 to N+1. In this case, we’re willing to tolerate a small risk of higher latency in exchange for lower hardware costs: if GSLB redirects traffic from one continent to another when our South American datacenter is over capacity, we can save 20% of the resources we’d spend on hardware. In the larger regions, we’ll spread tasks across two or three clusters for extra resiliency.

      Because the backends need to contact the Bigtable holding the data, we need to also design this storage element strategically. A backend in Asia contacting a Bigtable in the USA adds a significant amount of latency, so we replicate the Bigtable in each region. Bigtable replication helps us in two ways: it provides resilience should a Bigtable server fail, and it lowers data-access latency. While Bigtable only offers eventual consistency, it isn’t a major problem because we don’t need to update the contents often.

      We’ve introduced a lot of terminology here; while you don’t need to remember it all, it’s useful for framing many of the other systems we’ll refer to later.

-----------------------------------
RISK
-----------------------------------
https://sre.google/sre-book/embracing-risk/

AVAILABILITY
- service availability vs system availability
- in the case of single unclustered system system availability can be same as service availability
- however, in distributed clusters with shards, redundancy and load balancing + retries, individual nodes/elements may not result in service being down
- system availability = uptime / (uptime + downtime)
- servie availability = successful requests / (successful + failed requests)

COST-BENEFIT
Improvement from 3 9's to 4 9's: 99.9% to 99.99%
- improvement = .09% = 0.0009
- if the system is worth 1 million, this will be 900 dollars
- if improvement saves more than 900 dollars then it may be worthwhile doing it

COST OF DOING IMPROVEMENT
- additional equipment, coding and manpower to improve reliability
- opportunity cost - the same resources could have been used to develop something new that could generate revenue

MANAGING COSTS
- tiered hardware 
-- low latency, high availability for online/real-time requests
-- high throughput, lower availability for offline/batch requests which can wait or take much time

- tiered service
-- quicker expert response for realtime systems
-- slower tiered response for low-priority systems

RISK TOLERANCE
What level of availability is required?
Do different types of failures have different effects on the service?
How can we use the service cost to help locate a service on the risk continuum?
What other service metrics are important to take into account?

What level of service will the users expect?
Does this service tie directly to revenue (either our revenue, or our customers’ revenue)?
Is this a paid service, or is it free?
If there are competitors in the marketplace, what level of service do those competitors provide?
Is this service targeted at consumers, or at enterprises?

TYPES OF FAILURES
- one big outage vs several small outages/issues 
-- both may result in same total SLO, but effect may be different
- issue that results in profile-pics missing - vs - issue that exposes confidential personal info
-- number of events may be same - but, effects are different

METRICS AND SERVICE EXPECTATIONS
- a peculiar case could be main-page vs inserted-ad's
-- main page is more important for user-experience than inserted-ad's
-- the main page component should take higher precedence with low latecy
-- inserted-ad's can render a bit later than main content of the page

- in a batch system like orders management
-- individual steps may not so much affect the final delivery of the order much
-- but, each step should not be let pile-up as well carelessly - as such pile up may cascade latency and delay at some point

ERROR BUDGET
- define error budget of uptime, queries failed, requests served etc
- failures consume the budget 
- new releases are risky - so dev teams may then reduce releases, increase testing so that error budget is not exceeded
- when necessary SLOs/error-budgets may be loosened per needs of business (cost vs lost opportunity, client service ...)

----------------------------------
SLOs
----------------------------------
https://sre.google/sre-book/service-level-objectives/

SLIs
- Indicators
- Availability - uptime
- Availability - requests served successfully (alternatively error rate)
- Latency
- Throughput
- Durability - data is available/not-lost (especially in storage, database, data-stores)

Latency and throughput may contra-affect each other
- So, it could be acceptable latency at certain necessary throughput

SLOs
- Objectives
- single limit or a lower and an upper limit
- SLI ≤ target - or - lower bound ≤ SLI ≤ upper bound

-----------------------
TOIL
-----------------------
1. If a human operator needs to touch your system during normal operations, you have a bug. 
2. The definition of normal changes as your systems grow.

Toil is NOT overhead
- overhead is something that has to be done - like paperwork

Toil is the kind of work tied to running a production service that tends to be 
manual 
repetitive - not the first or second time and I am done, it happens repeatedly
automatable
tactical - "interrupt" driven & "reactive" 
devoid of enduring value, 
scales linearly as a service grows

Google goal - below 50% of each SRE’s time

At least 50% of each SRE’s time should be spent on engineering project work that will
- either reduce future toil 
- add service features
-- Feature development typically focuses on improving reliability, performance, or utilization
-- which often reduces toil as a second-order effect

Engineering work is what 
- enables the SRE organization to scale up "sublinearly" with service size - than just a dev team or an ops team
- We need to keep that promise by not allowing the SRE] to devolve into an Ops team

Toil left unchecked can end up taking 100% of team's time

CALCULATING TOIL
- oncall days (primary and backup) = toil
- non-urgent messages/emails
- response to issues
- releases

WHAT QUALIFIES AS ENGINEERING

- permanent improvement in your service
- guided by a strategy
- creative and innovative
- design-driven approach to solving a problem

Software engineering
- create/modify code
- create framework for development teams
- adding service features for scalability and reliability
- modifying infrastructure code to make it more robust

Systems engineering
- setup/modify configuration
- documentation
- tuning
- load balancer setup
- monitoring setup
- consulting on architecture, design, and productionization for developer teams

Versus:
  Toil
  Work directly tied to running a service that is repetitive, manual, etc.

  Overhead
  Administrative work not tied directly to running a service. 
  Examples include hiring, HR paperwork, team/company meetings, bug queue hygiene, snippets, peer reviews and self-assessments, and training courses.
  
------------------------------------
MONITORING
------------------------------------
https://sre.google/sre-book/monitoring-distributed-systems/

Golden Signals
- Latency
- Traffic
- Errors
- Saturation (capacity)

Symptoms vs Causes
- Symptoms - golden signals
- Causes - various
-- db bad sql
-- db down
-- kafka nodes down
-- ...
...

Whitebox monitoring
- from internal info of the systems
- like logs, db perf views, perf commands run on the systems, process states ...

Blackbox monitoring
- external symptoms
- like up/down, reponse from http endpoint, tcp socket connection ok/not-ok

Synthetic monitoring
Real-user monitoring

Pushed changes (CONFIGURATION CHANGES)
- recent changes
- release implementations
- sql plan changes

Use
- detect issues
- alerting
- trending/forecasting
- detect near-misses
- compare similar infra in other data center, other systems
- dashboards
- metric correlation

Keep noise low - EVERY PAGE SHOULD BE ACTIONABLE
Get sufficient details
Uniform granularity

Ranges of thresholds
- p95, p99 etc
- histograms of 10-20, 21-30 ms and so on

PITFALLS
- means/averages hide spikes and worst-performing P99's and such
- too much forced noise becomes too time consuming and distraction from many other problems

----------------------------------
PRACTICAL ALERTING
----------------------------------
https://sre.google/sre-book/practical-alerting/

SYNTHETIC MONITORING
Borgmon also records "synthetic" variables for each target in order to identify:
  If the name was resolved to a host and port
  If the target responded to a collection
  If the target responded to a health check
  What time the collection finished

Borgmon rules, consists of simple algebraic expressions that compute time-series from other time-series

Counters
Guages
Timers

Practical thresholds
- Example: alert when the error ratio over 10 minutes exceeds 1% and the total number of errors exceeds 1 per second
- Example: if failure persists for N minutes/seconds

--------------------------------------------
EFFECTIVE TROUBLESHOOTING (TBD - read the chapter once again)
--------------------------------------------
https://sre.google/sre-book/effective-troubleshooting/

Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. 
- Ignore that instinct!

Instead, your course of action should be:
- to make the system work as well as it can under the circumstances. 
- emergency options, such as 
-- diverting traffic from a broken cluster to others that are still working, 
-- dropping traffic wholesale to prevent a cascading failure, 
-- disabling subsystems to lighten the load. 
-- Stopping the bleeding should be your first priority; 
-- NOTE: you aren’t helping your users if the system dies while you’re root-causing.
-- HOWEVER, an emphasis on rapid triage doesn’t preclude taking steps to preserve evidence of what’s going wrong, such as logs, to help with subsequent root-cause analysis.

Making Troubleshooting Easier
There are many ways to simplify and speed troubleshooting. Perhaps the most fundamental are:
- Building observability—with both white-box metrics and structured logs—into each component from the ground up
- Designing systems with well-understood and observable interfaces between components.

Case Study
App Engine,73 part of Google’s Cloud Platform, is a platform-as-a-service product that allows developers to build services atop Google’s infrastructure. One of our internal customers filed a problem report indicating that they’d recently seen a dramatic increase in latency, CPU usage, and number of running processes needed to serve traffic for their app, a content-management system used to build documentation for developers.74 The customer couldn’t find any recent changes to their code that correlated with the increase in resources, and there hadn’t been an increase in traffic to their app (see Figure 12-3), so they were wondering if a change in the App Engine service was responsible.

Our investigation discovered that latency had indeed increased by nearly an order of magnitude (as shown in Figure 12-4). Simultaneously, the amount of CPU time (Figure 12-5) and number of serving processes (Figure 12-6) had nearly quadrupled. Clearly something was wrong. It was time to start troubleshooting.

Typically a sudden increase in latency and resource usage indicates either an increase in traffic sent to the system or a change in system configuration. However, we could easily rule out both of these possible causes: while a spike in traffic to the app around 20:45 could explain a brief surge in resource usage, we’d expect traffic to return to baseline fairly soon after request volume normalized. This spike certainly shouldn’t have continued for multiple days, beginning when the app’s developers filed the report and we started looking into the problem. Second, the change in performance happened on Saturday, when neither changes to the app nor the production environment were in flight. The service’s most recent code pushes and configuration pushes had completed days before. Furthermore, if the problem originated with the service, we’d expect to see similar effects on other apps using the same infrastructure. However, no other apps were experiencing similar effects.

We referred the problem report to our counterparts, App Engine’s developers, to investigate whether the customer was encountering any idiosyncrasies in the serving infrastructure. The developers weren’t able to find any oddities, either. However, a developer did notice a correlation between the latency increase and the increase of a specific data storage API call, merge_join, which often indicates suboptimal indexing when reading from the datastore. Adding a composite index on the properties the app uses to select objects from the datastore would speed those requests, and in principle, speed the application as a whole—but we’d need to figure out which properties needed indexing. A quick look at the application’s code didn’t reveal any obvious suspects.

It was time to pull out the heavy machinery in our toolkit: using Dapper [Sig10], we traced the steps individual HTTP requests took—from their receipt by a frontend reverse proxy through to the point where the app’s code returned a response—and looked at the RPCs issued by each server involved in handling that request. Doing so would allow us to see which properties were included in requests to the datastore, then create the appropriate indices.

While investigating, we discovered that requests for static content such as images, which weren’t served from the datastore, were also much slower than expected. Looking at graphs with file-level granularity, we saw their responses had been much faster only a few days before. This implied that the observed correlation between merge_join and the latency increase was spurious and that our suboptimal-indexing theory was fatally flawed.

Examining the unexpectedly slow requests for static content, most of the RPCs sent from the application were to a memcache service, so the requests should have been very fast—on the order of a few milliseconds. These requests did turn out to be very fast, so the problem didn’t seem to originate there. However, between the time the app started working on a request and when it made the first RPCs, there was about a 250 ms period where the app was doing…well, something. Because App Engine runs code provided by users, its SRE team does not profile or inspect app code, so we couldn’t tell what the app was doing in that interval; similarly, Dapper couldn’t help track down what was going on since it can only trace RPC calls, and none were made during that period.

Faced with what was, by this point, quite a mystery, we decided not to solve it…yet. The customer had a public launch scheduled for the following week, and we weren’t sure how soon we’d be able to identify the problem and fix it. Instead, we recommended that the customer increase the resources allocated to their app to the most CPU-rich instance type available. Doing so reduced the app’s latency to acceptable levels, though not as low as we’d prefer. We concluded that the latency mitigation was good enough that the team could conduct their launch successfully, then investigate at leisure.75

At this point, we suspected that the app was a victim of yet another common cause of sudden increases in latency and resource usage: a change in the type of work. We’d seen an increase in writes to the datastore from the app, just before its latency increased, but because this increase wasn’t very large—nor was it sustained—we’d written it off as coincidental. However, this behavior did resemble a common pattern: an instance of the app is initialized by reading objects from the datastore, then storing them in the instance’s memory. By doing so, the instance avoids reading rarely changing configuration from the datastore on each request, and instead checks the in-memory objects. Then, the time it takes to handle requests will often scale with the amount of configuration data.76 We couldn’t prove that this behavior was the root of the problem, but it’s a common antipattern.

The app developers added instrumentation to understand where the app was spending its time. They identified a method that was called on every request, that checked whether a user had whitelisted access to a given path. The method used a caching layer that sought to minimize accesses to both the datastore and the memcache service, by holding whitelist objects in instances’ memory. As one of the app’s developers noted in the investigation, “I don’t know where the fire is yet, but I’m blinded by smoke coming from this whitelist cache.”

Some time later, the root cause was found: due to a long-standing bug in the app’s access control system, whenever one specific path was accessed, a whitelist object would be created and stored in the datastore. In the run-up to launch, an automated security scanner had been testing the app for vulnerabilities, and as a side effect, its scan produced thousands of whitelist objects over the course of half an hour. These superfluous whitelist objects then had to be checked on every request to the app, which led to pathologically slow responses—without causing any RPC calls from the app to other services. Fixing the bug and removing those objects returned the app’s performance to expected levels.
