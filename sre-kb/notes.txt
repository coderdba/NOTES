Good ones
https://victorops.com/site-reliability-engineering/guide-to-sre-and-the-four-golden-signals-of-monitoring/
https://thenewstack.io/a-site-reliability-engineers-advice-on-what-breaks-our-systems/

Google books
SRE google's: 
- SRE Book: https://sre.google/workbook/table-of-contents/
- SRE Workbook: https://sre.google/workbook/table-of-contents/
Oreilly etc: https://sre.google/books/
- https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf

================
Key things
================

SLO
SLI
Error budget
Risk - cost vs benefit of improving reliability, 'as much as needed only' - maybe exceed a bit, not by too much (to reduce unnecessary cost and opportunity cost)
Tech debt
Reduce operational costs

================
Observability
================
Down
Hot
Slow
Errors
Supplementary metrics

Host - cpu, mem, disk space, cpu-steals, ping hostname, ping ip addr, connectivity to specific ports(ssh, 80, 443 etc)

================
BREAKING POINTS
================
https://thenewstack.io/a-site-reliability-engineers-advice-on-what-breaks-our-systems/

The “taxonomy” breaks catastrophic events into six categories.
Hitting limits
Spreading Slowness
Thundering Herds
Automation Interactions
Cyberattacks
Dependency Problems

Circuit breakers / Break-offs 
- dont keep on retrying forever hitting authentication system or dns and pose too much load on them
- instead, try a few times, put up a sorry page, send an alert
- Another solution: implement a circuit-breaker pattern. “Instead of dealing with retries on a per-request basis, you put a little widget of software in front that sees all the requests going out from a particular client, which might be serving many many user requests, to a backend service. And if it sees across all these requests that the back end service is not healthy, it’ll fail those requests fast, before they even go to the unhealthy service.”

One system can break other
- slow response from a called system ended up in too many connections piling up in load balancer - in ideal scenario, those would have come, worked and gone...

======================================
GOOGLE BOOK NOTES
======================================
--------------------
Tenets of SRE
--------------------
availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning

Durable Focus on Engineering
Google caps operational work for SREs at 50% of their time
SREs should receive a maximum of two events per 8–12-hour on-call shift
blame-free postmortem culture
exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them

Maximum Change Velocity Without Violating a Service’s SLO
error budget
100% SLO is unrealistic, lost in noise
What level of availability will the users be happy with, given how they use the product?
What alternatives are available to users who are dissatisfied with the product’s availability?
What happens to users’ usage of the product at different availability levels?

Monitoring and Alerting
Monitor for specific condition - but alerting/emailing every one of them is not efficient - alert only for actionable ones (alerting program should decide that)
Alerts - where immediate action is needed
Tickets - where delayed action is ok
Logging - for forensic purposes (like DB response time, kafka read/write time ...)

Emergency Response - mean time to failure (MTTF) and mean time to repair (MTTR)
Reduce human intervention or perform somehow until humans intervene - throttles, shutting of certain non-priority functions ...

Change Management
Implementing progressive rollouts
Quickly and accurately detecting problems
Rolling back changes safely when problems arise

Demand Forecasting and Capacity Planning
An accurate organic demand forecast, which extends beyond the lead time required for acquiring capacity
An accurate incorporation of inorganic demand sources into the demand forecast
Regular load testing of the system to correlate raw capacity(servers, disks, and so on) to service capacity

Provisioning
Do quickly and correctly
Do only when necessary - as capacity is expensive

Efficiency and Performance
utilization
capacity target at a specific response speed

------------------------
PRODUCTION ENVIRONMENT
------------------------
https://sre.google/sre-book/production-environment/

Software Infrastructure
make the most efficient use of hardware infrastructure
code heavily multithreaded, so one task can easily use many cores. 
To facilitate dashboards, monitoring, and debugging, every server has an HTTP server that provides diagnostics and statistics for a given task.

Development Environment
  Apart from a few groups that have their own open source repositories (e.g., Android and Chrome), Google Software Engineers work from a single shared repository [Pot16]. This has a few important practical implications for our workflows:

  If engineers encounter a problem in a component outside of their project, they can fix the problem, send the proposed changes ("changelist," or CL) to the owner for review, and submit the CL to the mainline.
  Changes to source code in an engineer’s own project require a review. All software is reviewed before being submitted.
  
  When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it notifies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.

Shakespeare: A Sample Service
To provide a model of how a service would hypothetically be deployed in the Google production environment, let’s look at an example service that interacts with multiple Google technologies. Suppose we want to offer a service that lets you determine where a given word is used throughout all of Shakespeare’s works.

      We can divide this system into two parts:

      A batch component that reads all of Shakespeare’s texts, creates an index, and writes the index into a Bigtable. 
      This job need only run once, or perhaps very infrequently (as you never know if a new text might be discovered!).
      An application frontend that handles end-user requests. This job is always up, as users in all time zones will want to search in Shakespeare’s books.
      The batch component is a MapReduce comprising three phases.

      The mapping phase reads Shakespeare’s texts and splits them into individual words. This is faster if performed in parallel by multiple workers.
      The shuffle phase sorts the tuples by word.
      In the reduce phase, a tuple of (word, list of locations) is created.
      Each tuple is written to a row in a Bigtable, using the word as the key.

Job and Data Organization
      Load testing determined that our backend server can handle about 100 queries per second (QPS). Trials performed with a limited set of users lead us to expect a peak load of about 3,470 QPS, so we need at least 35 tasks. However, the following considerations mean that we need at least 37 tasks in the job, or N+2:

      During updates, one task at a time will be unavailable, leaving 36 tasks.
      A machine failure might occur during a task update, leaving only 35 tasks, just enough to serve peak load.13
      A closer examination of user traffic shows our peak usage is distributed globally: 1,430 QPS from North America, 290 from South America, 1,400 from Europe and Africa, and 350 from Asia and Australia. Instead of locating all backends at one site, we distribute them across the USA, South America, Europe, and Asia. Allowing for N+2 redundancy per region means that we end up with 17 tasks in the USA, 16 in Europe, and 6 in Asia. However, we decide to use 4 tasks (instead of 5) in South America, to lower the overhead of N+2 to N+1. In this case, we’re willing to tolerate a small risk of higher latency in exchange for lower hardware costs: if GSLB redirects traffic from one continent to another when our South American datacenter is over capacity, we can save 20% of the resources we’d spend on hardware. In the larger regions, we’ll spread tasks across two or three clusters for extra resiliency.

      Because the backends need to contact the Bigtable holding the data, we need to also design this storage element strategically. A backend in Asia contacting a Bigtable in the USA adds a significant amount of latency, so we replicate the Bigtable in each region. Bigtable replication helps us in two ways: it provides resilience should a Bigtable server fail, and it lowers data-access latency. While Bigtable only offers eventual consistency, it isn’t a major problem because we don’t need to update the contents often.

      We’ve introduced a lot of terminology here; while you don’t need to remember it all, it’s useful for framing many of the other systems we’ll refer to later.

-----------------------------------
RISK
-----------------------------------
https://sre.google/sre-book/embracing-risk/

AVAILABILITY
- service availability vs system availability
- in the case of single unclustered system system availability can be same as service availability
- however, in distributed clusters with shards, redundancy and load balancing + retries, individual nodes/elements may not result in service being down
- system availability = uptime / (uptime + downtime)
- servie availability = successful requests / (successful + failed requests)

COST-BENEFIT
Improvement from 3 9's to 4 9's: 99.9% to 99.99%
- improvement = .09% = 0.0009
- if the system is worth 1 million, this will be 900 dollars
- if improvement saves more than 900 dollars then it may be worthwhile doing it

COST OF DOING IMPROVEMENT
- additional equipment, coding and manpower to improve reliability
- opportunity cost - the same resources could have been used to develop something new that could generate revenue

MANAGING COSTS
- tiered hardware 
-- low latency, high availability for online/real-time requests
-- high throughput, lower availability for offline/batch requests which can wait or take much time

- tiered service
-- quicker expert response for realtime systems
-- slower tiered response for low-priority systems

RISK TOLERANCE
What level of availability is required?
Do different types of failures have different effects on the service?
How can we use the service cost to help locate a service on the risk continuum?
What other service metrics are important to take into account?

What level of service will the users expect?
Does this service tie directly to revenue (either our revenue, or our customers’ revenue)?
Is this a paid service, or is it free?
If there are competitors in the marketplace, what level of service do those competitors provide?
Is this service targeted at consumers, or at enterprises?

TYPES OF FAILURES
- one big outage vs several small outages/issues 
-- both may result in same total SLO, but effect may be different
- issue that results in profile-pics missing - vs - issue that exposes confidential personal info
-- number of events may be same - but, effects are different

METRICS AND SERVICE EXPECTATIONS
- a peculiar case could be main-page vs inserted-ad's
-- main page is more important for user-experience than inserted-ad's
-- the main page component should take higher precedence with low latecy
-- inserted-ad's can render a bit later than main content of the page

- in a batch system like orders management
-- individual steps may not so much affect the final delivery of the order much
-- but, each step should not be let pile-up as well carelessly - as such pile up may cascade latency and delay at some point

ERROR BUDGET
- define error budget of uptime, queries failed, requests served etc
- failures consume the budget 
- new releases are risky - so dev teams may then reduce releases, increase testing so that error budget is not exceeded
- when necessary SLOs/error-budgets may be loosened per needs of business (cost vs lost opportunity, client service ...)


