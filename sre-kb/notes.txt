Good ones
https://victorops.com/site-reliability-engineering/guide-to-sre-and-the-four-golden-signals-of-monitoring/
https://thenewstack.io/a-site-reliability-engineers-advice-on-what-breaks-our-systems/


================
Observability
================
Down
Hot
Slow
Errors
Supplementary metrics

Host - cpu, mem, disk space, cpu-steals, ping hostname, ping ip addr, connectivity to specific ports(ssh, 80, 443 etc)

================
BREAKING POINTS
================
https://thenewstack.io/a-site-reliability-engineers-advice-on-what-breaks-our-systems/

The “taxonomy” breaks catastrophic events into six categories.
Hitting limits
Spreading Slowness
Thundering Herds
Automation Interactions
Cyberattacks
Dependency Problems

Circuit breakers / Break-offs 
- dont keep on retrying forever hitting authentication system or dns and pose too much load on them
- instead, try a few times, put up a sorry page, send an alert
- Another solution: implement a circuit-breaker pattern. “Instead of dealing with retries on a per-request basis, you put a little widget of software in front that sees all the requests going out from a particular client, which might be serving many many user requests, to a backend service. And if it sees across all these requests that the back end service is not healthy, it’ll fail those requests fast, before they even go to the unhealthy service.”

One system can break other
- slow response from a called system ended up in too many connections piling up in load balancer - in ideal scenario, those would have come, worked and gone...

======================================
GOOGLE BOOK NOTES
======================================
Tenets of SRE
availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning

Durable Focus on Engineering
Google caps operational work for SREs at 50% of their time
SREs should receive a maximum of two events per 8–12-hour on-call shift
blame-free postmortem culture
exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them

Maximum Change Velocity Without Violating a Service’s SLO
error budget
100% SLO is unrealistic, lost in noise
What level of availability will the users be happy with, given how they use the product?
What alternatives are available to users who are dissatisfied with the product’s availability?
What happens to users’ usage of the product at different availability levels?

Monitoring and Alerting
Monitor for specific condition - but alerting/emailing every one of them is not efficient - alert only for actionable ones (alerting program should decide that)
Alerts - where immediate action is needed
Tickets - where delayed action is ok
Logging - for forensic purposes (like DB response time, kafka read/write time ...)

Emergency Response - mean time to failure (MTTF) and mean time to repair (MTTR)
Reduce human intervention or perform somehow until humans intervene - throttles, shutting of certain non-priority functions ...

Change Management
Implementing progressive rollouts
Quickly and accurately detecting problems
Rolling back changes safely when problems arise

Demand Forecasting and Capacity Planning
An accurate organic demand forecast, which extends beyond the lead time required for acquiring capacity
An accurate incorporation of inorganic demand sources into the demand forecast
Regular load testing of the system to correlate raw capacity(servers, disks, and so on) to service capacity

Provisioning
Do quickly and correctly
Do only when necessary - as capacity is expensive

Efficiency and Performance
utilization
capacity target at a specific response speed

PRODUCTION ENVIRONMENT
https://sre.google/sre-book/production-environment/

Software Infrastructure
make the most efficient use of hardware infrastructure
code heavily multithreaded, so one task can easily use many cores. 
To facilitate dashboards, monitoring, and debugging, every server has an HTTP server that provides diagnostics and statistics for a given task.

Development Environment
  Apart from a few groups that have their own open source repositories (e.g., Android and Chrome), Google Software Engineers work from a single shared repository [Pot16]. This has a few important practical implications for our workflows:

  If engineers encounter a problem in a component outside of their project, they can fix the problem, send the proposed changes ("changelist," or CL) to the owner for review, and submit the CL to the mainline.
  Changes to source code in an engineer’s own project require a review. All software is reviewed before being submitted.
  
  When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it notifies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.


